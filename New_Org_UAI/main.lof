\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces Well, look at that. Can you guess if we are about to face another one soon enough?}}{9}{figure.caption.6}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The diagram of a typical expert system. Here, human involvement is fairly representative in the figure, and is illustratively indicating the overwhelming reliance on human touches.}}{11}{figure.caption.7}%
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces An illustration of a vector in two-dimensional vector form in endpoints representation, and directional-magnitude representation.}}{53}{figure.caption.21}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces The \textbf {parallelogram law} for vector addition of two vectors $x$ and $y$ on adjacent side.}}{55}{figure.caption.22}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustration of matrix multiplication. Taken from \cite {ANDRILLI20101}.}}{59}{figure.caption.26}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Illustration of matrix multiplication for $2\times 2$ shape.}}{60}{figure.caption.27}%
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {6.1}{\ignorespaces Markov's inequality bounds the probability for the shaded region $\mathbb {P}[X\geq a]$}}{84}{figure.caption.42}%
\addvspace {10pt}
\contentsline {figure}{\numberline {7.1}{\ignorespaces The problem-solving scheme from the mathematical modelling perspective}}{97}{figure.caption.44}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces There exists an unbreakable wall in the black-box condition - throwing a dart in blind, except perhaps it can be right.}}{98}{figure.caption.45}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces A typical pendulum with degree 1, for parameter $\theta $ as angle, and a rod of length $\ell $ connecting the origin to the mass $m$.}}{99}{figure.caption.46}%
\contentsline {figure}{\numberline {7.4}{\ignorespaces With the question $Q$, you can ask everything, including the... not so pure one.}}{102}{figure.caption.47}%
\contentsline {figure}{\numberline {7.5}{\ignorespaces Plato's allegory of the cave by Jan Saenredam, according to Cornelis van Haarlem, 1604, Albertina, Vienna}}{105}{figure.caption.51}%
\addvspace {10pt}
\contentsline {figure}{\numberline {8.1}{\ignorespaces An illustration of statistical learning theory on the evaluation of the risks and errors, during learning process. $c'$ is presented in the `orbital' vicinity around $c$, with its distance of certain metric define how 'accurate' the reconstruction from distribution can be. Of the hypothesis set $\mathcal {H}$, there exists the Bayes hypothesis $h_{B}$ and an arbitrary `random' hypothesis $h$, and their respective measure.}}{115}{figure.caption.52}%
\contentsline {figure}{\numberline {8.2}{\ignorespaces An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal {D}$, and second is the $\mathsf {Update}$ process to re-align $c$ to the actual target.}}{118}{figure.caption.54}%
\contentsline {figure}{\numberline {8.3}{\ignorespaces \textbf {Conceptual representation of the sample set partitions and its effect on iterative process}. (1), for $S_{1}$, and of the specified ordering, $h$ is able to make it to the optimal point compared to the actual concept $c$. The bubble around $c$ is what we call irreducible error, intrinsic of the observational space. (2) for $S_{2}$, of the changing dataset, while of the same partition but also changing order, gives different volatile path, and perhaps suboptimal performance compared to the first dataset case. Note that they are the supposed \textit {\color {orange!70!black}optimal path} of both $S_{1},S_{2}$. If randomization is introduced, may suboptimal path will occur, and the result will differ.}}{122}{figure.caption.55}%
\contentsline {figure}{\numberline {8.4}{\ignorespaces Partitioning process and its error potential consideration. We assume each partition includes the irreducible error $\epsilon $ accompanied by the $n$ partition, belongs to the furthest partitioning set. Within every increasing partition, for supposed distributed data (unordered data), the generalization risk is further decomposed.}}{123}{figure.caption.56}%
\contentsline {figure}{\numberline {8.5}{\ignorespaces Illustration of the notion of hyperplane in two and three dimensions. This can be extended to $n>3$ dimension, but no figurative illustration can be found (or ever understood). Taken from \href {https://r4ds.github.io/bookclub-islr/hyperplane.html}{Introduction to Statistical Learning using R Book Club} by The R4DS Online Learning Community.}}{145}{figure.caption.60}%
\contentsline {figure}{\numberline {8.6}{\ignorespaces Illustration of a halfspace \textbf {region} created by a hyperplane on the side of the axis. If the halfspace is created of the unit frame hyperplane (aligning with the axis), then it is called a \textit {\color {orange!70!black}normal space partitioning halfspace}.}}{145}{figure.caption.61}%
\contentsline {figure}{\numberline {8.7}{\ignorespaces VC-dimension of intervals on the real line. (a) Any two points can be shattered. (b) No sample of three points can be shattered as the $(+,-,+)$ labelling cannot be realized. Taken from \cite {10.5555/2371238}.}}{146}{figure.caption.62}%
\contentsline {figure}{\numberline {8.8}{\ignorespaces Unrealizable dichotomies for four points using hyperplanes in $\mathbb {R}^{2}$. (a) All four points lie on the convex hull. (b) Three points lie on the convex hull while the remaining point is interior. Taken from \cite {10.5555/2371238}.}}{147}{figure.caption.63}%
\contentsline {figure}{\numberline {8.9}{\ignorespaces Illustration of convex and non-convex set. The segment $[x,y]$ must be fully contained in the region of the set, otherwise it is not convex.}}{148}{figure.caption.64}%
\contentsline {figure}{\numberline {8.10}{\ignorespaces Illustration of (left-hand side) $d=1$ Radon partition, and (middle and right-hand side) $d=2$ Radon partition. More options are available as $d$ increases.}}{151}{figure.caption.65}%
\addvspace {10pt}
\contentsline {figure}{\numberline {9.1}{\ignorespaces The simplistic, schematic illustration of the structure of the biological neuron.}}{156}{figure.caption.66}%
\contentsline {figure}{\numberline {9.2}{\ignorespaces An illustration of Santiago Ram√≥n y Cajal on the structure and design of a biological brain network. Many of these was made during his career.}}{158}{figure.caption.67}%
\contentsline {figure}{\numberline {9.3}{\ignorespaces Examples of the rich variety of nerve cell morphologies found in the human nervous system. Tracings are from actual nerve cells stained by impregnation with silver salts (the socalled Golgi technique the method used in the classical studies of Golgi and Cajal). Asterisks indicate that the axon runs on much farther than shown. Note that some cells, like the retinal bipolar cell, have a very short axon, and that others, like the retinal amacrine cell, have no axon at all. The drawings are not all at the same scale. Some more details about the jargon is the \textit {\color {orange!70!black}retinal bipolar cells}, which are neurons that connect the outer retina to the inner retina, for processing layer (or projection neurons, where all information are relayed from this connection.); the \textit {\color {orange!70!black}retinal ganglion cell, amacrine cells} are the same visual processing unit; Cerebellar Purkinje cells (a type of GABAergic neurons) uniquely determined for cerebella cortex (for processing large data, and coordinating functions like cognition and emotions.). Reused from \cite {purves_neuroscience_2004}.}}{159}{figure.caption.68}%
\contentsline {figure}{\numberline {9.4}{\ignorespaces \textbf {An illustrative example of the abstraction and categorization by 'size' of different components and constructs in the neuron model.} By the order of abstraction $k$, we assign a notion of size on different neural structure, by increasing complexity, and backward compatibility (described to be composed of previously defined objects). The first two stage for $k=1,2$ includes the standard basis components $\{n_{0,i}\}$ and the standard neuron class $N_{0}$, respectively.}}{161}{figure.caption.69}%
\contentsline {figure}{\numberline {9.5}{\ignorespaces \textbf {The standard minimal configuration of any neuron $x\in \mathcal {N}_{i}$}. We denote $p$, $q$ for particular neuron input and output sequences.}}{166}{figure.caption.70}%
\contentsline {figure}{\numberline {9.6}{\ignorespaces Commutative diagram of the standard $\mathcal {N}_{0}(\mathbb {R})$ class. The in-out objects are denoted in blue, the operators are denoted in red, and the objective mass (parameters) are denoted in yellow. The procedure is then denoted of four successive processes of $S_{i}$, up to $S_{4}$.}}{167}{figure.caption.71}%
\contentsline {figure}{\numberline {9.7}{\ignorespaces \textbf {Illustration standard neuron class $\mathcal {N}_{0}$}. $(a)$. We regard the component of the model as $\mathbf {M}$, consists of the mass and the operations $\mathbf {H}$. $(b)$. Instead of considering the operation as subcomponent of the model structure, decomposition gives them separated with two types of operation - either \textit {\color {orange!70!black}processing} operators (operations that prepare the parameters) or \textit {\color {orange!70!black}transforming} operators (act on the prepared processing that it receives).}}{168}{figure.caption.72}%
\contentsline {figure}{\numberline {9.8}{\ignorespaces Chaining of multiple standard unit on each other.}}{169}{figure.caption.73}%
\contentsline {figure}{\numberline {9.9}{\ignorespaces Illustration of the chaining process and the nested function chaining between $\sigma _{i}$.}}{170}{figure.caption.74}%
\contentsline {figure}{\numberline {9.10}{\ignorespaces Initial starting configuration for \(x_{1}, x_{2}, x_{3}\) and their functionals \(\sigma _{1}, \sigma _{2}, \sigma _{3}\) under the same initializer.}}{170}{figure.caption.75}%
\contentsline {figure}{\numberline {9.11}{\ignorespaces Sequential configuration for \(x_{1},x_{2},x_{3}\) and their functionals \(\sigma _{1},\sigma _{2},\sigma _{3}\) of the same initializer with \(\mathbf {w}=(2.0,-2.5,2.5)\).}}{171}{figure.caption.76}%
\contentsline {figure}{\numberline {9.12}{\ignorespaces \textbf {Illustrative simplified commutative diagram schematic of $r$-input neuron process unit.} The specific field dimension transition for a standard neuron of $\mathbb {R}$ is denoted specifically between transitions.}}{172}{figure.caption.77}%
\contentsline {figure}{\numberline {9.13}{\ignorespaces \textbf {Schematic of the $\mathsf {AND}$ and $\mathsf {OR}$ logical configuration}. The only change in their construction is that the criterion in the function is now different - from $|\mathbf {x}|=\Sigma (\mathbf {x})$ (which means all signals' sum must be equal to their absolute magnitude - in agree state), or $\Sigma (\mathbf {x})\ne 0$ (as long as a single signal is active is enough).}}{173}{figure.caption.78}%
\contentsline {figure}{\numberline {9.14}{\ignorespaces Schematic of the $\mathsf {NOT}$ logical configuration.}}{173}{figure.caption.79}%
\contentsline {figure}{\numberline {9.15}{\ignorespaces Schematic of the $\mathsf {NAND}$ and $\mathsf {NOR}$ logical configurations.}}{174}{figure.caption.80}%
\contentsline {figure}{\numberline {9.16}{\ignorespaces Results and values of the predicate $\psi _{\text {circle}}$ on various geometrical shape. The detail of what gives the criterion is not mentioned, however implicitly defined to be naturally encoded. Taken from \cite {10.5555/50066}.}}{175}{figure.caption.81}%
\contentsline {figure}{\numberline {9.17}{\ignorespaces Results and values of the predicate $\psi _{\text {convex}}$ on various geometrical shape. While still being implicitly defined, computationally this predicate takes more complexity than the circle predicate. Taken from \cite {10.5555/50066}.}}{175}{figure.caption.82}%
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {11.1}{\ignorespaces Minimal neuron structure}}{184}{figure.caption.84}%
\contentsline {figure}{\numberline {11.2}{\ignorespaces The compound structure construction. The same component can be seen, for $n_i,n_o$ and $M$. Multiple consecutive components construct some components, and further outward. Also, we also reflect the complexity of $\mathcal {C}$ for a given architecture.}}{185}{figure.caption.85}%
\addvspace {10pt}
\contentsline {figure}{\numberline {12.1}{\ignorespaces (a) A typical example of bias-variance tradeoff in a statistical dataset. (b) When graphed into a continuous notion, we gain the complexity-error graph. Notice that it specifically goes for the \textit {\color {orange!70!black}test error}, which fits - the representative problem of prediction.}}{192}{figure.caption.86}%
\contentsline {figure}{\numberline {12.2}{\ignorespaces {\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}.}}{193}{figure.caption.87}%
\contentsline {figure}{\numberline {12.4}{\ignorespaces {\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs.}}{196}{figure.caption.92}%
\contentsline {figure}{\numberline {12.3}{\ignorespaces {\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Reproduced from \cite {nakkiran_deep_2019}. }}{196}{figure.caption.91}%
\contentsline {figure}{\numberline {12.5}{\ignorespaces The representative order of representation and description. As of the name implied, in transition to a mathematical formalism and language, there must then exist a representation to each and every element of certain subject. The process of doing is this called \textit {\color {orange!70!black}external encoding}, and is true also between portion of mathematical-encoded system to each other, if they are distinct. The reverse act is called again, \textit {\color {orange!70!black}decoding}, and between mathematical subjects to each other might as well be called \textit {\color {orange!70!black}internal encoding}, with respect to the mathematical language.}}{206}{figure.caption.94}%
\contentsline {figure}{\numberline {12.6}{\ignorespaces A conceptual illustration on the running flow of an $n$-layer GNN on particular structure of interest. Note that the data section itself has particular embedding structure on its own.}}{225}{figure.caption.98}%
\addvspace {10pt}
\contentsline {figure}{\numberline {13.1}{\ignorespaces (a) Figure of the original organization of the biological model of the brain functions. (b) Specifically, note that it is specifically for optical case, but can be extended to others type. Furthermore, the layer between last $A$-unit and the response units, there exists a pattern of feedback loop.}}{229}{figure.caption.99}%
\addvspace {10pt}
\contentsline {figure}{\numberline {1}{\ignorespaces The typical hard limit transfer function with fixed $a$, and fixed range for $x$ in $[0,1]$.}}{235}{figure.caption.104}%
\contentsline {figure}{\numberline {2}{\ignorespaces The typical hard limit transfer function with variable inhibition $a$, and fixed range for $x$ in $[0,1]$.}}{236}{figure.caption.105}%
\contentsline {figure}{\numberline {3}{\ignorespaces The typical symmetric hard limit transfer function with static inhibition $a$, and fixed range for $x$ in $[-1,+1]$. As specified, this is the normal-extended range.}}{237}{figure.caption.107}%
\contentsline {figure}{\numberline {4}{\ignorespaces The saturating linear with linear region of $[0,1]$. A smoother variation would be something like sigmoidal functions, that is.}}{237}{figure.caption.109}%
\contentsline {figure}{\numberline {5}{\ignorespaces The symmetric saturating linear with linear region of $[-1,1]$, a positive-negative variation of the saturating linear.}}{238}{figure.caption.110}%
\contentsline {figure}{\numberline {6}{\ignorespaces The sigmoidal function channel}}{238}{figure.caption.112}%
\contentsline {figure}{\numberline {7}{\ignorespaces The logarithmic sigmoidal function channel. Notice that the range of \texttt {logsigmoid} is $[-\infty , 0]$, making it somewhat weird of a choice for a transfer function.}}{238}{figure.caption.113}%
\contentsline {figure}{\numberline {8}{\ignorespaces The hyperbolic tangent transfer function channel.}}{239}{figure.caption.115}%
