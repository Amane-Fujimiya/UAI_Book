\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {12}Double Descent}{191}{chapter.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Note}{191}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Developing analysis}{191}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Statistical learning theory}{191}{subsection.12.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Double descent}{191}{subsection.12.2.2}\protected@file@percent }
\oddpage@label{85}{191}
\oddpage@label{86}{191}
\citation{6797087}
\citation{belkin_reconciling_2019}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.2.1\else \numberline {12.2.1}Theorem\fi \thmtformatoptarg {Bias-variance tradeoff}}{192}{theorem.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces (a) A typical example of bias-variance tradeoff in a statistical dataset. (b) When graphed into a continuous notion, we gain the complexity-error graph. Notice that it specifically goes for the \textit  {\color  {orange!70!black}test error}, which fits - the representative problem of prediction.}}{192}{figure.caption.86}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces {\bf  Curves for training risk (dashed line) and test risk (solid line).} ({\bf  a}) The classical \emph  {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf  b}) The \emph  {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite  {belkin_reconciling_2019}.}}{193}{figure.caption.87}\protected@file@percent }
\newlabel{fig:double-descent}{{12.2}{193}{{\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}}{figure.caption.87}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.2.1\else \numberline {12.2.1}Definition\fi \thmtformatoptarg {Random Fourier features}}{193}{definition.1}\protected@file@percent }
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces {\bf  Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf  Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs.}}{196}{figure.caption.92}\protected@file@percent }
\newlabel{fig:unified}{{12.4}{196}{{\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs}{figure.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces {\bf  Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf  Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Reproduced from \cite  {nakkiran_deep_2019}. }}{196}{figure.caption.91}\protected@file@percent }
\newlabel{fig:errorvscomplexity}{{12.3}{196}{{\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Reproduced from \cite {nakkiran_deep_2019}}{figure.caption.91}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.2.2\else \numberline {12.2.2}Definition\fi \thmtformatoptarg {Effective Model Complexity}}{196}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {hypothesis}{\ifthmt@listswap Hypothesis~12.2.1\else \numberline {12.2.1}Hypothesis\fi \thmtformatoptarg {Generalized Double Descent hypothesis, informal}}{196}{hypothesis.1}\protected@file@percent }
\newlabel{hyp:informaldd}{{12.2.1}{196}{Generalized Double Descent hypothesis, informal}{hypothesis.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Issues}{197}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}The messiness of analysis}{197}{subsection.12.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}The ambiguity of analysis}{197}{subsection.12.3.2}\protected@file@percent }
\citation{nakkiran_deep_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.3}Stepping in the wrong direction}{198}{subsection.12.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.4}Main problems}{198}{subsection.12.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.5}Hypothesis}{198}{subsection.12.3.5}\protected@file@percent }
\citation{10.5555/2371238}
\citation{VeltenetalMathematicalModelling}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.6}A rather simple solution}{200}{subsection.12.3.6}\protected@file@percent }
\citation{Vapnik1999-VAPTNO}
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~12.3.1\else \numberline {12.3.1}Conjecture\fi }{201}{conjecture.1}\protected@file@percent }
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~12.3.2\else \numberline {12.3.2}Conjecture\fi }{201}{conjecture.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.4}The perspective of modelling theory}{202}{section.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Structures of object}{202}{subsection.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Representation}{203}{subsubsection*.93}\protected@file@percent }
\@writefile{loe}{\contentsline {assume}{\ifthmt@listswap Assumption~12.4.1.1\else \numberline {12.4.1.1}Assumption\fi }{203}{assume.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.4.1\else \numberline {12.4.1}Definition\fi \thmtformatoptarg {Representation scheme}}{203}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap Note~12.4.1\else \numberline {12.4.1}Note\fi }{205}{note.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces The representative order of representation and description. As of the name implied, in transition to a mathematical formalism and language, there must then exist a representation to each and every element of certain subject. The process of doing is this called \textit  {\color  {orange!70!black}external encoding}, and is true also between portion of mathematical-encoded system to each other, if they are distinct. The reverse act is called again, \textit  {\color  {orange!70!black}decoding}, and between mathematical subjects to each other might as well be called \textit  {\color  {orange!70!black}internal encoding}, with respect to the mathematical language.}}{206}{figure.caption.94}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Description}{206}{subsubsection*.95}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.5}The perspective of modified learning setting}{206}{section.12.5}\protected@file@percent }
\citation{6797087}
\citation{6797087}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Bias-variance: A history}{207}{section.12.6}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.6.1\else \numberline {12.6.1}Theorem\fi \thmtformatoptarg {Bias-variance tradeoff}}{207}{theorem.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.6.1\else \numberline {12.6.1}Definition\fi \thmtformatoptarg {Parameterization}}{208}{definition.1}\protected@file@percent }
\citation{6797087}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.1}Another approach - No-free-lunch}{209}{subsection.12.6.1}\protected@file@percent }
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~12.6.1\else \numberline {12.6.1}Conjecture\fi \thmtformatoptarg {General insight}}{209}{conjecture.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.6.2\else \numberline {12.6.2}Theorem\fi \thmtformatoptarg {No-Free-Lunch}}{209}{theorem.2}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~12.6.1\else \numberline {12.6.1}Corollary\fi }{209}{col.1}\protected@file@percent }
\citation{10.1145/1968.1972}
\citation{Vapnik1999-VAPTNO}
\citation{6797087}
\citation{belkin_reconciling_2019}
\citation{Vapnik1999-VAPTNO,10.5555/2371238,10.5555/2621980,STL_Hajek_Maxim_2021,bousquet2020theoryuniversallearning}
\citation{6797087,Domingos2000AUB}
\citation{belkin_reconciling_2019,schaeffer_double_2023,nakkiran_deep_2019,lafon_understanding_2024}
\citation{davies_unifying_2023,d_ascoli_triple_2020}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Abstract}{211}{section.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.8}Introduction}{211}{section.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8.1}Statistical Learning and Double Descent}{211}{subsection.12.8.1}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8.2}Relation to Graph Neural Network}{212}{subsection.12.8.2}\protected@file@percent }
\citation{STL_Hajek_Maxim_2021,10.5555/2371238,10.5555/2621980}
\@writefile{toc}{\contentsline {section}{\numberline {12.9}Outline}{213}{section.12.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.10}Background}{213}{section.12.10}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.10.1\else \numberline {12.10.1}Definition\fi \thmtformatoptarg {Empirical risk}}{214}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.10.2\else \numberline {12.10.2}Definition\fi \thmtformatoptarg {Generalization risk}}{214}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.10.3\else \numberline {12.10.3}Definition\fi \thmtformatoptarg {Empirical learning problem}}{214}{definition.3}\protected@file@percent }
\newlabel{eq:lp12}{{12.20}{214}{Empirical learning problem}{equation.12.20}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.10.4\else \numberline {12.10.4}Definition\fi \thmtformatoptarg {Generalization learning problem}}{214}{definition.4}\protected@file@percent }
\citation{noauthor_bias-variance_nodate,hellstrom_bias_2020,6797087,Scott_Fortmann_Bias}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.10.1\else \numberline {12.10.1}Theorem\fi }{215}{theorem.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.11}Bias-variance tradeoff}{215}{section.12.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.11.1}Defining the bias and variances}{215}{subsection.12.11.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.11.1\else \numberline {12.11.1}Definition\fi \thmtformatoptarg {Bias, I}}{215}{definition.1}\protected@file@percent }
\citation{6797087}
\citation{6797087}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.11.2\else \numberline {12.11.2}Definition\fi \thmtformatoptarg {Variance, I}}{216}{definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.11.2}Precursor (Geman, 1992)}{216}{subsection.12.11.2}\protected@file@percent }
\citation{brown2024biasvariance}
\citation{brown2024biasvariance,PfauBregmanDivergence}
\citation{lafon_understanding_2024}
\citation{Grenander1952OnES}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.11.1\else \numberline {12.11.1}Theorem\fi \thmtformatoptarg {Bias-variance decomposition}}{217}{theorem.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.11.2\else \numberline {12.11.2}Theorem\fi \thmtformatoptarg {Bias-variance tradeoff}}{217}{theorem.2}\protected@file@percent }
\citation{10.5555/2621980}
\citation{brown2024biasvariance}
\citation{PfauBregmanDivergence,buschjager_generalized_2020,unified_bias_composition}
\citation{neal2019biasvariancetradeofftextbooksneed}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.11.3}Formalism issues and uncertainty}{218}{subsection.12.11.3}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~12.11.3\else \numberline {12.11.3}Theorem\fi \thmtformatoptarg {No-Free-Lunch}}{218}{theorem.3}\protected@file@percent }
\citation{10.5555/2371238,lafon_understanding_2024}
\citation{brown2024biasvariance}
\citation{10.5555/2621980,10.5555/2371238}
\citation{belkin_reconciling_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.11.4}Approximation-Estimation tradeoff}{219}{subsection.12.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.12}Double descent}{219}{section.12.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.13}The break-off between theoretical and modern practice}{220}{section.12.13}\protected@file@percent }
\citation{goodfellow2016deep}
\citation{gareth_james_introduction_2013}
\citation{Vapnik1999-VAPTNO}
\@writefile{toc}{\contentsline {section}{\numberline {12.14}Preliminary experiments}{221}{section.12.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.14.1}Polynomial model}{221}{subsection.12.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.14.2}Support Vector Machine (SVM)}{221}{subsection.12.14.2}\protected@file@percent }
\citation{Cristianini2000AnIT}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent,buschjager_generalized_2020}
\citation{GRP_Hamilton}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent}
\@writefile{toc}{\contentsline {section}{\numberline {12.15}Experiments}{223}{section.12.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.15.1}Main result}{223}{subsection.12.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quick introduction to graph theory}{223}{subsubsection*.96}\protected@file@percent }
\citation{GRP_Hamilton,Scar04}
\citation{Scar04,Veli_kovi__2023,tanis2024introductiongraphneuralnetworks,lopushanskyy2024graphneuralnetworksgraph}
\citation{Scar04,GRP_Hamilton}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~12.15.1\else \numberline {12.15.1}Definition\fi \thmtformatoptarg {Graph-theoretical edge learning}}{224}{definition.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Graph Neural Network}{224}{subsubsection*.97}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces A conceptual illustration on the running flow of an $n$-layer GNN on particular structure of interest. Note that the data section itself has particular embedding structure on its own.}}{225}{figure.caption.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.15.2}Analysis of GNN}{225}{subsection.12.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.15.3}Experiment 1: Identifying bias-variance in GNN}{225}{subsection.12.15.3}\protected@file@percent }
\citation{Oono2020Graph}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{Oono2020Graph}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\citation{bronstein2021geometricdeeplearninggrids,Bronstein_2017}
\citation{Scar04}
\citation{GRP_Hamilton}
\@writefile{toc}{\contentsline {section}{\numberline {12.16}Conclusion}{226}{section.12.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.17}Related works}{226}{section.12.17}\protected@file@percent }
\@setckpt{content/draft/c1_research}{
\setcounter{page}{227}
\setcounter{equation}{40}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{3}
\setcounter{chapter}{12}
\setcounter{section}{17}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{section@level}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{241}
\setcounter{lastsheet}{260}
\setcounter{lastpage}{246}
\setcounter{figure}{6}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{parnotemark}{1}
\setcounter{float@type}{8}
\setcounter{thmt@dummyctr}{205}
\setcounter{tabularnote}{0}
\setcounter{nicematrix_draft}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{115}
\setcounter{Hfootnote}{53}
\setcounter{bookmark@seq@number}{198}
\setcounter{memhycontfloat}{0}
\setcounter{mem@Hpagenote}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{col}{0}
\setcounter{conjecture}{0}
\setcounter{setting}{0}
\setcounter{assume}{0}
\setcounter{hypothesis}{0}
\setcounter{axiom}{0}
\setcounter{question}{0}
\setcounter{example}{0}
\setcounter{note}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{205}
\setcounter{tabularnotesi}{0}
\setcounter{tabularnotes*i}{0}
}
