\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Classical learning theory}{109}{chapter.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Before the learning theory}{109}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Classical theory}{110}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Principles}{111}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Concept and hypothesis}{111}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Phenomenological aberration}{112}{subsection.8.2.3}\protected@file@percent }
\@@wrindexm@m{main}{empirical risk|hyperpage}{113}
\oddpage@label{50}{113}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.1\else \numberline {8.2.1}Definition\fi \thmtformatoptarg {Empirical risk}}{113}{definition.1}\protected@file@percent }
\@@wrindexm@m{main}{generalization risk|hyperpage}{113}
\oddpage@label{51}{113}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.2\else \numberline {8.2.2}Definition\fi \thmtformatoptarg {Generalization risk}}{113}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.2.1\else \numberline {8.2.1}Theorem\fi }{113}{theorem.1}\protected@file@percent }
\newlabel{thm:minimalNeu}{{8.2.1}{113}{}{theorem.1}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.3\else \numberline {8.2.3}Definition\fi \thmtformatoptarg {Empirical learning problem}}{114}{definition.3}\protected@file@percent }
\newlabel{eq:lp1}{{8.5}{114}{Empirical learning problem}{equation.8.5}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.4\else \numberline {8.2.4}Definition\fi \thmtformatoptarg {Generalization learning problem}}{114}{definition.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces An illustration of statistical learning theory on the evaluation of the risks and errors, during learning process. $c'$ is presented in the `orbital' vicinity around $c$, with its distance of certain metric define how 'accurate' the reconstruction from distribution can be. Of the hypothesis set $\mathcal  {H}$, there exists the Bayes hypothesis $h_{B}$ and an arbitrary `random' hypothesis $h$, and their respective measure.}}{115}{figure.caption.52}\protected@file@percent }
\newlabel{fig:genvsemp_fig}{{8.1}{115}{An illustration of statistical learning theory on the evaluation of the risks and errors, during learning process. $c'$ is presented in the `orbital' vicinity around $c$, with its distance of certain metric define how 'accurate' the reconstruction from distribution can be. Of the hypothesis set $\mathcal {H}$, there exists the Bayes hypothesis $h_{B}$ and an arbitrary `random' hypothesis $h$, and their respective measure}{figure.caption.52}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.5\else \numberline {8.2.5}Definition\fi \thmtformatoptarg {Bayes risk}}{115}{definition.5}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2.6\else \numberline {8.2.6}Definition\fi \thmtformatoptarg {Bayes model}}{115}{definition.6}\protected@file@percent }
\@writefile{loe}{\contentsline {setting}{\ifthmt@listswap Setting~8.2.1\else \numberline {8.2.1}Setting\fi }{116}{setting.1}\protected@file@percent }
\@writefile{loe}{\contentsline {question}{\ifthmt@listswap Question~8.2.1\else \numberline {8.2.1}Question\fi }{116}{question.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{About $\mathcal  {X}$ and $\mathcal  {Y}$}{116}{subsubsection*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Estimation and approximation error}{117}{subsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Structural example of learning setting}{117}{section.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal  {D}$, and second is the $\mathsf  {Update}$ process to re-align $c$ to the actual target.}}{118}{figure.caption.54}\protected@file@percent }
\newlabel{fig:PhaseDiagram}{{8.2}{118}{An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal {D}$, and second is the $\mathsf {Update}$ process to re-align $c$ to the actual target}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}First section}{118}{subsection.8.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Second section}{118}{subsection.8.3.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.3.1\else \numberline {8.3.1}Definition\fi \thmtformatoptarg {Deterministic - \textit  {\color  {orange!70!black}discriminative modelling}}}{119}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.3.2\else \numberline {8.3.2}Definition\fi \thmtformatoptarg {Probabilistic - \textit  {\color  {orange!70!black}generative modelling}}}{119}{definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Third section}{119}{subsection.8.3.3}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.3.3\else \numberline {8.3.3}Definition\fi \thmtformatoptarg {Loss function}}{119}{definition.3}\protected@file@percent }
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~8.3.1\else \numberline {8.3.1}Conjecture\fi \thmtformatoptarg {Loss function convexity}}{119}{conjecture.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.3.4\else \numberline {8.3.4}Definition\fi \thmtformatoptarg {Empirical risk minimization}}{120}{definition.4}\protected@file@percent }
\@writefile{loe}{\contentsline {proposition}{\ifthmt@listswap Proposition~8.3.1\else \numberline {8.3.1}Proposition\fi }{120}{proposition.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}The problems within classical learning theory}{120}{section.8.4}\protected@file@percent }
\newlabel{sec:LearningProblems}{{8.4}{120}{The problems within classical learning theory}{section.8.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Data and the general setting}{120}{subsection.8.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Observational partitioning}{121}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \textbf  {Conceptual representation of the sample set partitions and its effect on iterative process}. (1), for $S_{1}$, and of the specified ordering, $h$ is able to make it to the optimal point compared to the actual concept $c$. The bubble around $c$ is what we call irreducible error, intrinsic of the observational space. (2) for $S_{2}$, of the changing dataset, while of the same partition but also changing order, gives different volatile path, and perhaps suboptimal performance compared to the first dataset case. Note that they are the supposed \textit  {\color  {orange!70!black}optimal path} of both $S_{1},S_{2}$. If randomization is introduced, may suboptimal path will occur, and the result will differ.}}{122}{figure.caption.55}\protected@file@percent }
\newlabel{fig:randomwalk_descent}{{8.3}{122}{\textbf {Conceptual representation of the sample set partitions and its effect on iterative process}. (1), for $S_{1}$, and of the specified ordering, $h$ is able to make it to the optimal point compared to the actual concept $c$. The bubble around $c$ is what we call irreducible error, intrinsic of the observational space. (2) for $S_{2}$, of the changing dataset, while of the same partition but also changing order, gives different volatile path, and perhaps suboptimal performance compared to the first dataset case. Note that they are the supposed \textit {optimal path} of both $S_{1},S_{2}$. If randomization is introduced, may suboptimal path will occur, and the result will differ}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Partitioning process and its error potential consideration. We assume each partition includes the irreducible error $\epsilon $ accompanied by the $n$ partition, belongs to the furthest partitioning set. Within every increasing partition, for supposed distributed data (unordered data), the generalization risk is further decomposed.}}{123}{figure.caption.56}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Learning criteria on time - PAC theory}{123}{section.8.5}\protected@file@percent }
\@@wrindexm@m{main}{PAC learning|hyperpage}{124}
\oddpage@label{52}{124}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Classical PAC-learning}{124}{subsection.8.5.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.1\else \numberline {8.5.1}Definition\fi \thmtformatoptarg {Model-specific learning}}{125}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.2\else \numberline {8.5.2}Definition\fi \thmtformatoptarg {Model-free learning}}{125}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.3\else \numberline {8.5.3}Definition\fi \thmtformatoptarg {PAC Learning, Preliminary Definition}}{126}{definition.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Representation size and PAC-configuration}{126}{subsubsection*.57}\protected@file@percent }
\@@wrindexm@m{main}{representation scheme|hyperpage}{126}
\oddpage@label{53}{126}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.4\else \numberline {8.5.4}Definition\fi \thmtformatoptarg {Representation scheme}}{127}{definition.4}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.5\else \numberline {8.5.5}Definition\fi \thmtformatoptarg {Representation complexity}}{127}{definition.5}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.5.6\else \numberline {8.5.6}Definition\fi \thmtformatoptarg {PAC-learning}}{128}{definition.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Generalization bound for PAC-learning}{128}{section.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}Consistent Learning}{128}{subsection.8.6.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.6.1\else \numberline {8.6.1}Definition\fi \thmtformatoptarg {Consistent Learner}}{128}{definition.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}Finite $H$, consistent hypothesis}{128}{subsection.8.6.2}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.6.1\else \numberline {8.6.1}Theorem\fi \thmtformatoptarg {Learning bound - finite $\mathcal  {H}$, consistent case}}{129}{theorem.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.3}Examples}{130}{subsection.8.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Boolean Conjunction}{130}{subsubsection*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Universal Concept Classes}{130}{subsubsection*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.4}Finite hypothesis sets $H$ - inconsistent case}{130}{subsection.8.6.4}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~8.6.1\else \numberline {8.6.1}Corollary\fi }{130}{col.1}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~8.6.2\else \numberline {8.6.2}Corollary\fi \thmtformatoptarg {Generalization bound - single hypothesis}}{132}{col.2}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.6.2\else \numberline {8.6.2}Theorem\fi \thmtformatoptarg {Learning bound - finite $\mathcal  {H}$, inconsistent case}}{132}{theorem.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.7}(Agnostic) General PAC-learning}{132}{section.8.7}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.7.1\else \numberline {8.7.1}Definition\fi \thmtformatoptarg {Agnostic PAC-learning}}{133}{definition.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7.1}Noise}{133}{subsection.8.7.1}\protected@file@percent }
\@@wrindexm@m{main}{noise|hyperpage}{133}
\oddpage@label{54}{133}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.7.2\else \numberline {8.7.2}Definition\fi \thmtformatoptarg {Noise}}{134}{definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.8}Occam's Razor}{134}{section.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}Occam Learning and Succinctness}{134}{subsection.8.8.1}\protected@file@percent }
\oddpage@label{55}{135}
\oddpage@label{56}{135}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.8.1\else \numberline {8.8.1}Definition\fi \thmtformatoptarg {Occam algorithm}}{135}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.8.1\else \numberline {8.8.1}Theorem\fi }{135}{theorem.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.8.2\else \numberline {8.8.2}Theorem\fi \thmtformatoptarg {Occam's Razor}}{136}{theorem.2}\protected@file@percent }
\newlabel{eq:Occam1}{{8.8.2}{136}{Occam's Razor}{theorem.2}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.8.3\else \numberline {8.8.3}Theorem\fi \thmtformatoptarg {Occam's Razor, Cardinality Version}}{136}{theorem.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.9}Rademacher Complexity}{136}{section.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Rademacher descriptions}{137}{subsection.8.9.1}\protected@file@percent }
\@@wrindexm@m{main}{ERC|hyperpage}{137}
\oddpage@label{57}{137}
\@@wrindexm@m{main}{Rademacher variables|hyperpage}{137}
\oddpage@label{58}{137}
\@@wrindexm@m{main}{Rademacher complexity|hyperpage}{137}
\oddpage@label{59}{137}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.9.1\else \numberline {8.9.1}Definition\fi \thmtformatoptarg {Rademacher complexity}}{138}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.9.1\else \numberline {8.9.1}Theorem\fi }{138}{theorem.1}\protected@file@percent }
\newlabel{ref:3.x}{{8.32}{138}{}{equation.8.32}{}}
\newlabel{ref:3.x1}{{8.33}{138}{}{equation.8.33}{}}
\newlabel{eq:3.13}{{8.46}{139}{Rademacher descriptions}{equation.8.46}{}}
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~8.9.1\else \numberline {8.9.1}Corollary\fi }{140}{col.1}\protected@file@percent }
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma~8.9.2\else \numberline {8.9.2}Lemma\fi }{140}{lemma.2}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.9.3\else \numberline {8.9.3}Theorem\fi \thmtformatoptarg {Rademacher complexity bounds - binary classification}}{141}{theorem.3}\protected@file@percent }
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example~8.9.1\else \numberline {8.9.1}Example\fi }{141}{example.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.2}Growth function}{142}{subsection.8.9.2}\protected@file@percent }
\@@wrindexm@m{main}{growth function|hyperpage}{142}
\oddpage@label{60}{142}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.9.2\else \numberline {8.9.2}Definition\fi \thmtformatoptarg {Growth function}}{142}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma~8.9.4\else \numberline {8.9.4}Lemma\fi \thmtformatoptarg {Massart's lemma}}{142}{lemma.4}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~8.9.2\else \numberline {8.9.2}Corollary\fi }{143}{col.2}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~8.9.3\else \numberline {8.9.3}Corollary\fi \thmtformatoptarg {Growth function generalization bound}}{143}{col.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.10}Vapnik-Chervonenkis Theory}{143}{section.8.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.1}Clarification {\small  of the linear halfspaces}}{144}{subsection.8.10.1}\protected@file@percent }
\@@wrindexm@m{main}{halfspace|hyperpage}{144}
\oddpage@label{61}{144}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Illustration of the notion of hyperplane in two and three dimensions. This can be extended to $n>3$ dimension, but no figurative illustration can be found (or ever understood). Taken from \href  {https://r4ds.github.io/bookclub-islr/hyperplane.html}{Introduction to Statistical Learning using R Book Club} by The R4DS Online Learning Community.}}{145}{figure.caption.60}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Illustration of a halfspace \textbf  {region} created by a hyperplane on the side of the axis. If the halfspace is created of the unit frame hyperplane (aligning with the axis), then it is called a \textit  {\color  {orange!70!black}normal space partitioning halfspace}.}}{145}{figure.caption.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10.2}VC-dimension}{145}{subsection.8.10.2}\protected@file@percent }
\@@wrindexm@m{main}{shattering|hyperpage}{145}
\oddpage@label{62}{145}
\citation{10.5555/2371238}
\citation{10.5555/2371238}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.10.1\else \numberline {8.10.1}Definition\fi \thmtformatoptarg {Shattering}}{146}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.10.2\else \numberline {8.10.2}Definition\fi \thmtformatoptarg {VC-dimension}}{146}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example~8.10.1\else \numberline {8.10.1}Example\fi \thmtformatoptarg {Intervals on the real line}}{146}{example.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces VC-dimension of intervals on the real line. (a) Any two points can be shattered. (b) No sample of three points can be shattered as the $(+,-,+)$ labelling cannot be realized. Taken from \cite  {10.5555/2371238}.}}{146}{figure.caption.62}\protected@file@percent }
\@writefile{loe}{\contentsline {example}{\ifthmt@listswap Example~8.10.2\else \numberline {8.10.2}Example\fi \thmtformatoptarg {Hyperplane}}{146}{example.2}\protected@file@percent }
\citation{10.5555/2371238}
\citation{10.5555/2371238}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Unrealizable dichotomies for four points using hyperplanes in $\mathbb  {R}^{2}$. (a) All four points lie on the convex hull. (b) Three points lie on the convex hull while the remaining point is interior. Taken from \cite  {10.5555/2371238}.}}{147}{figure.caption.63}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Illustration of convex and non-convex set. The segment $[x,y]$ must be fully contained in the region of the set, otherwise it is not convex.}}{148}{figure.caption.64}\protected@file@percent }
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma~8.10.1\else \numberline {8.10.1}Lemma\fi }{148}{lemma.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.10.3\else \numberline {8.10.3}Definition\fi \thmtformatoptarg {Convex hull}}{148}{definition.3}\protected@file@percent }
\@writefile{loe}{\contentsline {lemma}{\ifthmt@listswap Lemma~8.10.2\else \numberline {8.10.2}Lemma\fi }{149}{lemma.2}\protected@file@percent }
\newlabel{lem:lemma_cathedory_radon}{{8.10.2}{149}{}{lemma.2}{}}
\newlabel{eq:condition_radon1}{{8.75}{149}{}{equation.8.75}{}}
\newlabel{eq:condition_radon2}{{8.76}{149}{}{equation.8.76}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.10.3\else \numberline {8.10.3}Theorem\fi \thmtformatoptarg {Caratheodory}}{149}{theorem.3}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.10.4\else \numberline {8.10.4}Theorem\fi \thmtformatoptarg {Radon's theorem}}{151}{theorem.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Illustration of (left-hand side) $d=1$ Radon partition, and (middle and right-hand side) $d=2$ Radon partition. More options are available as $d$ increases.}}{151}{figure.caption.65}\protected@file@percent }
\newlabel{eq:radon1}{{8.92}{152}{VC-dimension}{equation.8.92}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~8.10.5\else \numberline {8.10.5}Theorem\fi \thmtformatoptarg {Sauer's lemma}}{152}{theorem.5}\protected@file@percent }
\@setckpt{content/chapter4}{
\setcounter{page}{154}
\setcounter{equation}{101}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{5}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{2}
\setcounter{chapter}{8}
\setcounter{section}{10}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{section@level}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{168}
\setcounter{lastsheet}{260}
\setcounter{lastpage}{246}
\setcounter{figure}{10}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{parnotemark}{1}
\setcounter{float@type}{8}
\setcounter{thmt@dummyctr}{160}
\setcounter{tabularnote}{0}
\setcounter{nicematrix_draft}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{77}
\setcounter{Hfootnote}{37}
\setcounter{bookmark@seq@number}{137}
\setcounter{memhycontfloat}{0}
\setcounter{mem@Hpagenote}{0}
\setcounter{definition}{3}
\setcounter{theorem}{5}
\setcounter{col}{0}
\setcounter{conjecture}{0}
\setcounter{setting}{0}
\setcounter{assume}{0}
\setcounter{hypothesis}{0}
\setcounter{axiom}{0}
\setcounter{question}{0}
\setcounter{example}{2}
\setcounter{note}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{155}
\setcounter{tabularnotesi}{0}
\setcounter{tabularnotes*i}{0}
}
