\chapter{Linear algebra}
Linear algebra is, perhaps, the golden horse of mathematics. In one way or another, it stands on par with calculus in the list of "top mathematical stuff that I don't want to learn" for almost everyone, but also the list of "top mathematical topics that are so necessary that I would have to read it either way.". Indeed, of mathematics, linear algebra is perhaps the most important field, yet requires not so much bargain on the mind to study something that is full of proof but not too abstract. 

Before learning something, it is best to ask the question of how useful it is for us to learn it, especially when the book is already bloated with contents, and the reader might as well only need to have a hand-waving knowledge of linear algebra. It is well-known of certain fact that linear algebra is particularly "hard". In some way or another, that is true, mostly because of the way it is interpreted and approach, or how the materials are presented. For example, a later topic that we will perhaps include (if there are times and spaces) is the concept of eigenvalue and eigenvectors of its theorem or properties. To prove such, most of the time (or most books) must define determinant to further their work for such proof. This is difficult, nonintuitive, and often derived without any sort of motivation just like how matrix is defined yet there exists no talks about what it actually represents every time you put your pen down and write the $3\times 3$ matrix (somehow, it is a \textit{linear transformation}). So to effectively learn something, we have to sort such things out, to worth the time that we will eventually spend learning this, and I myself writing this. 

Linear algebra in a rough sense can be said to be the study of functions on vectors. More specifically so, then it is the encoding, the language and methods of the general scaling of a high-dimensional space, depends on how you define dimensional, and is restricted to a class of such space called \textit{vector space}. Indeed, one feels pretty much natural extending objects to finite many dimensions (the case of infinitely many dimensions is studied in a sense, of \textit{functional analysis}). Other than that, it is also can be said to explore interactions in \textit{linearity sense} - of which you can only displace something on a straight line (I don't care if that straight line is oriented in any way, just get it straight), or just either shrink or extend it, again, linearly with respect to a straight line. In application sense, any system with a particularly large amount of dependencies, parameters, variables, et cetera, will benefit from the usage of linear algebra. Such is also said of encoding transformation, functions in which changes the state of certain system of interest, can also be represented in linear algebra. 

One of the main thing that is also good of linear algebra is that, most of the questions posed in an introductory course, can be answered in said introductory course. That is, it is relatively stable, unlike in number theory, or so, where questions as simple as the twin primes problem takes forever to solve. Or rather, linear algebra is \textit{thoroughly understood}, which makes it a very powerful tool for any kind of problems that can be represented neatly by it. for our cases, those applications will be apparent throughout the text. In fact, I recommend you to skip this section in a kind of binge-reading way - just browsing through. Whenever you see the application or usage, going back. 

With that said, I will approach the problem of linear algebra in a fairly different way. Or rather, not so much I guess. Because in the field we are researching, and the topic by itself has many utilizations and analogous notations or conventions on objects of linear algebraical properties, we would separate our investigation into two parts: the part on the \textit{representation}, encoding, the data structure in which we will represent our object; and the actual linear algebra itself. 
\section{Notation and structure}
Mathematics works in an encoding way. That is, even though we say that mathematics is abstract, as perhaps certainly it can take every values possible in existence of permission, as for all things to be spelled out, they need something to represent and express it \footnote{This is generally true. Even for the notion of infinity is often misunderstood in such case, as for the restriction of even the human brain there can be no infinite imagination, or infinite representation of an endless line of numbers. There are, usually, thought experiences using the notion of infiniteness, implicitly defined, and often spoke of ambiguity rather than it can be grounded in rigorous manner. On the other side of our argument, representation, encoding is everywhere to work on such abstract and arbitrary concept - the process of writing something down is one of said action, in the language of writing.}. In linear algebra, this notation is what trivially presented as the \textbf{array form}, and the special case of the array form used in linear algebra is \textbf{matrix}. 
\subsection{Array form}
Consider a large number of data, normally represented as a list of $x,y,b,qr,\dots,m$. This list can be either ordered or not. Enforce the specific encapsulation and positional ordering, we have the one-dimensional array: \footnote{We use \texttt{bmatrix}, the matrix form notation. However, usually, it is also presented by the normal square bracket with comma, such as: \begin{equation*}
    \mathrm{Ar} = [a_1, a_2, b_1, b_2, c_1, c_2,\dots]
\end{equation*}}
\begin{equation*}
    \begin{bmatrix}
        x& y & b & qr & \dots &m
    \end{bmatrix}
\end{equation*}
An array has the positional order of index for an index set $I$. Conventionally, this order is presented from left to right. So one can expect an array $A$ to take the form
\begin{equation*}
    A = \begin{bmatrix}
        a_1 & a_2 & a_2 & \dots & a_n
    \end{bmatrix}
\end{equation*}

For each element now is indexed by the subscript $i=1,2,3,\dots,n$. if you want, it is similar to a mask, such that for all $a\in A$, we have done a masking of data such that we have the pair $(1,x),(2,y),(3,qr)$ and so on for each $a$ of the respective ordering. Similarly, we can also present the two-dimensional array by adding another dimension for expansion: 

\begin{equation}
    A' = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \dots & a_{1n}\\
        a_{21} & a_{22} & a_{23} & \dots & a_{2n}\\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & a_{m3} & \dots & a_{mn}\\
    \end{bmatrix}
\end{equation}

In which now $(i,j)$ is limited in the range $(m,n)$ of the index. Even though we write this, normally, there are no restriction toward the absolute filling of the array. There, we have the definition of the \textit{uneven array}, which is specified to have different row counts and hence, different subsequently column counts, if we are to treat array just like a table. However, for generality, we can assume all of those uneven array are actually even, well-shaped array with null values. 

Even though we say dimension, the notation orientation does not care much of your general ideas. 

Here, the term \textit{dimensional} or then \textit{dimension} refers to the total index set specified to describe the positional ordering of elements inside an array structure. Before we continue, let's settle the convention. For each array, the index is written in the subscript of individual components. So, for one-dimensional, it's just $a_i$, for two, it is $a_{ij}$, three then $a_{ijk}$, and so on. All elements in an array $A$, in the abstract form will take similar element, the $a$s in the array notation. So with index and abstract form, we will just have $a_{i}$ and so on, not $a_i$, $b_{j}$ for $i\neq j$ everywhere. In the specified form (of values, for example), then the index is hidden, and the notation switch to their values. However, to specify or access certain elements, you will need to tell the index. Another thing to note is that simply speaking, geometrically, the dimension is simply the orthogonal axis, like in Cartesian coordinate system usually, that can be used to specify the index or positional order of particular element in the structure. 

Coming back to the dimension, this implies we can get to $n$-dimensional array, and that is perhaps correct. In a more mainstream machine learning language, this is called a \textbf{tensor}\footnote{In case of ambiguity, this tensor is not the tensor in tensor calculus, or any rigorous manner of tensor analysis. It is only a name to talk of multidimensional array of data only. The structure itself is again, only data structure - there are no intrinsic operational meanings embedded.}. An example of a three-dimensional array can be represented as a cube of values, for index $I^{3}\subset \mathbb{N}^{3}$, of the 3-tuple $(i,j,k)$ for each element. Though, this kind of notation can be presented in a lot of... well troublesome way, even if they are the same in essence. Mostly come down to notation abuse and illustration, though. 
\begin{center}
    \begin{tikzpicture}[auto matrix/.style={matrix of nodes,
        draw,thick,inner sep=0pt,
        nodes in empty cells,column sep=-0.2pt,row sep=-0.2pt,
        cells={nodes={minimum width=1.9em,minimum height=1.9em,
         draw,very thin,anchor=center,fill=white,
         execute at begin node={%
         $\vphantom{x_|}\ifnum\the\pgfmatrixcurrentrow<4
           \ifnum\the\pgfmatrixcurrentcolumn<4
            {#1}^{\the\pgfmatrixcurrentrow}_{\the\pgfmatrixcurrentcolumn}
           \else 
            \ifnum\the\pgfmatrixcurrentcolumn=5
             {#1}^{\the\pgfmatrixcurrentrow}_{N}
            \fi
           \fi
          \else
           \ifnum\the\pgfmatrixcurrentrow=5
            \ifnum\the\pgfmatrixcurrentcolumn<4
             {#1}^{T}_{\the\pgfmatrixcurrentcolumn}
            \else
             \ifnum\the\pgfmatrixcurrentcolumn=5
              {#1}^{T}_{N}
             \fi 
            \fi
           \fi
          \fi  
          \ifnum\the\pgfmatrixcurrentrow\the\pgfmatrixcurrentcolumn=14
           \cdots
          \fi
          \ifnum\the\pgfmatrixcurrentrow\the\pgfmatrixcurrentcolumn=41
           \vdots
          \fi
          \ifnum\the\pgfmatrixcurrentrow\the\pgfmatrixcurrentcolumn=44
           \ddots
          \fi$
          }
        }}}]
       \matrix[auto matrix=z,xshift=3em,yshift=3em](matz){
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
       };
       \matrix[auto matrix=y,xshift=1.5em,yshift=1.5em](maty){
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
       };
       \matrix[auto matrix=x](matx){
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
        & & & & \\
       };
       \draw[thick,-stealth] ([xshift=1ex]matx.south east) -- ([xshift=1ex]matz.south east)
        node[midway,below] {$D$};
       \draw[thick,-stealth] ([yshift=-1ex]matx.south west) -- 
        ([yshift=-1ex]matx.south east) node[midway,below] {joints};
       \draw[thick,-stealth] ([xshift=-1ex]matx.north west)
         -- ([xshift=-1ex]matx.south west) node[midway,above,rotate=90] {time};
      \end{tikzpicture}
\end{center}
For our special case, or a matrix, it is the two-dimension case. Most of the operations acting on matrix can be indeed generalized, though it is most of the time more effective to define it on such. And, without being said, matrix's operations and functions, as well as its uses, are more rigorous perhaps, than the others $n$-dimension, and from array analogue itself. 

We will now jump to the more interpretable object that linear algebra will use. While array representation has its own meaning and usage, it is often poorly equipped to be used and utilized in a mathematical sense, simply because, as we said, it is a data structure only with no added purpose or interpretation of its structure. The next section, we will deal with two fundamental objects of linear algebra - the \textit{vector} and \textit{matrix} object.

\section{Vectors}

For a mathematical object, there are components to specify it details or particular structure. In the context of linear algebra, we deal with the variable spaces, where each mathematical object can take place. Usually, the structure that give rises to such space, which differs from case-to-case basis of different fields, is called a \textbf{mathematical structure}, in an informal sense. Linear algebra then, deals with the question of working on multi-descriptive objects, where its mathematical structure is considered of the \textbf{specification} that is needed to describe the mathematical object respectively. For example, by then, we can roughly get the mathematical object of real number, in the structure $\mathbb{R}$, to be a zeroth-order object by linear algebraical means. That is, it can be specified using a singular "point" - its own quantification value. Then, for complex number of the form $x= a+ bi$, then it can be then considered a first-order object in such sense, simply because its parameter can be, informally, listed as one row or one column, and each contains the previous zeroth-order component. Such zeroth-order object, if they are of a specific field, for example, the field of real number $\mathbb{R}$, then we have a special name for it: a \textbf{scalar}\index{scalar} on the field $\mathbb{R}$. Then, for a first-order complex number space $\mathbb{C}$, we can then call it to be the two-dimensional space on the field $\mathbb{R}$. By such, we have considered the singular complex space $\mathbb{C}$ to contains \textbf{vectors}\index{vector} of which takes zeroth-order component over $\mathbb{R}$ - two of them, that is. 

The more nominal example that one can think of, and will be implied throughout the chapter, is the description of a \textit{directed object}, or a \textit{directed path}. For example, the direction of the displacement in space of a plane, the acceleration accompanied by a car, the direction of rotation of a geometrical shape, or the direction in which the gradient is, for the landscape of a function. Such cannot be described, usually, by one singular value. It needs to be of something with magnitude; and some expression of direction. This is fulfilled by vector itself. 

If we think only vectors and scalar as mathematical object but on the side of data representation, it is rather easy to see the intuitive necessity of such. Many things require more than just one value, or numerical representation to defines its mathematical form. Such can be said of many physical notions, from forces - if your world is not one-dimensional, that is; or velocities, accelerations, positions, directions, et cetera. Any such entity will require more than one specification, and often, it can be reached with the simple composition of many single values together. This, form the notion of vector as we have above. Additionally, we also note that again, we would be dealing with analytical, numerical objects. So, vectors and the like will be plenty of operations and actions on it, as well as the nuance accompanied. 

It is then, rather more detail for once, to talk of the more reduced notion of a vector. The more physical, deliberate definition hence specifies the vector by its apparent properties - an object of both \textit{magnitude} and \textit{direction}. Do note that this definition can be extended for the other interpretations, as we see below: 
\begin{definition}[Vector]
    A (nonzero) \textbf{vector} $\vec{v}$, or $\mathbf{v}$, is a quantity or object with both \textit{magnitude} and \textit{direction}. It is also expressed by its \textit{vector components}, of which we denote as $\vec{v}=(v_1, v_2,\dots,v_n)$, for all $v_i$, $i=1,\dots,n$ as the $i$th component of $\vec{v}$. The requirements that a vector and operations on it must satisfy, and the space to specify such vector constitute a \textbf{vector space}. 

    Geometrically, a vector $\mathbf{v}$ is specified also by its segment drawn from a point $P$ (called \textbf{initial point}) to point $Q$ (called \textbf{terminal point}). Its magnitude is then the length of the segment, denoted by $\lvert\lvert \vv{PQ}\rvert \rvert$, and its \textbf{direction} is the same as of the directed line segment. The \textbf{zero vector} is the generalization of a point, denoted $\mathbf{0}$.
\end{definition}

We will leave the vector space and structure to later section on the mathematical structure of linear algebra itself. For now, we would consider the vector in its sense of a data structure, somewhat. Our notation of a vector is then pretty similar to the first-order array, or one-dimensional array in previous section. A vector $\mathbf{v}$ is hence presented as: 
\begin{equation}
    \mathbf{v} = \begin{pmatrix}
        v_{1} & v_{2} & v_{3} & \dots & v_{n}
    \end{pmatrix}
\end{equation}

or, in a vertical form, 

\begin{equation}
    \mathbf{v} = \begin{pmatrix}
        v_{1} \\
        v_{2} \\ 
        v_{3} \\
        \vdots \\
        v_{n} 
    \end{pmatrix}
\end{equation}

Notationally, they are called the \textbf{row vector} and the \textbf{column vector}. For now the differences are irrelevant - they specify the same object, and the same vector. The subtleties only comes when we consider certain interpretation and structure with the 'row' and the 'column' - which hints at matrix. 

Under such context, the \textit{dimension} is the length of the component description, or the number of component that a vector has. If a vector is in the special case where its value is zero (we work on maths, and it is numerical), then we say that it is t\textit{he $i$th-flatten vector over $n$-space}, where $n$ is its dimension. 

For vectors, there are certainly some special vectors. The \textbf{zero vector}, denoted $\mathbf{0}$ or $\vec{0}$, is the conception of singular free point, with respect to any particular reference coordinate frame. That is, a singular special point called the \textbf{origin} that has zero magnitude and zero direction. Similarly, there is the \textbf{unit vector}, denoted $\mathbf{1}$ or $\vec{1}$, or $\hat{i},\hat{j},\dots$ of lower-case characters, is defined such that $\lvert \mathbf{1}\rvert = 1$ for any given direction. A collection of unit vectors by specific conditions form the \textbf{coordinate system}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{img/vector1.png}
    \caption{An illustration of a vector in two-dimensional vector form in endpoints representation, and directional-magnitude representation.}
    \label{fig:vector1}
\end{figure}

There is a subtle detail in considering the notation for vector. One can see that the notation we used to represent vector is oddly similar to the notation used for $n$-dimensional points in an $n$-space, for example, the Euclidean space $\mathbb{R}^{3}$, which each point can be specified to be $(x,y,z)$ along each axis. Indeed, this is troublesome: if they are mixed up together, it would be difficult to distinguish the notation for coordinate system and one for the vector representation, especially in case of complex manipulation. We can offer certain solutions to this problem. The first one, is to re-interpret coordinate in a space just as similar as vectors - every point is actually a vector starting from the origin, travelling to the point itself. In such sense, a vector is considered like a path composition with respect to each axis, representing the total component path taken to specify the location of such point. Another way to think about a vector is that it is a \textit{compressed form}\index{compressed form} of two point specification - the \textit{starting point} $A_{s}$ and the \textit{endpoint} $B_{e}$, though often we would call them all together as endpoints. Then, a vector is the reduced representation for all pairs of endpoint such that, the vector is defined to be the final absolute path - or in terms of \textit{calculus of variations} sense, the shortest path - a straight line - toward the two endpoints. Any straight line, with direction that satisfies the component path from one endpoint to another is indeed, that one vector; or simply speaking. The direction is characterized by indeed, the order of the path, either from $A$ to $B$ or $B$ to $A$, and also based off their intrinsic location. In such representation, we separate the endpoints from the vectors - an endpoint has no direction and is dimensionless, stationary, while a vector specifies a path from two endpoints together, of the total difference between the two endpoints. In the end, though it is based in the interpretation of the case study, and what structures are considered. In a more advanced situation, one can also specify a point scalar as an element of a manifold, and a vector as an element of the tangent space to such manifold. 

Two vectors are equal if the components used to specify them are the same. Geometrically, it means they have the same magnitude and direction. \footnote{Settle aside the expression of component for a vector, usually, we can get a lot of the properties and actions on vector using only their intrinsic, geometrical interpretation, that is, the magnitude-direction expression for a vector (as illustrated in Figure~\ref{fig:vector1}). Previously, we have said that anything of certain magnitude and direction, or path, between two endpoints, is the same vector. As we are concerned of here, it is true that only the magnitude and direction of the vector are significant; hence consequently, we regard vectors with the same magnitude and direction as being equal irrespective of their position. This is true for the geometry of a vector, as we guarantee.}
\begin{definition}
    Two nonzero vectors are \textbf{equal} if they have the same magnitude and the same direction. Any direction with zero magnitude is equal to the zero vector. 
\end{definition}

Although we defined zero vector and equal magnitude, we still do not know for sure what can be used to make such definition operational. To tackle this, we have to define the notion of vector length, in the respective reference frame, and of respective measure, for example, in $\mathbb{R}$ or $\mathbb{Z}$. 

Recalling that the distance between two points $P(x_{1},x_{2},x_{3})$ and $Q=(y_{1},y_{2},y_{3})$ in the Euclidean space $\mathbb{E}^{3}=\mathbb{R}^{3}$\footnote{This equivalency is more on convention than not. Some books will say it being the same, some books will regard Euclidean space of higher-dimension than real space. However, a property holds in those definitions is that Euclidean space is defined without special origin. Furthermore, consider the pure vector space $\mathbb{R}^{3}$. If we grant it the inner product, it becomes Euclidean since one of the requirement for a space to be Euclidean, is for Euclid's axiom to hold. That is why sometimes we call Euclidean space the special case of real vector space.} is such that
\begin{equation}
    d(P,Q) = \sqrt{(y_{1}-x_{1})^{2} + (y_{2}-x_{2})^{2} + (y_{3}-x_{3})^{2}}
\end{equation}
Using such, we define the following as the vector \textbf{magnitude}:
\begin{theorem}[Vector magnitude]
    For a vector $\mathbf{v}=(a,b,c)$ in $\mathbb{R}^3$, the magnitude of $\mathbf{v}$ is: 
    \begin{equation}
        \lvert\lvert \mathbf{v} \rvert\rvert = \sqrt{a^{2}+b^{2}+c^{2}}
    \end{equation}
\end{theorem}
This can be scaled up or down for any given $n$.
\begin{proof}
    By definition of vector, its magnitude is the length of the segment. Arbitrary specification of end-points (endpoint-independent) property gives $(a,b,c)$ the component length with respect to each dimension. The rest follows. Also notice we can prove by exhaustion for all permutation of choice of $a,b,c$. 
\end{proof}

We have been introduced to the notion of vector. Naturally, a question should arise: what can we do with it, and with many of it? Fortunately, we have options for such aspect. For vectors, operations to be investigated would be two operations, the addition operation and the scalar multiplication for the time being. Let's begin with addition.

\begin{theorem}[Vector sum]
    The \textbf{sum} of vectors $\mathbf{v},\mathbf{w}$, denoted by $\mathbf{v}+\mathbf{w}$ is obtained by translating $\mathbf{w}$ so that its initial point is at the terminal point of $\mathbf{v}$, the initial point of $\mathbf{v}+\mathbf{w}$ is the initial point of $\mathbf{v}$, and the endpoint is the new terminal point of $\mathbf{w}$.
\end{theorem}

In component form, we can get, 
\begin{equation}
    \mathbf{v} + \mathbf{w} = \begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{pmatrix}
    + \begin{pmatrix}
        y_{1}\\
        y_{2}\\
        \vdots\\
        y_{n}
    \end{pmatrix}
    =
    \begin{pmatrix}
        x_{1}+y_{1}\\
        x_{2}+y_{2}\\
        \vdots\\
        x_{n}+y_{n}
    \end{pmatrix}
\end{equation}
This operation only happens if they have the same dimension. However, we can mitigate this partially by the following convention. 
\begin{lemma}
    If $\mathbf{v}$ and $\mathbf{w}$ acts on each other where $\dim{(\mathbf{v})}=n\neq m = \dim{(\mathbf{w})}$, then: If $n>m$, $\mathbf{w}$ is extended of zero value in the additional dimension; if $n<m$, $\mathbf{v}$ is applied of the same.\footnote{We have not defined the vector space yet at this point. However, based on the notion of dimension described before, and considering the space of which we put on the dimension notion in to specify the vectors, this result is palpable. Nevertheless, if we can define a vector space, then it means either one contains the \textbf{vector subspace} of the other.}
\end{lemma}
The behaviour of this sum is intuitive - linking directed line altogether with each other. Because of this, we will have the following observation. 
\begin{theorem}[Parallelogram law]
    The sum of two vectors $x$ and $y$ that acts at the same point $P$ is the vector beginning at $P$ that is represented by the diagonal of parallelogram having $x$ and $y$ as adjacent sides. 
\end{theorem}
\begin{question}
    Can you realize the parallelogram law in analytical terms of its coordinate?
\end{question}
This can be illustrated in Figure~\ref{fig:vector_sum_parallel}. Do note that it is more or less, a not so trivial point. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{img/vector_sum-001.png}
    \caption{The \textbf{parallelogram law} for vector addition of two vectors $x$ and $y$ on adjacent side.}
    \label{fig:vector_sum_parallel}
\end{figure}
Aside from additive operation, vector can also be multiplied, albeit restricted to multiplying with a scalar $\lambda$ only \footnote{Since this is not a textbook, you should have noticed by now, if you have educations of linear algebra beforehand, that there exists the \textbf{vector product} of the special case of matrix multiplication. However, of such is usually not the standard interpretation of vector, and is rather specified with meanings than abstracted notion --- that is to say that the scope of the `vector' considered in this section is not that wide, yet.}. For this, of a vector $\mathbf{v}$, we define $\lambda\cdot \vec{v}$ as: 
\begin{equation}
    \lambda\mathbf{v} = 
    \lambda\begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \lambda x_{1}\\
        \lambda x_{2}\\
        \vdots\\
        \lambda x_{n}\\
    \end{pmatrix}, \quad \lambda \in \mathbb{R}
\end{equation}
The scalar is multiplied, component-wise, for each element of the vector. Non-algebraically, this is effectively the action of scaling with respect to direction of a given vector by $\lambda$ amount(s). By now, you might also have noticed that we switch between analytical (algebraical, if you like) and geometrical description, and this is not unnatural, and sometimes offer understanding that complements the other method. 
\begin{definition}[Scalar, multiplication]
    We define a \textbf{scalar} as a quantity equal to a single component, and geometrically a point\footnote{Interesting fact. There are two entries on which the word scalar was used. The word scalar derives from the Latin word scalaris, an adjectival form of scala (Latin for "ladder"). The first use is in François Viète's Analytic Art (In artem analyticem isagoge) (1591), where it is written that "Magnitudes that ascend or descend proportionally in keeping with their nature from one kind to another may be called scalar terms.". Another one is by William Rowan in the 19th century, to convey the sense of something that could be represented by a point on a scale or graduated ruler.}. For a scalar $k$ and a nonzero vector $\mathbf{v}$, the \textbf{scalar multiple} of $\mathbf{v}$ by $k$, denoted by $k\mathbf{v}$, is the vector whose magnitude is $\lvert k\rvert \lvert \lvert \mathbf{v}\rvert\rvert $, same direction as $\mathbf{v}$ for $k>0$ and reverse if $k<0$, and is the zero vector if $k=0$. For the zero vector $\mathbf{0}$, we define $k\mathbf{0}=\mathbf{0}$ for any scalar $k$. 
\end{definition}

Using our analytical formalism, we can try to prove those operations. Indeed, let's see how that might potentially work. 
\begin{theorem}
    Let $\mathbf{v}=(v_{1},v_{2},v_{3})$, $\mathbf{w}=(w_{1},w_{2},w_{3})$ be vectors in $\mathbb{R}^{3}$, let $k$ be a scalar. Then: 
    \begin{enumerate}[itemsep=1pt,topsep=0pt]
        \item $k\mathbf{v}=(kv_{1},k v_{2})$. 
        \item $\mathbf{v}+\mathbf{w}=(v_{1}+w_{1},v_{1}+w_{3})$. 
    \end{enumerate}
\end{theorem}
\begin{proof}
    (a) We would be using basic geometry for such purpose of assumptions for further proofs. Without loss of generality, we assume that $v_{1},v_{2}>0$ (the other possibilities are handled in a similar manner). If $k\neq 0$, then $(kv_{1},kv_{2})$ lies on a line with slope $kv_{2}/kv_{1}=v_{2}/v_{1}$, which is the same as the slope of the line on which $\mathbf{v}$ (and hence $k\mathbf{v}$ lies), and $(kv_{1},kv_{2})$ points in the same direction on the line as $k\mathbf{v}$. Furthermore, 
    \begin{equation}
        \lvert (kv_{1},kv_{2}) \rvert = \sqrt{(kv_{1})^{2}+(kv_{2}^{2})} = \sqrt{k^{2}v_{1}^{2} + k^{2}v_{2}^{2}} = \sqrt{k^{2}(v^{2}_{1}+ v_{2}^{2})} = \rvert k \lvert \sqrt{v_{1}^{2}+v_{2}^{2}} = \lvert k \rvert \lvert \lvert \mathbf{v}\rvert \rvert 
    \end{equation}
    This indicates that they have the same magnitude and direction. This concludes the proof. 

    (b) Without loss of generality, we assume that $v_{1},v_{2},w_{1},w_{2}>0$. We see that when translating $\mathbf{w}$ to start at the end of $\mathbf{v}$, the new terminal point of $\mathbf{w}$ is $(v_{1}+w_{1},v_{2}+w_{2})$, so by definition of $\mathbf{v}+\mathbf{w}$ this must be the terminal point of it. This concludes the proof.  
\end{proof}

\subsection{Properties of vectors}
With a lot of objects in mathematics, they can always be described in mainly two ways: either analytically (algebraically) or geometrically. Let's investigate the geometrical view first. 
\subsubsection{Interpretation of references}
A vector can be considered to be a \textit{free object} in the geometrical space. That is, it is not bounded by coordinates likes points, or geometrical structure embedded within the space by specification. Rather, it is free in the sense that the vector is similar to itself, by means of linear translation throughout the geometrical space, for example, moving the vector left or right, up or down. Analytically, it means that a vector description refers to its designed specification, and not positional descriptions. What does this mean? I would want to say it in terms of some rigours, but certainly, it depends on the interpretation, at the foremost aspect, but that would be pretty much useless. For my own understanding, one can regard the description of a coordinate $(x,y,z)$ and a vector $x(x_1, x_2, x_3)$ by some subtleties, independent or dependent on the coordinate basis or not. A coordinate is dependent of the coordinate system, or the space specification, or the descriptors, because its component depends on each of the component space. Or, rather, it depends on the origin where everything is referenced from. Vector, on the other hand, use the notation a bit differently. Sure, it is still the path, or rather, if we are to treat it similarly to the indexing of an array, then it is similar to the array index to recognize an object's position. However, the descriptor is not taking positions or index, it is taking the relatively speaking, \textit{path, length} instead. Or rather, it is the compression of two values, just as we spoke up there. So each vector is actually representing: 
\begin{equation}
    \mathbf{x} = (x_{1},x_{2},x_{3},\dots,x_{n}) = (x_{1e}- x_{1b}, x_{2e}- x_{2b},\dots, x_{ne}- x_{nb}) 
\end{equation}
where the subscript $ie, ib$ is the $i$th component's end point and begin point. Now, I have to admit I don't know how to get this to be further than it is. Then, we would have the coordinate system using the pretty much special case of this system, \footnote{So, yeah, same notation, different interpretation. Especially when you consider that you can, and indeed would likely want to write them down together. Though, because of this, most of the time, we would consider them to be relatively the same, because as we said, we can translate them down to anywhere, as long as the descriptor, which specifies the direction, magnitude, et cetera, stays the same - which includes the origin point. }
\begin{equation}
    x = (x_{1},x_{2},x_{3},\dots,x_{n}) = (x_{1e}- 0, x_{2e}- 0,\dots, x_{ne}- 0) 
\end{equation}
\subsubsection{Basic vector algebra}
Moving on, for basic vector operations, we have the following. 
\begin{theorem}[Vector algebra]
    For any vectors $\mathbf{u},\mathbf{v},\mathbf{w}$, and scalar $k,l$, we have: 
    \begin{enumerate}[topsep=0.5pt,itemsep=1pt,label=(\emph{\alph*})]
        \item $\mathbf{v}+\mathbf{w}=\mathbf{w}+\mathbf{v}$ (Commutative Law)
        \item $\mathbf{u}+(\mathbf{v}+\mathbf{u})=(\mathbf{u}+\mathbf{v})+\mathbf{w}$ (Associative Law)
        \item $\mathbf{v}+\mathbf{0}=\mathbf{v}=\mathbf{0}+\mathbf{v}$ (Additive identity)
        \item $\mathbf{v}+(-\mathbf{v})=\mathbf{0}$ (Additive Inverse)
        \item $k(l\mathbf{v})=(kl)\mathbf{v}$ (Associative law for scalar multiplication)
        \item $k(\mathbf{v}+\mathbf{w})=k\mathbf{v}+k\mathbf{w}$ (Distributive law)
        \item $(k+l)\mathbf{v}=k\mathbf{v}+l\mathbf{v}$ (Distributive law)
    \end{enumerate}
\end{theorem}
\begin{proof}
    Proved using algebraic manipulation or geometrical proofs. 
\end{proof}
Notice that using the commutative law, we gain the parallelogram law proof, since it permits the motion of attaching different vector to the end of each other. For $n$ vectors apparent, any 2-permutation resolves the same theorem. 
\subsubsection{The `missing' operations}
As far as elementary operations are concerned, we notice some operations that `seem to be missing'. That are the \textit{vector-vector multiplication} and \textit{scalar-vector addition}. We can multiply and add scalar to scalar, multiply scalar to vector, add vector to vector, but not the other two. 

\section{Matrix}

Matrix is the special type of array that is used in construction of linear algebra. Specifically, it is array, in special consideration, and added structures. Mathematically, it is defined as followed. 
\begin{definition}[Matrix]
    A matrix $A$ is a two-dimensional array, with dimensions' size of $m\times n$ for $m$ is the horizontal dimension (the column) and $n$ the vertical dimension (the column). \footnote{This is just a normal convention.}. For each element $a\in A$ of matrix $A$, it is indexed by $a_{ij}$ for $1\leq i \leq m$ and $1\leq j \leq n$. It is represented as: 
    \begin{equation}
        A = \begin{pmatrix}
            a_{11} & a_{12} & a_{13} & \dots & a_{1n}\\
            a_{21} & a_{22} & a_{23} & \dots & a_{2n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & a_{m3} & \dots & a_{mn}\\
        \end{pmatrix}
    \end{equation}
\end{definition}

The bracket is pretty much cosmetic, though, to distinguish from the use of array and matrix, we will use $[\cdot]$ for array and $(\cdot)$ for matrices. 

Given such, what can we do with matrices? First, we can \textit{add} them together; given $A,B$ being two matrices, then $A+B$, the \textit{matrix addition} is defined as: 
\begin{equation}
A + B = \begin{pmatrix}
            a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13}& \dots & a_{1n}+ b_{1n}\\
            a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23}& \dots & a_{2n}+ b_{2n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            a_{m1} + b_{m1} & a_{m2} + b_{m2} & a_{m3} + b_{m3}& \dots & a_{mn}+ b_{mn}\\
        \end{pmatrix}
\end{equation}
Furthermore, \textit{scalar multiplication} to a matrix is also available, and $\lambda A$ is defined as: 
\begin{equation}
    \lambda A = \begin{pmatrix}
            \lambda a_{11} & \lambda a_{12} & \lambda a_{13} & \dots & \lambda a_{1n}\\
            \lambda a_{21} & \lambda a_{22} & \lambda a_{23} & \dots & \lambda a_{2n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            \lambda a_{m1} & \lambda a_{m2} & \lambda a_{m3} & \dots & \lambda a_{mn}\\
        \end{pmatrix}
\end{equation}
For multiplication, the simplest operation that can be realized from component-based multiplication is the \textbf{Hadamard multiplication} or component based multiplication, denoted as $A\odot B$, defined as: 
\begin{equation}
    A\odot B = \begin{pmatrix}
            a_{11} \odot b_{11} & a_{12} \odot b_{12} & a_{13} \odot b_{13}& \dots & a_{1n}\odot b_{1n}\\
            a_{21} \odot b_{21} & a_{22} \odot b_{22} & a_{23} \odot b_{23}& \dots & a_{2n}\odot b_{2n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            a_{m1} \odot b_{m1} & a_{m2} \odot b_{m2} & a_{m3} \odot b_{m3}& \dots & a_{mn}\odot b_{mn}\\
        \end{pmatrix}
\end{equation} 
\begin{remark}
    Let $\mathbf{A}=diag(a_{1},\dots,a_{n})$ in $\mathbb{R}^{m\times n}$, and $\mathbf{B}\in\mathbb{R}^{n\times k}$. Define also a vector $\mathbf{a}=(a_{1},\dots,a_{n})^{\top}\in\mathbb{R}^{n}$, which represents the diagonal of $\mathbf{A}$. Then $$\mathbf{AB}=[\mathbf{a}\dots \mathbf{a}]\circ \mathbf{B}$$
The former takes $\mathcal{O}(n^{2}k)$ operations, while the latter takes only $\mathcal{O}(nk)$ operations, which is one magnitude faster. 
\end{remark}
All of our operations up to this point has been with two matrices $A,B$ of the same shape, that is, $m\times n$ for both. What happens if they are now then different? Either we can justify the operation by `extending it', however, usually, this requires discrete specific description for that to be able to be considered. Hence, matrix-matrix operations like such above is restricted of the entry shape. 

Finally, we have one more important operation to be defined here at last. For two matrices $A,B$ of shape $m\times n$ and $n\times p$, the \textbf{matrix multiplication} operation that outputs an $m\times p$ matrix, is available as: 
\begin{equation}
    A\times B = \begin{pmatrix}
        a_{11} & \dots & a_{1n} \\
        a_{21} & \dots & a_{2n} \\
        \vdots & \ddots & \vdots \\
        a_{m1} & \dots & a_{mn}
    \end{pmatrix}
    \begin{pmatrix}
        b_{11} & \dots & b_{1p} \\
        b_{21} & \dots & b_{2p} \\
        \vdots & \ddots & \vdots \\
        b_{n1} & \dots & b_{np}
    \end{pmatrix} = 
    \begin{pmatrix}
        c_{11} & \dots & c_{1p} \\
        c_{21} & \dots & c_{2p} \\
        \vdots & \ddots & \vdots \\
        c_{m1} & \dots & c_{mp}
    \end{pmatrix}
\end{equation}
where \begin{equation}
    c_{ik} = \sum_{j<n} a_{ij}b_{jk}
\end{equation}
For certain reason in the theory, or \textbf{matrix theory} and linear algebra in general, the matrix multiplication is explicitly defined only for such shape configuration. Otherwise, the matrix multiplication simply does not exist.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{img/jjgh.jpg}
    \caption{Illustration of matrix multiplication. Taken from \cite{ANDRILLI20101}.}
    \label{fig:matmuls}
\end{figure}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{img/matmulss.png}
    \caption{Illustration of matrix multiplication for $2\times 2$ shape.}
    \label{fig:matmuls2}
\end{figure}
\subsection{Special matrices, operations, and properties}
Matrix has quite a few of special types that is defined for either decomposing the many matrices, or to complement them in expression. Operations aside from basic `interactive operations' as above is also available, most of the time acting on the matrix by itself. Those operations, special matrices and properties are there to then draw out reasonable consequences and results. 
\subsubsection{Matrix shape}
Informally, A \textbf{matrix} is a rectangular array of numbers arranged in rows and columns. We say that a matrix $\mathbf{A}\in \mathbb{R}^{m\times n}$ is (given the entry field $\mathbb{R}$)
\begin{enumerate}[topsep=1pt,itemsep=1pt]
    \item A square matrix if $m=n$. 
    \item A long matrix if $m<n$
    \item A tall matrix if $m>n$
\end{enumerate}
A \textbf{diagonal} matrix is a square matrix $\mathbf{A}\in \mathbb{R}^{n\times n}$ whose off diagonal entries are all zero, so $a_{ij}=0$ for all $i\neq j$: 
\begin{equation}
    \mathbf{A}=\begin{pmatrix}
a_{11}  &  &  &  \\
 & \ddots  &  &  \\
 &  &  & a_{nn}
\end{pmatrix}
\end{equation}
A diagonal matrix is uniquely defined through a vector that contains all the diagonal entries, and denoted as follows:
\begin{equation}
    \mathbf{A}=\mathrm{diag}(1,2,\dots,n)=\mathrm{diag}(\mathbf{a})
\end{equation}
Using the diagonal matrix, we gain the following result. 
\begin{proposition}[Diagonal multiplication]
    Given two matrices $A,B$ of compatible shape. Then, if one is a diagonal matrix, denoted $A_{diag},B_{diag}$, then the following is true. 
    \begin{equation}
        A_{diag}\mathbf{B}=\begin{pmatrix}
a_{1}  &  &  &  \\
 & \ddots  &  &  \\
 &  &  & a_{n}
\end{pmatrix}\begin{pmatrix}
\mathbf{B}(1,:) \\
\vdots \\
\mathbf{B}(n,:)
\end{pmatrix}=\begin{pmatrix}
a_{1}\mathbf{B}(1,:) \\
\vdots \\
a_{2}\mathbf{B}(2,:)
\end{pmatrix}
    \end{equation}
and 
\begin{equation}
    \mathbf{A}B_{diag}=\begin{pmatrix}
\mathbf{A}(1,:) \\
\vdots \\
\mathbf{A}(n,:)
\end{pmatrix}\begin{pmatrix}
b_{1}  &  &  &  \\
 & \ddots  &  &  \\
 &  &  & b_{n}
\end{pmatrix}=\begin{bmatrix}
b_{1}\mathbf{A}(:,1)\dots b_{N}\mathbf{A}(:,n)
\end{bmatrix}
\end{equation}
\end{proposition}

The two unit matrices that is often seen is the \textbf{identity} and \textbf{zero} matrix. Given the name, $\mathbf{I}$ is used to denote the identity matrix: $$\mathbf{I}=\begin{bmatrix}
1 & 1 & \dots & 1 \\
1 & 1 & \dots & 1 \\
\vdots  & \vdots  & \ddots  & \vdots \\
1 & 1  & \dots & 1
\end{bmatrix}$$
and $\mathbf{O}$ for the zero matrix, 
$$\mathbf{O}=\begin{bmatrix}
0 & 0 & \dots & 0 \\
0 & 0 & \dots & 0 \\
\vdots  & \vdots  & \ddots  & \vdots \\
0 & 0  & \dots & 0
\end{bmatrix}$$
with their sizes implied by the context, entries of all $1$ or $0$, respectively. 

\subsubsection{Transpose, inverse, trace and rank}
Given a matrix $A$, then $A^{\top}$ is called the \textbf{transpose} of $A$ for its shape reversed, $A_{m\times n}$ to $A'_{n\times m}$ with $b_{ij}=a_{ij}$ for all $i,j$. A square matrix $\mathbf{A}_{n\times n}$ is said to be symmetric if $\mathbf{A}^{\top}=\mathbf{A}$. We have the following result. 
\begin{proposition}
    Let $\mathbf{A}$ be a matrix of size $m\times n$, $\mathbf{B}$ be a matrix of size $n\times p$. Then, 
    \begin{enumerate}[itemsep=1pt,topsep=1pt]
        \item $(\mathbf{A}^{\top})^{\top}=\mathbf{A}$.
        \item $(k\mathbf{A})^{\top}=k\mathbf{A}^{\top}$
        \item $(\mathbf{A}+\mathbf{B})^{\top}=\mathbf{A}^{\top}+\mathbf{B}^{\top}$
        \item $(\mathbf{AB})^{\top}=\mathbf{B}^{\top}\mathbf{A}^{\top}$
    \end{enumerate}
\end{proposition}
If we consider next the notion of the \textbf{inverse} of a matrix, the true, classical inverse definition works only for square matrix, by definition. 
\begin{definition}[Matrix inverse]
    A \textit{square matrix} $\mathbf{A}\in\mathbb{R}^{n\times n}$ is said to be invertible if there exists another square matrix of the same size $\mathbf{B}$ such that $\mathbf{AB}=\mathbf{BA}=\mathbf{I}$. 
\end{definition}
In this case, $\mathbf{B}$ is called the matrix inverse of $\mathbf{A}$ and denoted as $\mathbf{B}=\mathbf{A}^{-1}$. We have the following property: 
\begin{proposition}
    Let $\mathbf{A},\mathbf{B}$ be two invertible matrices of the same size, and $k\neq 0$. Then 
    \begin{align}
        (k\mathbf{A})^{-1} & = \frac{1}{2}\mathbf{A}^{-1}\\
        (\mathbf{AB})^{-1} & = \mathbf{B}^{-1}\mathbf{A}^{-1}\\
        (\mathbf{A}^{\top})^{-1} & = (\mathbf{A}^{-1})^{\top}
    \end{align}
\end{proposition}
The \textbf{trace} of a square matrix $A\in \mathbb{R}^{n\times n}$ is defined as the sum of the entries in its diagonal, such that: 
\begin{equation}
    \mathrm{trace}(A)=\sum_{i}a_{ii}
\end{equation}
We sometimes denote it as $\mathrm{Tr}(A)$. Clearly, $\mathrm{Tr}(A)=\mathrm{Tr}(A^{T})$. Trace is a \textbf{linear} operator, so $\mathrm{Tr}(kA)=k\mathrm{Tr}(A)$ and $$\mathrm{Tr}(A+B)=\mathrm{Tr}(A)+\mathrm{Tr}(B)$$ If $A$ is an $m\times n$ matrix and $B$ is an $n\times m$ matrix then $$\mathrm{Tr}(AB)=\mathrm{Tr}(BA)$$ Note that as matrices, $AB$ is not necessarily equal to $BA$. 

Continuing, for a matrix $A$, the largest number of linearly independent rows (or columns) contained in the matrix is called the rank of $A$, denoted $rank(A)$. 

A square matrix $P\in\mathbb{R}^{n\times n}$ is said to be of full rank, or \textit{nonsingular}, if $rank(P)=n$, otherwise, it is said to be rank deficient (or \textit{singular}). A rectangular matrix $A\in\mathbb{R}^{m\times n}$ is said to have full column rank if $rank(B)=n$. Similarly, a rectangular matrix $A\in\mathbb{R}^{m\times n}$ is said to have full row rank if $rank(B)=m$. 

