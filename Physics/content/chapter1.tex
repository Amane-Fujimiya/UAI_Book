\chapter{Introduction to Artificial Intelligence}
This book is concerned with mainly the matter of \textbf{artificial intelligence}. This is a very broad term, a very misunderstood notion, and a very fruitful endearing venture. 

Historically, artificial intelligence begins with the question of rethinking the origin of rationality: what gave rise to the rational process, the logical thoughts? Partaking on this problem in history was mostly in the field of the \textit{philosophy of mind}, which is prominent during the Greek era of philosophical boom, the Renaissance, and the overall European intervention to the topic. For around almost all the existence of humanity, this issue, phrased and approached differently from the view of the human mind and body, philosophy dominated in the search for the origin of intelligence, even though said word is not even clearly defined, and is historically put in a much lower place than other notions or questions regarding the same concept. Only until the dawn of the 20th century, where the new computer has finally grown to be substantially more useful than previously primitive computer has been, that the change shifted from philosophical debate with no ends, to actual reverse engineering and implementation of similar type machines. Furthermore, with the advancements of technology and the Technological Revolution of the 18th century, neurology and other medical analysis field finally took part in discovering one's self - by accessing what is deemed the most important of separating man from the apes. And with that, comes the term \textit{artificial intelligence} in its form. 

This chapter serves as the introductory session to the notion of artificial intelligence. By doing this, we would look at the loose picture of the conceptual ideas, the philosophical approach to defining artificial intelligence, questions regarding what is intelligence and artificial as a whole, and a variety of debates surrounding the notion as it is. By doing this, perhaps we can come off to a conclusion on what the book is about, what it was aimed for, and what question would it partake and attempt to solve in the later section. For now, however, we take a role as the historian. Many have tried before, and many have failed before. So what is the history of the term artificial intelligence as a whole, and what comes even before it? 

\section{History of artificial intelligence}

If we have somehow settled on the notion and importance of artificial intelligence in the question of finding intelligence, it is then imperative that we find for it a formal treatment and perhaps an entire field of research dedicated to structuring the underlying system of intelligence. The birth of artificial intelligence as a field, with the supporting argument using \textit{computation system}, or computer in simplicity, started in a workshop of 1956, the \textit{Dartmouth workshop}. 

\blockquote{We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.}

And with this, came officially the field of artificial intelligence, in the formal sense. 

For this historical account, we would like to take the inquiry mostly from \cite{AIWashington}, for a more detail analysis. More so can be found in \textit{Artificial Intelligence - A modern approach} \cite{10.5555/1671238}. More authoritative sources and literatures can be found and potentially replace most of the following section. Though I doubt about the brief historical account that it is necessary in doing so. Well, perhaps it is. It is recommended in such section, to review the work \textit{The Quest for Artificial Intelligence (2010)} by Nils J. Nilsson, \textit{Artificial Intelligence, Foundations of Computational Agents} by David L. Poole and Alan K. Machworth. 
\subsection{Roughly, artificial intelligence}

The term artificial intelligence was first coined by John McCarthy in 1956, when he held the first academic conference on the subject. Though,argubaly, the issues were discussed, as the journey went far and wide before then, even in the time of Descartes in the 1700s when he conjectured the intelligent machine's existence. More recently, in Vannevar Bush's \textit{As We May Think (July 1945)}, widely recognized to be visionary in some way or another, he proposed a system which amplifies people's own knowledge and understanding, with the aspect of machine assistance. Five years later, Alan Turing wrote a paper on the notion of machines being able to simulate human beings and the ability to do intelligent things, such as playing chess. 

However one might want to say about intelligence and qualities of such, it is not of a single doubt that this endeavour has begun very soon, ever since the debate on the philosophical idea of what constitute the mind, the conjectures between knowledges and what constitutes as true knowledge, the various machines such that can say to lift the burden of the mortal human - promptly the industrial revolution, and the wishful thinking of reproducing one's self. Artificial intelligence now solely belongs to the landscape of the most powerful non-human structure we would have know, that is capable of a fraction of the abstract thoughts, being computers. 

No one can refute a computer's ability to process logic. After all, we create it based on logic and components that is configured in logical sense. But to many, it is unknown, and misrepresented, too, that it can host the entity that can \textit{think}. The dichotomy of defining the precise definition of think here is important, because there has been some strong opposition as to whether or not this notion is even possible. In the end, the definition of artificial intelligence is perhaps one thing to desire of. Yet we shall then not define it in its entirety, because as later will be discussed, to define artificial intelligence requires separating `artificial' and `intelligence'. As of the face of the Earth when those words are written, no one has been ever close to finding out the truth. 
\subsection{The themes of AI}
The main advances over the past sixty years have been advances in search algorithms, machine learning algorithms, and integrating statistical analysis in to understanding the world at large. However, most of the breakthroughs in AI aren't noticeable to most people (figured, they are not so well-educated on the subject). Rather than talking machines in movies, AI is used in more subtle ways such as examining purchase histories and influence marketing decisions more than not. 

What most people think of as `true AI' hasn't experienced rapid progress over the decades. A common theme in the field has been to overestimate the difficulty of foundational problems. Significant AI breakthroughs have been promised in 10 years or less, begins earlier than the time when Marvin Minsky's words of having human-level intelligent machine in 'fewer than 6 years' was cited. In addition, there is a tendency to redefine what intelligent means after machines have mastered an area or problem, which is troublesome to specifically correct what can be called intelligence. 

In the field of AI, expectations seems to always outpace the reality. After decades of research, no computer has come close to pass the Turing Test, a figurative framework for defining and measuring intelligence, of a given system. Expert System have grown but have not become as common as human experts; and while we've built software that can beat humans at some games, open-ended games are still far from the mastery of computers. Is the problem simply that we haven't focused enough resources on basic research, as it created two AI winters historically, or is the complexity of AI one that we haven't yet come to grasp yet? And instead, like in the case of proposing playing chess, we focus much more on specialized problems, rather than understanding the notion of understanding? Perhaps, that is the case, that somehow diluted ourselves from seeing the real picture, by masking the false one with expectation to the real alignment. 

\subsection{Precursor - From the ancient time to 18th century}

Throughout human history, people have used technology to model themselves. There is evidence of this from ancient China, Egypt, and Greece, bearing witness to the universality of this activity. Each new technology has, in its turn, been exploited to build intelligent agents or models of mind. 

Throughout the history, the pursuit of intelligent model and the process of rationalism of the mind has been pursued by directly study it - or the \textit{philosophy of mind}; and as you suspected, it is mostly dominated by philosophical research. In fact, the amount of backlogs and historical researches specifically in this type of philosophy, engulfed quite a lot of paperworks - more so than one should be able to consume in a year's calendar of afternoon tea time, that is. Perhaps it is more apparent to realize that the concept of \textit{duality} - that is, the mind and the body is dual but inseparable part of being human, or its existence thereof, is also from such study. Well, it is best to say that if we stick too much to such philosophical form, this book might have never been written. 

In a more diverged and recent years, Hobbes (1588-1679), who has been described by Haugeland as the "Grandfather of AI", espoused the position that thinking was symbolic reasoning, like talking out loud or working out an answer with pen and paper. The idea of symbolic reasoning was further developed by Descartes, Pascal, Spinoza, Leibniz, and others who were pioneers in the European philosophy of mind. The idea of symbolic operations became more concrete with the development of computers, and sooner or later, was tested out of light. 

It is also at this point that I would also like to note about the altitude toward the philosophy of mind and its branches of analysis. While it is true, that sticking too much to said philosophy will dilute yourselves of the fundamental problems to solve about intelligence and the latter term sticking the word artificial in, there has always been the priori concepts and illustrations to the problem by itself. That comes of as nothing more but the philosophy of the old, in which even now we utilize. Perhaps more believable, is from the word of Dreyfus: 
\blockquote[Hubert L. Dreyfus]{As I studied the RAND papers and memos, I found to my surprise that, far from replacing philosophy, the pioneers in CS had learned a lot, directly and indirectly from the philosophers. They had taken over Hobbes' claim that reasoning was calculating, Descartes' mental representations, Leibniz's idea of a “universal characteristic” - a set of primitives in which all knowledge could be expressed, -- Kant's claim that concepts were rules, Frege's formalization of such rules, and Russell's postulation of logical atoms as the building blocks of reality. In short, without realizing it, AI researchers were hard at work turning rationalist philosophy into a research program.}

Though, he also directly argued that this might be more catastrophic than not to the AI researchers. 
\blockquote[Hubert L. Dreyfus, Heideggerian]{At the same time, I began to suspect that the critical insights formulated in existentialist armchairs, especially Heidegger's and Merleau-Ponty's, were bad news for those working in AI laboratories-- that, by combining rationalism, representationalism, conceptualism, formalism, and logical atomism into a research program, AI researchers had condemned their enterprise to reenact a failure.}

It is wise to use or at least consider the concepts and notions of the old. However, it is also imperative that there must be some other meanings to the word old that does not mean classic, especially from the philosophical point of view for a subject that is so diverged from the general guarantee, that correctness cannot be fully gauged. So, at the end, may it be in the quote itself. "Do listen to the old, and carry the journey of the young. But do not be so bold to pursuit the uncertain future, nor too reserved to glorify the lacking knowledge of the past."

\subsection{The 1940s}

In 1943, neurophysiologist (that's right, this job existed at the time) Warren McCulloch and logician Walter Pitts collaborated on a groundbreaking paper titled, "A Logical Calculus of the Ideas Immanent in Nervous Activity", published in the "Bulletin of Mathematical Biophysics." \cite{mcculloch_logical_1943}. The central aim of their work was to investigate the possibility of representing logical functions through the conception of what is then called the first formulation of an \textit{artificial neuron}, which is fairly common in the neurophysiological field of the time, in which they adopted a model of simplified neuron structure. The details of the paper are pretty much, well, complex to have a look at, because it is aimed toward logical representation, which at the time, they chose to represent them in a fairly convoluted, difficult notational scheme. I mean, seriously, using symbolism of Language II (Carnap, 1938), Russell and Whitehead \textit{Principia} (1927) and else is fairly not so nice for the reader, though arguably it is used for correctness. Though, we can still try decrypting the paper as it is. Actually, no, because it is pretty cumbersome. But we can try seeing what is the consequence of it. 

McCulloch and Pitts illustrated, rather convincingly through symbolism and a network of assumptions and framework, that predicate logic can be represented by this framework of neurological units in a net. Using the binary logical approach (apparent in their assumptions for the calculus that `the activity of the neuron is an \textit{all-or-none} process') and of the static case for the network, logical operations can be executed, albeit in a fairly strict form. The descriptions of said network is also pretty intricate - they concerned of the problem for \textit{learning}, as well as the non-linearity of the signalling process, that is, the nets with circles for excitation, being represented or at least conducted in the theory as recursive functions. But more important than such, is the apparent connection to the notion of the computational machine, or rather, the (Universal) Turing machine (UTM). They argued that their network model could theoretically simulate and computation performed by a Turing machine, which is, in its own, a surprise to be sure of. 

Later on, based on this, Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strength between neurons can lead to new and more complex structure. His rule, now called \textbf{Hebbian learning}, remains an influential model to this day, despite the call of modern practitioner for it to be obsolete. \index{Hebbian learning}

Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the first, perhaps apparent neural network computer in 1950. Later, at Princeton, Minsky studied universal computation in neural network. Historical accounts of the time recited concerning opinion about whether if the subject of study can be called mathematics, but von Neumann reportedly said "If it isn't now, it will be someday". 

There are many more historical records correlated to the development of the theory of intelligence, often patched up with artificial in the former. For example, W. Grey Walter (1947) demonstrated the development of the autonomous robots known as "tortoises", named Elmer and Elsie; which, in the same year, Norbert Wiener publishes \textit{Cybernetics}, quite a work at the time. They are settled to showcase what Walter then termed "machina speculatrix" behaviour, encapsulating the essence of a contemplative machine, for what it means. In 1948, an interdisciplinary conference was held at Caltech in Pasadena, California, on the topics of how the nervous system controls behaviour and how the brain might be compared to a computer - the Hixon Symposium of Cerebral Mechanisms in behaviour, which is also considered to be quite substantial in the timeline. In 1945, Turing writes a pioneering, but unpublished paper of "Intelligent Machinery" later on published a paper of the same content in 1950, proposing also the Turing Test. While those are important and pretty much equally interesting, by historical content, I don't think we should be of concerned about them more than we have already given to the main matter. 
\subsection{The 1950s to 1960s}

The 1950s is called the period of the \textit{birth of intelligence}. Granted, this is perhaps because of the exact Dartmouth workshop that happened in summer of 1956, which ultimately comes of inviting John McCarthy, Marvin Minsky, Claude Shannon (the one whose works came off as information theory), Nathaniel Rochester, Trenchard More, Arthur Samuel, Ray Solomonoff Oliver Selfridge and more together, to think, and study about how machine can be made, to use language, form abstraction and concepts, or rather, as recited in \textit{The Quest for Artificial Intelligence}, it comes from the dream of extracting oneself to a machine that is capable of simulating such. Though, perhaps the term \textit{computational rationality} would have been equally, if not more reasonable of an expression, but seems like McCarthy resisted using such. If you are wondering yet why there can be the word computers at start, then it is because one of the main assumption in this event is that "\dots the conjecture that every aspect of learning or any other feature of intelligence can be in principle be so precisely described that a machine can be made to simulate it.", which inherently make it the target for the computational structure. \index{computational rationality}

It is well in my interest (and certainly you maybe) that this step up is one of the fundamental assumption in which will shape what artificial intelligence to become in later years. Or rather, it is more reasonable this way, and is pretty obvious, of the transition from physical models and physical representation, to the more abstract, conceptual, mathematically expressed system in which one does not have to take into account the physical hardware, but only the abstract software itself. By removing physical binding, the theory of artificial intelligence by itself has focused on the modelling of the process inherent within specimens exhibiting intelligent, rather than focusing on replicating the specimen itself. As for once we said the Wright brother did not create the plane because they want to replicate the birds, it is similarly said that we want to create intelligence, artificially, without having to reconstruct the brain. Or, alien-case, some organ that is responsible for such. 

In such session, it is recorded that Allen Newell and Herbert Simon have already proposed the notion of the Logic Theorist (LT) \cite{1056797NewellSimon}, claimed to "\dots have invented a computer program capable of thinking non-numerically, and thereby solved the venerable mind-body problem" - again, quite a strong claim. Though, it is also this program that is able to prove most of the theorems in Chapter 2 of Russell and Whitehead's \textit{Principia Mathematica}, which gave Russell quite the delight when the program had come up with a proof for one theorem that was shorter than the one in the original book. Though, on the flip side, the editors of the \textit{Journal of Symbolic Logic} were less impressed by such notion. \index{Logic Theorist}

While the workshop itself is not particularly successful, in the sense of giving any breakthroughs, it did introduce all the major figures to each other, and, by extension, formally start the journey of finding the dream of artificial intelligence. It's also those figures, that in 20 years time into the period that dominated the entire field. Interestingly, few of them will be directly against each other, and one particular will create the first winter of AI research. 

The early year of AI were full of successes - in a limited way, and with it comes expectations. Given the primitive computers and programming tools of the time, even by the time of Turing where the formal system of computer science and encoding was formulated, while the structure and hardware allows the interpretation of computers to be perhaps no more than arithmetical machines - arguably, not quite so even since the dawn of World War II - it is astonishing whenever a computer did anything remotely clever, or perhaps certainly to encode the logical framework that itself was based upon, that many tried to relentlessly refuse to believe. The intellectual establishment, by and large, preferred to believe that "a machine can never do $X$", and then someone demonstrated that it can. This has been around for a long while, and even by today's time, it is still being done regardless. A consequence of this type of progress is, probably mentioned anyway, the reestablishment of what separate man and machines of year-by-year basis - the Turing test seems to further and further every year and then. \index{intelligent agent}

Because we are using Norvig's text on artificial intelligence, the historical account can historically be separated into four central aspects of analysis. Though naive as it can be, and overvaluing as it might is, all those approaches to AI has been followed throughout the history, each by different people with different methods. This is perhaps one of the most fundamental rules when researching anything - to work on anything, one must have a definition and concept of such, for it to be anywhere ambiguous or descriptive as possible, and for controversial versus widely accepted in its sense. 

    Generally, Norvig \cite{10.5555/1671238} separated in categories, into 4 main approaches. 

    \begin{itemize}
        \item \textbf{Thinking Humanly}: Despite the description being rather arbitrary, it settled itself in the cognitive science field of study, or rather, incited of the \textit{cognitive modelling paradigm}. The original idea is that if we are going to say that a given program thinks like a human, then what and how human thinks? We then need to get inside the actual workings of human minds, either through introspection, through analytical psychology, and observing the brain in action, which correlates to neuroscience. Once we gain all the sufficiency criteria for forming a theory about the mind, then comes the computer, in which case it is guaranteed to match certain trace of human process in it. Ultimately, this resulted in the interdisciplinary field of \textit{cognitive science}, which brings together computer models from AI, and experimental techniques to construct testable theories of the human mind. 
        \item \textbf{Acting Humanly}: The study of, "make(ing) computers do things at which, at the moment, people are better." This is expressed most prominently in the approach which leads to the \textit{Turing Test}, proposed by Alan Turing (1950) to provide a satisfactory operational definition of intelligence. We perhaps want to emphasize on the word being operational definition. In the short term consideration, the test requires computer to possess the following capabilities: natural language processing, knowledge representation, automated reasoning, machine learning (to adapt and detect plus extrapolate patterns). By default, Turing's test deliberately avoided direct physical interaction between the interrogator (human) and the machine, because, the assumption is that physical realization is unnecessary for intelligence. However, the more comprehensive test called \textbf{total Turing Test} also includes computer vision and robotics to the frame. This, in essence, also outlines most of the six disciplines that made up the modern research of artificial intelligence. 
        \item \textbf{Thinking Rationally}: This approach, can be fairly cited to be the "laws of thought" general paradigm. Their principles started of the earlier day of Greek philosophy and those that codify "right thinking", that is, "irrefutable reasoning processes". These laws of thoughts were supposed to govern the operation of the mind, and thereby, initiated the field of logic. And by extension, comes to the point of initializing the study of logical AI, or more widely known to be the \textbf{Symbolic AI} school of thought. There are two main obstacles to this approach. The first being rather obvious - it does not scale well, for example, to state in formal terms the informal knowledge and actions, and write it in the language of logical notations. Second, logic is concrete, and thereby, not real life - that is, there is a difference between solving a problem "in principle" and solving it in practice. This makes them ineffective in the long run, and furthermore can be more than deceiving to be called good. 
        \item \textbf{Acting Rationally}: This comes off as the theory of \textit{intelligent agent}. An \textbf{agent} is just something that acts \footnote{To be fair, this distinction is useful, because aside from agent, there is another model of the internal process that, that separate of thinking, and acting altogether.}. Computer agents are expected to operate autonomously, depends on the definition, perceive their environment, also depends on the interpretation, persist over a prolonged period of time, adapt to changes, and facilitate the notion of \textit{goals}. A \textbf{rational agent} is then one that acts to achieve the supposed criteria, for the outcome of what is expected to be the target. The rational-agent approach has two supposed advantages. First, it is more general than the law of thoughts approach, because correct inference is just one of several possible mechanisms for achieving rationality. In such case, going small, but principle is perhaps more effective than trying to encapsulate everything into a more general notion of rationalization. Second, it is more amenable to scientific development than approaches based on human behaviour or human thoughts. 
    \end{itemize}

By now, there must be observed to be two schools of thought present in the study of artificial intelligence. One is concerned of the old-fashioned research of McCulloch and Pitts in 1943, ultimately resulted in the theory of \textbf{connectionism}\index{connectionism}, which aims to facilitate the smallest singleton component of human brain - the \textit{neuron}, and its wider framework called the \textit{neural network}. Hence, artificial intelligence in this sense aimed to create and understand the theory of artificial neuron and neural network. Prominent in this portion of research can be attributed to Winograd and Cowan (1963) for demonstrating how a large number of elements could collectively represent an individual concept, with a corresponding increase in robustness and parallelism (which, undoubtedly, later on became known as the technique of \textit{encoding representation}). As mentioned before, Hebb's learning, Widrow-Hoff learning, presented in the \textbf{ADALINE system}, and Rosenblatt's \textbf{perceptron} are some of the illustrative groundbreaking formalism existed within this timeframe. Despite such, as we have seen above with the old figures of the Dartmouth workshop, there exists the entirely opposite in spirit as well as nature of the approach, led by Marvin Minsky, called the \textbf{symbolic conformation} approach, or more generally known to be the \textit{Symbolic AI} paradigm. Works prominent in symbolic view includes the famous Newell and Simon (1976) \textbf{physical symbol system}, Herbet Gelernter's (1959) Geometry Theorem Prover, a bunch of other works conducted by Minsky's student in his lab, for example, the domain of \textbf{microworlds} in which James Slagle's Saint program (1963), Tom Evan's ANALOGY program (1968) and Daniel Bobrow's STUDENT program (1967) we realized.  This approach, which relies on logic and symbolic approach to construct and formalize the thinking machine, led by Minsky, McCarthy, Papert, and a lot of the AI researching community of the time, went against the connectionism view. Minsky and Papert personally attacked the theory of artificial perceptron by their book \textit{Perceptron (Minsky, Papert, 1969)}, with perhaps some coincidental timing and the rise of symbolism as the winner in the late 1960s, to then effectively "shut down" the direction of research toward the theory of connectionism. Though, one might ask, from the first glance, what is so different of the two? 

In essence, the detail of connectionist AI postulates that learning of associations from data (with little to no prior knowledge) is crucial for understanding behaviour. This perhaps comes of as not so surprising for the camp of symbolic AI, as their theorists postulate and suggest that the intelligence that underlies human activity can be formalized, or must, as the string of logical conclusions and operations that, quoted, "By extension, one can see the process without questioning why it was there, rather than observe actions whilst blinded of the process that led to it". Recent debate, even to the current time, between the two AI paradigms has been prompted by advances in connectionist AI since the turn of the century that have significant applications, and furthermore, the technological evolution of the age of abundance of data - the internet. So, you might be better off comparing the two to be the debate between \textit{empirical approach}, to the \textit{intrinsic rationalization approach}, respectively. In the end, however, we need both. \cite{Goel_2022}

\subsection{The first AI winter (1966-1973)}

History suggested, or rather, indicated the overwhelmingly positive altitude, most of the time, from AI researchers about their predictions of the upcoming progresses. Herbert Simon in 1957 is often quoted: 
\blockquote[Herbert Simon, 1957]{It is not my aim to surprise or shock you --- but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create. Moreover, their ability to do these things is going to increase rapidly until—in a visible future --- the range of problems they can handle will be coextensive with the range to which the human mind has been applied}

Terms such as visible future can be interpreted in various ways, but Simon also made more concrete predictions, by the notion of 10 years for certain capabilities. These predictions came true (or approximately true), however, within 40 years rather than 10. Simon's overconfidence was due to the promising performance of early AI systems on simple examples. In almost all cases, however, these early systems turned out to fail miserably when tried out on wider selections of problems and on more difficult problems. That is to say, they particularly did not scale well. 

In the 1970s, the entire AI field of study entered a period of time described as the "AI Winter". According to the AI Newsletter, the phrase was borrowed from the term "Nuclear winter", which originated from the Cold War perspective of nuclear war. Indeed, during the AI winter, commercial and scientific activities in AI declined dramatically. Research fund was ruled out, supports were cancelled, and overall the field grinds to a halt. Arguably, AI is still recovering from the winter that lasted nearly two decades, with another small subsequent winter period, the second AI Winter, happened just before the onset of 1990s. 

The major cause of the AI winter can be attributed to the story of the government's decision to pull back on AI research, of missed promises and overreaching predictions. This is perhaps more apparent in the two infamous reports, specifically the Automatic Language Processing Advisory Committee (ALPAC) report by U.S. Government in 1966, and the Lighthill report for the British government in the 1973. 

I am not going to get into the details of such story, but they have the same shadow of impact - both did not deliver what was expected. Whilst the dream and predictions is absurd, they failed to deliver what they were tasked of, ultimately because of several factors. Observationally, it comes from mostly the problem with scalability, as previously argued, which is bad of those early model. More than such is the fallacy that most early program knew nothing of their subject matter, and only operates on clever, often simple syntactic manipulation. Again, we specify on the word simple. And perhaps more importantly, is the question of the illusion of unlimited computational power that AI researchers often made during the time. 

The last difficulty that leads to this downfall is then perhaps of some fundamental limitations on the basic structures being used to generate intelligent behaviour. By the language of representation theory, Marvin Minsky and Papert's book Perceptron has demonstrated that perceptron model is pretty much poor in representing concepts or any structure of the underlying representation spaces. Although it did not consider the more complex system of multilayer perceptron, research funding for neural network research soon dwindled to almost nothing. Sometimes, people still attributed the backward crisis of AI research to such event, though it is rather not so convincing of an argument. 

On a flip side, it also brings about to us an interesting ordeal of the hype cycle of AI research (Menzies, 2003)
\begin{figure}[h!]
    \centering
    \includegraphics{img/image.png}
    \caption{Well, look at that. Can you guess if we are about to face another one soon enough?}
\end{figure}

\subsection{Knowledge-based systems - expert system (1969-1979)}

During the first decade of AI research, the camp of either symbolic or connectionist AI has been to construct a general-purpose construct, in which deduction and reasoning can be made on such system to derive new situations or new knowledge. Such approaches have been called the \textit{weak method}, \cite{10.5555/1671238}, because although it is general in some partial way of the logical facts, it cannot scale well to large or difficult problem instances. This comes off as the scaling issue in both the complexity of the task, and the expression of the system by itself, in which reasoning might be applied in principle \footnote{This can be further clarified. The aspect of complexity in the task is reasonable - with increasingly large system, comes increasingly many objects in action, many rules or patterns, many properties and aspect to consider, which makes it harder to design a working well system in such complex term using symbolic approach or phenomenologically, the search mechanism of connectionist. And second of all, formalizing them into symbolic logic, while the logical predicates and laws are the same, the entire problem setting and the system that use such logical system is entirely inoperable with scales. For the connectionist camp, at least up to said point, there are no evidences or ways to effectively search for the correct representation or dynamic system using their model of the computing neural unit.}. furthermore, philosophically, it ran into the problem that is analogous of the Chinese Room Argument - those reasoning systems are perhaps only symbolic manipulation, and is not actually intelligent. Naturally (speaking), the alternative to weak methods is to use more powerful, specialized knowledge that allows for larger reasoning steps and can more easily handle typically occurring cases in narrow area respectively. Think about it as the reduction of generality. This resulted in the domain called \textbf{expert system}, for which, if you followed from above, connectionist has already been phased out (apparently, at this point there are no funding so they have no jobs\dots), so mostly it is a mostly pure symbolic approach, just with added flavours. \index{expert system}

Informally, expert system are computer programs aiming to model human expertise in one or more specific knowledge areas. They usually consist of three basic components: a \textit{knowledge database} \index{knowledge database} with facts and rules representing human knowledge and experience, an \textit{inference engine} processing consultation and determining how inferences are being made\index{inference engine}, and an input/output interface for interaction with the user or usually, just the problem case itself. 

According to \cite{articleMetaxiotis}, expert system can be characterized by: 
\begin{itemize}
    \item Using symbolic logic rather than numerical calculation (so no connectionism, as we said). 
    \item The processing is \textit{data-driven}. 
    \item A knowledge database containing explicit contents of certain area of knowledge. 
    \item The ability to interpret its conclusion in the way that is understandable to the user (the \textit{explainability criteria})
\end{itemize}
While most of the characterization can be said to be weirdly strict, except for the second one, the last characteristic is perhaps of very large interest within the modern framework of artificial intelligence, the requirement of \textbf{Explainable AI}. This sentiment (or requirement) is particularly stronger in the field of medical science and overall medical field, where explicit conclusion must be specified of its content and deduction - for example, why is this diagnosis true, and what factor is taken into account? This is perhaps reasonable - nobody wants to be that one guy anyway. 

Historically, expert systems emerged in the early 1950s when the Rand-Carnegie team developed the general problem solver to deal with theorems proof, geometric problems and chess playing. About the same time, LISP, the later dominant programming language in AI and expert systems, was invented by John McCarthy in MIT. 

Perhaps one of the main example of expert system can be traced back to the \texttt{DENDRAL} program (Buchanan et al., 1969), developed at Stanford by Ed Feigenbaum, Bruce Buchanan, and Joshua Lederberg. They teamed up to solve the problem pretty much very specific, and not so general as we have seen: inferring molecular structure from the information provided by a mass spectrometer. The input to the program consists of the elementary formula of the molecule, and the mass spectrum giving the masses of various fragments of the molecule generated when it is bombarded by and electron beam. The naive program generated all possible structures consistent with the formula, and then predicted what mass spectrum would be observed for each, comparing this with the actual spectrum. 

This, by itself is pretty much intractable even for moderate-sized molecule, even with the modern standard computation (perhaps). So, instead, they opted for shortcuts - an informed shortcut that is - by utilizing \textit{human specialized input}, or expert input. They used analytical chemist to know what they were looking for in the problem, and apply it to the rule system of the program. This approach, while seems to be pretty obvious, turned out to be fairly powerful and successful of the time. \texttt{DENDRAL} program was rendered successful, most is can be said is because

\blockquote[Feigenbaum et al., 1971]{All the relevant theoretical knowledge to solve these problems has been mapped over from its general form in the [spectrum prediction component] (“first principles”) to efficient special forms (“cookbook recipes”).}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{img/expertsystem1.png}
    \caption{The diagram of a typical expert system. Here, human involvement is fairly representative in the figure, and is illustratively indicating the overwhelming reliance on human touches.}
\end{figure}

However, it is to be said that this type of approach is, arguably not artificial intelligence anymore, in the true sense of it. Rather, it is similar to a conditional, variable logic system in which the dynamics are specified by junctions in a box, where rules dictate conditions in which junction should a ball rolls up to certain path, and every structure has to be mapped, and configured by the designer himselfs. In a modern word, it is eerily similar to a proof machine, just with different goal in mind. However, it is sometime enough for progress. At least, though, we get to know how an isolated "dynamic system" might work, which will benefits us later on. 

Eventually, more follows suite. There is then XCON, a computer hardware configuration system, MYCIN, a medical diagnosis system with about 450 rules that make it performs sometimes even better than junior doctors, and more. MYCIN, specially, contain the notion of certainty factor \index{certainty factor}, which is used to reflect uncertainty in a diagnosis decision. Quite similar to fuzzy logic, I wonder anyone have tried using fuzzy logic style into it? The more epitome of natural language processing, under the lens of expert system, is \texttt{PROLOG}, a computational linguistic programming language to aid in the construction of this kind of new expert construct. Eventually, by some hilarious means, expert system were considered revolutionary to the point of threatening humanity by itself and its virtue of existence. Or rather, I would say for now, the onset of another winter of AI, toward which expectation reaches even further than before. How bad can it be of successes to be on the receiving end of those expectations: 

\begin{quote}
    The success of these systems stimulated a near-magical fascination with smart applications. Expert systems were largely deemed as a competitive tool to sustain technological advantages by the industry. By the end of 1980s, over half of the Fortune 500 companies were involved in either developing or maintaining of expert systems. The usage of expert systems grew at a rate of 30\% a year. Companies like DEC, TI, IBM, Xerox and HP, and universities such as MIT, Stanford, Carnegie-Mellon, Rutgers and others have all taken part in pursuing expert system technology and developing practical applications. Nowadays, expert system has expanded into many sectors of our society and can be found in a broad spectrum of arenas such as health care, chemical analysis, credit authorization, financial management, corporate planning, oil and mineral prospecting, genetic engineering, automobile design and manufacture and air-traffic control.
\end{quote}

Seems to me, quite a lot. While I was bashing expert system back there, it is certain of its use, specifically with systems and generic scenario where not a lot of things change, and more so on the front of static system, unlike natural language where ambiguity is almost always present. It is at this point, that since then, AI becomes an industry on its own, and is not just the umbrella terms for the weird little computational model here and there. And, you guess it, the second winter hits.
\subsubsection{The problems with expert systems}
On the surface, it is good. However, if it is the end of the road, we wouldn't be here and talking about something else than expert system. There are then indeed, problems with how the expert system turned out to be - many unresolved technological issues and performance limitation that severely affect the development and implementation of expert systems. This prompted the \textit{second AI winter}, arguably, which is even more damaging than the first one because of the rapid industrialization style, and the failed promises of various companies. The keys issues facing expert ssytem by then, are software, knowledge acquisition, handling uncertainty, and validation. Which, most of them can be grouped into the word \textit{scalability}. And, added to the list which I stole from somewhere, there is also the \textit{structural issues} with expert system, much like so in general for today's approach of ML. 

First, is the issue called \textbf{software standards and interoperability}. It seems like at the time, a lot of people want to do things their way, and because of that, there are no general standards in expert system software, development methodology, protocols or infrastructure. This makes the expert system section totally specialized in not just goals, but also designs, implementation, architectures, and all, even though they are based of some languages like \texttt{PROLOG} or the more famous \texttt{Lisp}. Though, arguably, because of its fairly prominent utilization, there has been pushes for standardizing it like IEEE Standard, so, this line of argument might dissapear in the air later on. 

Second, is the fact that knowledge acquisition in the context of expert system is very complex and dependent on human experts makes it both shallow and unscalable. Their facts are human-dependent structure, static and was articulated into a set of which does not reflect tha actual knowledge itself. This renders the expert system to be similar to a set of rules with added semantic, and with logical rules for its inner composition, rather than artificial intelligence. Furthermore, it also does not learn from experience, if one wants it to have the action of learning, since it only goes on to follow the set of rules and how they operate. Best of all, those knowledges are too shallow for any reasoning to be maded of extended semantic, as the interpretation is often hard to configure, and many times, because constructions and rules are in nature subjective of human experts, incompatibility is also an issue. This also covers the third point about the \textbf{handling of uncertain situations} or out-of-bound cases. 

Many tried to fix it. For once, there exists the scheme of \textit{Case-based reasoning} (CBR) theory that focuses on solving new problems based on similar past problem solutions, which seems to be able to eliminate the complex task of maintaining rules and facts through the use of adaptive acquisition of problem-solving techniques. However, again, this is perhaps shallow as it can get, and added to the semantic, there are also biases, and the uncertainty of past experiences being compatible with new situation, by virtue of different situations and scenario. CYC project by Cycorp Inc. aims to assemble also, the processing of commonsense knowledge in which somewhat allegedly, CBR cannot handle. yet I think this is not so much of a fruitful pursuit, because even the notion of \textit{common sense} is perhaps distinctively believed, even by a majorative mean. Such is also to say the surprisingly complex and unscalability that one would face if they try to encode common sense in such way. 

The worse reason on the list, though, is the lack of \textbf{validation on system operations}. The quality of expert systems is often measured by comparing the results to those derived from human experts. However, there are no clear specifications in validation techniques. How to adequately evaluate an expert system remains an open question, although attempts have been made to utilize test cases develop by again, expert, to do the job. However, many believe (including me) that this will turn out to be worse. 

\subsection{Neural network, the return (1986)}

What makes a system a good exhibition of artificial intelligence? Many said of different qualities, like in the way of symbolic AI, for the way and realization of universal logic calculi, or expert system and the tiresome knowledge banks acquisitions. But for many, one representative quality of being intelligent, is \textit{the learning ability}, or the representative sufficiency of learning action. Interestingly, logic can also be learnt, but in a rather strict sense and not so confounding on its own. Connectionism, with their neural networks, however, has a different view. 

IN the failure of expert system, and to the larger extend, symbolic computation in utilizing and mitigating their disadvantages in designs of the time, many turned to the hidden contender of the old neural network approach, which sought conformality and uniform of expression, in a dynamic way rather than the strict, universally required criteria of symbolic formation, for example. It might seem obvious that at some level, human manipulates symbols, of which by Terrence Deacon's book \textit{The Symbolic Species} (1997) suggests that this is the defining characteristic of humans. Though, most ardent connectionists questioned whether symbol manipulation had any real explanatory role in detailed models of cognition. This question remains unanswered, but the current view is that connectionist and symbolic approaches are complementary, not competing. As occurred with the separation of AI and cognitive science, modern neural network research has bifurcated into two fields, one concerned with creating effective networks, and the other concerned with careful modelling of empirical properties and actual neurons and ensembles of neurons. Though, that is not to say that McCulloch and Pitts's neuron, and other at this point, \textit{classical} neural network structures are still considered of principle to be the same. Many principles and 'axioms' have been removed, and many more have been added underway, except the only reasonable target - that the ability to reason or to be intelligent is somehow, dependent on those neurons. 
\subsection{Adoption of scientific method (1987)}

By the virtue of \cite{10.5555/1671238}, it is said that of such date, there are deliberate efforts to centre or to ground the wing of those who fly high in the sky, respective to the way AI isolated itself from the large portion of the scientific world. By then, AI was perceived to be the earlier rebellion against the limitation of existing fields, and then was reincorporated into the frame by such forces. Furthermore, AI has also adopted the scientific method. To be accepted, hypotheses must be subjected to rigorous empirical experiments, and the results must be analysed statistically for their importance. Though, with that said, there are some subtle point to argue and requisitioned. 

Every science start with conceptual understanding and conceptual structures. The main goal of the earlier prospects with artificial intelligence is to understand the manifestation and formation of intelligent to be created. Hence, in such times, there are many theories that subjected itself to the category of conceptual framework. In fact, if we are looking at the current trend of defining and conceptualizing \textit{Artificial General Intelligent} (AGI), it shows the same sign and way. This is not a problem, at all, that some might see the above shift and criticize the old progresses as costing us a fair amount of time and being rather too adventurous --- to the point of isolationism. Though arguable agreeable, it is rather far from the truth. 

There is also the problem with other fields of scientific research, for example, in their mention, information theory, stochastic modelling, optimization theory, and control theory. Partially, it can be of the reason that we ourselves, as human, do not believe or want to justify a computer full of simply binary bits and machines can be considered intelligent of quality similar to human, it is also believed that the essence of artificial intelligence cannot be fully captured using simply just, mathematics and rigours. Such is true of the time, when ideas are not tested out yet and many innovations and conceptual proposals are there, still waiting for someone to rule it out because of its relative insufficiency in both practical purpose, and partial correctness (because nobody knows if it is actually correct, or not). Only by the time that the infinite possibility of an entire new, controversial, and expansive field is narrowed down to the possibility within grasp, and the problems that was said to be present - for example, the reproducibility problem, was identified in the non-formal way of conducting researches. \footnote{We use the word non-formal instead of non-mathematical, because a lot of theories, while being formal, aren't necessarily mathematically rigorous. Indeed, mathematical rigours are not required, and is often scuffed off the table in a lot of theories and fields. Even in physics, there are also the distinctively air about mathematics - if everything is mathematical already, then it is not physics - and it is true, that a lot of physics cannot be mathematically defined in a rigorous way, for it to not be labelled as another sub-field of mathematics. Plus, the fact that many sees language as logical, more so than mathematics, kind of proves the point.}. 

Then, there is also the fact that those fields, throughout the development of artificial intelligence in our timeline, is severely lacking the tools of which artificial intelligence requires of the time. Indeed, science does not revolve around AI, and many fields continue to progress in the timeframe of 50 years, since the 1940s to the 1990s, as we recall from the year 1987. That is an entire 50 years of development. Only when there are tools or some effective theories from other fields that see acceptance in adoption, by then, that artificial intelligence would want to take a turn toward those theories for obvious reason - to utilize them with the tools that they provide, that we can use - which obviously does not exist before. For example, try in the hand of information theory in the onset of artificial intelligence in the 1940s. Can you find the tools required of information theory that artificial intelligence conceptual of the time would be able to utilize, least of it that the most axiomatic principle of AI at the time of the Dartmouth workshop is that "intelligence can be observed and constructed computationally"? Probably not, at least from my view. Abandoning this fact, that sciences did, indeed, lack what is required or at least desired of AI research, is particularly frustrating, like as if other fields of 'mathematically rigorous' - which they are not, have all the tools necessary from the beginning. 

Nevertheless, this marks an important point of the development of artificial intelligence research, after which we know what is the relatively usable and effective approach, which allows the theoretical analysis and actual formalization to be taken into place. Again, one might even suggest this is because of the scale ergonomic, and I would not bat an eye. This, in turn, tells much of the maturity of the field as a whole, and the focus of actual construction and application purposes. Nowadays, there are \textbf{hidden Markov models} (HMMs) for speech recognitions modelling, and the principle of stochastic evolution stands as an effective analytical tool for uncertain process. Machine translation now have information theory into its aid, neural network coming back with their rigorous analysis, both in statistic and by mathematical analysis. And finally, recognizing the probabilistic possibility of neural network and a few learning theory of which utilize classical non-network models, probability theory were utilized heavily into the field, which even birthed the \textbf{Bayesian network} formalism. 

\subsection{Intelligent agents (1995)}

Perhaps encouraged by the progress in solving the subproblems of AI - as we have seen by not tackle the hard general problem of AI first during the 1960s, researchers have also started to look at the "whole agent" problem again, though this time, with experiences of past failures. The work of Allen Newell, John Laird, and Paul Rosenbloom on \textsf{SOAR} (Newell, 1900, Laird et al., 1987) is the best-known example of a complete agent architecture. In a modern date, ChatGPT (Open AI, 2018) is one of the attempt to create a completely functional, albeit limited, semi-universal agent. 
\subsection{The revolution of Internet (2001)}



\section{Defining intelligence}

Inside the term \textit{artificial intelligence}, there is the word \textit{intelligence}. A normal person will tell you that they are intelligent. But it just so happens that this notion of qualification is harder to define when one participates in the active action of finding it. So, what is it? This is the question we should take in. 

Perhaps this is one of the most important question when encountering, or researching artificial intelligence. Intelligence is difficult to define, and is natural as a human concept, yet ambiguous as it can be. If one wants to tread on the road to artificial intelligence, for whatever it is, they must have a makeshift, for now, definition of artificial intelligence, and intelligence in mind. By then, for me myself, I chose to restrain myself from defining the exactness of intelligence, by rather justifying the act of working on artificial intelligence as actually reverse engineering intelligence by itself. The detail arguments? It will be in there. 

\subsection{Historical accounts}

We start this section with a series of historical accounts. Shane, Marcus (2007)'s paper \textit{A collection of intelligence} and Masahiro's (2023) \textit{Descartes and Artificial Intelligence}\footnote{See more in \href{https://www.philosophyoflife.org/jpl2023si_book.pdf}{Journal of Philosophy of Life Vol.13, No.1 (January 2023):1-4}} might be a great place to start this, since they provide a non-trivial amount of definitions and attempts already there, which serve us more as exhibition for observant in this section and beginning. 

We list of the interesting definitions that has been given to artificial intelligence, and the question of what is intelligence. Interestingly, those definition are often phenomenological and "soft" rather than not, since a concrete definition cannot be achieved, or rather, be formalized within the right of the established theory. 
\begin{description}
    \item[R. J. Sternberg] \dots I prefer to refer to it as 'successful intelligence.' And the reason is that the emphasis is on the use of your intelligence to achieve success in your life. So I define it as your skill in achieving whatever it is you want to attain in your life within your sociocultural context — meaning that people have different goals for themselves, and for some it's to get very good grades in school and to do well on tests, and for others it might be to become a very good basketball player or actress or musician. 
    \item[D. K. Simonton] \dots certain set of cognitive capacities that enable an individual to adapt and thrive in any given environment they find themselves in, and those cognitive capacities include things like memory and retrieval, and problem-solving and so forth. There's a cluster of cognitive abilities that lead to successful adaptation to a wide range of environments.
    \item[H. Nakashima] Intelligence is the ability to process information properly in a complex environment. The criteria of properness are not predefined and hence not available beforehand. They are acquired as a result of the information processing.
    \item[P. Voss] \dots the essential, domain-independent skills necessary for acquiring a wide range of domain-specific knowledge - the ability to learn anything. Achieving this with 'artificial general intelligence' (AGI) requires a highly adaptive, general-purpose system that can autonomously acquire an extremely wide range of specific knowledge and skills and can improve its own cognitive ability through self-directed learning. 
    \item[Jensen, Huarte, Dearborn] \dots the ability to learn, the ability to understand, either principles, truths, facts, or common sense, to profit from experiences; the ability to comprehend, or the capacity to reason.
    \item[A. Anastasi] Intelligent is functionally of multiple components combined.
    \item[J. Peterson] \dots a bunch of stimuli. 
    \item[Humphreys] \dots the resultant of the process of acquiring, storing in memory, retrieving, combining, comparing, and using in new contexts information and conceptual skills.
\end{description}

Out of those definitions, we see two patterns, or the point of view in defining the operational definition of intelligence - one is the \textit{top-down}, empirical approach, and one is again, the \textit{ground-up} structural approach. 

\subsubsection{Two ways of definition}
Out of those definitions, there are two kinds of defining the notion of intelligence, we call it the \textbf{top-down} and the \textbf{ground-up} approach. The top-down line of thought demonstrate, most of the time conjectures, the existence of intelligence as a whole, without finding the actual shell that contains it. If intelligence is \textit{general}, then their implementation follows, but to a sufficient degree, it can be achieved everywhere. It guarantees, partially, of certain school of thoughts the generalizability of intelligence as the ground base to re-create such, which is characterized, often, by current machine learning discipline. This approach beside from guarantees such existence, also has the capability to `test' a subject of being, `intelligent'. 
\vspace{2mm}

This is done by setting up agenda and criteria, of which the current theory serves as more of a black box for the actual `machine' that contain it, but enough exhibitions fitting those criteria for intelligent. Fortunately, this also sets certain criteria for artificial intelligence to be specified so in the name. The Godel's argument is one of such example in this line of thoughts, theorized by J. R. Lucas (1961), Penrose (1994, 1989), and Benacerraf (1967), similar to the Chinese Room Argument (Searl, 1980). Coincidentally, the notion of \textbf{computationalism} is also formed out of this approach. 
\vspace{2mm}

The \textit{ground-up} approach of defining intelligence is simply the polar opposite: Instead of defining intelligence by criteria, they create machines or models that have intelligence seems to be the emergence behaviour from those model. That is to say, they define intelligence by not defining it but constructing it. Though, this type of approach still requires the intuitive feeling of intelligence to figure out or identify such emerging signs of a growing construct, but it is more or less general, as it does not depend on certain opinion, or fixed high-level criteria to classify it. 

Nevertheless, if one wants to work on notions or related measure to intelligence, or in general, any given topic, it is imperative to form any form of knowledge, barely so as the \textit{definiendum}. 
\subsubsection{Problems with defining intelligence}
A traditional viewpoint is that definition is a curse. Some says definition is only the typographical shortcut - mainly the opinion in maths - of convenience, certain idea consider definition to be the encapsulation of features. Most of the time, definition is even separated into different forms: either dictionary, stipulative, descriptive, explicative, or ostensive \footnote{Clarification for each of those terms, and a general idea of such concept. A definition is made up of two parts: the definiendum and the definiens. The definiendum is the term that is to be defined, whereas the definiens is the group of words or concepts used in the definition that is supposed to have the same meaning as the definiendum. 

A \textit{dictionary definition} reports the existing meaning of a term, in one sense of this phrase, and aims to provide definitions that contain sufficient information to impart an `understanding' of the term, whether this understanding involve operations and overall intuitions of such meaning. 

\textit{Stipulative definitions} imparts a meaning to the defined term, but involve no commitment that the assigned meaning agrees with prior uses (if any) of the term; basically, assigning new meanings to a term, whether the term has already got a meaning or not. \textit{Descriptive definitions} on the other hand is similar to stipulative, but also aim to be adequate of existing usage. 

\textit{Explicative definition} offers neither descriptively nor stipulatively, but as explication, aim to respect some central uses of a term, but stipulative on others. This, nevertheless, can be understood as having both, in a sense. The central aspect, however, relies on the fact that explicative definition views identical function with different definiendum the same, as long as the definiens retains its core functions, and truth, for some case the consequences might differ from such, as long as the essential remains. 

Finally, \textit{Ostensive definitions} while depend on context and experience, its essential function is to enrich certain language, by the mean of limiting the windows of perception to certain object which entails shared attributes - so for example, `let $x$ be a number in $\mathbb{Z}$' while we know that there are many other integers. In fact, this way of defining is considered, for some thinkers to be the source of all primitive concepts (Russell, Whiteley). We would be using these notions of definitions in the future.}, of which none are mutually exclusive, but works rather by the mean of overlapping each other but retains certain uniqueness. In one such way, what we desire from the definition of intelligence is unclear. 

A provisional definition can be obtained, however, at least from the subjective point of view, to at least defining intelligence by generalizing what others have done in the progress of realizing this: \index{Intelligence}

\begin{definition}[Intelligence]\label{def:intel1}
    Philosophically, intelligence of a subject $S$ is the representation of that subject's probability to \textbf{think} of its internal structures, and \textbf{act} or not, based of such premise of thoughts.
\end{definition}

This definition is the type of descriptive definition. The word \textit{think} requires a lot of times to work out, since it is not apparent how to think. Arguments about the state of think and its actual representative are various, and inconclusive. Furthermore in such, there is also the issue with the correlation between intelligence and consciousness, which is troublesome, and yet again controversial enough to guarantee itself another entire book dedicated still. 

Perspectively, we would like the \textit{identification of intelligence}. Note that, in our view, intelligence is an umbrella term, for now. \index{Intelligent}

\begin{definition}[Intelligent]
    A subject $S$ is considered intelligent if, for certain proxy of observations, fits the set of representation that correlates it to the notion of intelligence. 
\end{definition}

Intelligence, most of the time, is rather dubious to look at. Generally, and incidentally, it is because of the concept itself is not so clear and rather ambiguous. What exactly rings in your brain when you heard of \textit{intelligence}? The subject of which concerns the typical human brain, is the philosophy of mind, or rather, the famous one being \textbf{dualism}, or Cartesian dualism. And, such philosophy will always be philosophy - without implementation, and without actual realization of the underlying subject. 

\subsection{The Descartes experiments}

Instead of defining intelligence by referring to oneself, or by exploring such notion by assuming properties of human, Descartes took a different approach - what is required to project such notion to another, perhaps inanimate being, to be equal to man? Arguably, this experiment also falls into the category of the thought experiment of artificial intelligence; however, it is more befitted being mentioned and argued in the section more on the intelligence side of the word. Under the Descartes interpretation, let's have a look at the conditional, and rather empirical approach to the question. It seems, and as it is, more closely to the \textbf{black box argument}, as I called it as such, to provide classification and justification of which relies solely on empirical test, but not knowing the true essence of what inside. \cite{Descartes1950-DESDOM}

Descartes's argument start with the justification of the apparent reactive behaviours observed by human themselves, who at the time, was largely considered to be the only species capable of advanced rational thoughts and processes. 

\begin{displayquote}
    (I)f someone touched it (= the machine) in a particular place, it would ask what one wishes to say to it, or if it were touched somewhere else, it would cry out that it was being hurt, and so on. But it could not arrange words in different ways to reply to the meaning of everything that is said in its presence, as even the most unintelligent human beings can do. [Descartes, 1700]
\end{displayquote}

Here, Descartes argues that in order for human-like robots to acquire intelligence, they have to gain a universal capability to accurately react to any unknown situation that may happen in the environment. However, what machines can do is no more than to respond to a single situation one-on-one via a specific organ, hence, they cannot be considered to have a universal capability that even unintelligent human beings can enjoy. 

Continuing, Descartes argues that those machines do not act on their knowledge, but the disposition of organs.

\begin{displayquote}
    For whereas reason is a universal instrument that can be used in all kinds of situations, these organs need a specific disposition for every particular action. It follows that it is morally impossible for a machine to have enough different dispositions to make it act in every human situation in the same way as our reason makes us act.
\end{displayquote}

The argument is quite clear. Human is universal of the environment. Whereas machine is no more than a combination of abilities that are applicable only to certain situation that the creator could imagine when they built the automated machine. 

From this, we can design a very specific test, under the assumption taken, and the principle argued, on determining between human and machine. Notice that the empirical approach of such does not specify \textbf{intelligence}; rather, it attempted to question the difference between the natural highest epitome of intelligence (human) to the analogous artificial construct (machine). 

Suppose $A(i,o)$ is a machine capable of 'intelligent' act, in a sense, of which we would call it as 'action with reasons'. Then, under the argument, there exists a frame $M(A)$ of which encapsulate all capabilities of this machine. Unspecified of the correct notion of this frame, the empirical test relies on the fact that $A$ as a machine, is provided with in/out resources, of such that it is guaranteed of information received, and actions being capable. We state that $M(A)$ is finite, as the environment $E$ of which $A$ would be received is limitless. Or rather, suppose that $\{M_{i}(A)\}$ is the set of all given actions and reactions of which $A$ can output, provided that it receives the information. Then, there exists an element $M_{k}(A)$ such that $$\exists k \: \text{s.c}\:M_{k}(A)\notin \{M_{i}(A)\}\forall A$$ meaning that there will always be certain parameters that is out of control for the given machine to effectively respond to such. Under such constraint, the machine would fall out of control, and inevitably fails to respond, hence render the machine useless under such circumstance. This, seems to be different, as far as Descartes saw, of the human behaviour. 

Of all, Descartes \textit{rejects} the possibility that there exists an artificial, man-made construct or machine that can be intelligent, to the degree of which human presents. By then, it lifts the omniscience of human intelligence to a higher ground, much higher than previously perceived to be of the mechanical machines. 

\subsection{The economic of definiendum} 

It is clear from the above discussion that intelligence is very hard to define. Not only it is hidden in the most complex structure of a biological organism - in fact, biologically, the brain, or its smaller form in living organism, is typically not \textit{required until further evolution}. 

In one way or another, we want to \textit{define by creation}, if to say more than reverse engineering ourselves. By this, we assume that the existence of intelligence is composed of meaningful parts, of which comes up to certain levels, become the current intelligence that we enjoy. 

Stands on such ground, it is sufficed to say, or rather, to observe and clarify that artificial intelligence does not curtail of an infeasible attempt to try constructing subject of intelligent without knowing intelligence. Rather, \textit{artificial intelligence} is the \textit{process} of finding intelligence, others than the philosophical ground of defining such. Philosophy works, but our desires differentiate from that. Certainly, one of the most important field of research along such line has been neuroscience, neurobiology, and the field of neurological research, aside from theoretical modelling of conceptual models. Understanding and observing the most advanced mechanism of intelligence as of date, is one way to study and reverse engineering intelligence. And with that, again, goes artificial intelligence. 

\section{Formalizing artificial intelligence - an attempt}

What do we think of AI? What can be AI, what is perceived to be one, and how do we create them? The latter half of the word is hard, but, not so much for the first one, or \textit{artificial}. We can have a sense of what being called artificial by examining it. \index{artificial}

\begin{definition}[Artificial]
    We say an object is artificial only if it is not natural, or rather, it is \textbf{intentionally created} by meaning intentions, and not the general evolution of states, or the natural transformation of biology. 
\end{definition}

Most of human's construct hence can be called as artificial. The definition above is stated so, is to generalize to certain notion of intended creation, for in such event if we ever discover alien lifeform, then their "AI" would not be argued against such to be not artificial because of the word's basic meaning related to human, and hence can be considered a stipulative definition in its stead. 

\begin{hypothesis}
    The subject of intelligence, in one way called the working state, is different from the underlying mechanisms that host it, but dependent two-way.
\end{hypothesis}

\begin{hypothesis}
    Intelligence on \textbf{machine}\footnote{We are using the \textbf{dictionary definition} for the machine here, as an apparatus using mechanical power and having several parts, each with a definite function and together performing a particular task.} has itself the basic principles of \textbf{recursive representation} (i.e. with sufficient complexity, one system can host another of the same type in itself), and \textbf{relative abstraction} (i.e. the entire machine is supported by individual layers of components classified by certain metric, and the top layer are those that dictates such behavioural outcomes that is the subject of classification to either intelligent, or not).
\end{hypothesis}

In one way or another, an artificial intelligent subject can the theorized to consist of two things - the host, and the process. We call it as such for clarification, such is not to use the notion of \textit{mind} and \textit{body} for even now we have no conclusive decision on the matter named as said. The duality of the subject starts here, however, as we soon see.

Nevertheless, it is evident that while we abbreviated intelligence into only the container of necessitated traits considered so, the notion of artificial intelligence brings about a lot more than such. The previous principle assumption that we make indirectly agrees with the notion that the process and the host are dependent, yet distinct. This is, historically, against the Descartes' view on the metaphysical reality of the duality of mind and body. Yet, it seems not so contrived to think of it as the case, than not.

For the definition of artificial intelligence, or the construct that supports it, to make sense, we need to evaluate again, from what we have seen, what is even the term. As noted by definition on the notion of \textit{artificial} in the preceding section, being \textit{artificial} mostly comes of from the consideration of evolutionary processes - of which the interaction in the physical worlds, the biological worlds, and overall, anecdotally, of anything that is non-human of its (human) own capability to morph objects into an intended state - this is what normally resided to. Then, artificial intelligence refers to a set of observations, observable qualities deemed sufficiently of all intents and purposes intelligent, by any given constructs that is created artificially so.

This breaks down to the two conceptual ways to talk about artificial intelligence.
\begin{conjecture}[Artificial intelligence]
    Artificial intelligence is the classification for any such object of constructs sufficiently reflects those qualities that fit the standard of intelligence, of which also created \textbf{artificially} of intent and purposes (as reflected in definition of artificial).
\end{conjecture}

The second (conjecture) definition goes into a bit more detail on such term. We link artificial construct (a more generalized name) toward machines, for whatever the machine can be. 

\begin{conjecture}
    Artificial intelligence refers to (a)\textbf{construct(s)} - of which consists of the \textbf{machine} and its \textbf{process}, for such that the machine supports the process to reflects the observed results quantified in one way or another, to be interpreted as intelligence by the construct that is standard for those terms. Those constructs however, are absent, or not, by choice, of the \textit{existential facility} - or of either a rigid static facility of such - and hence artificially made.
\end{conjecture}

Arguably, the second definition is far more interesting than the first one. However, the claims, of such, can be hypothesized as perhaps not so ideal generalization:

\begin{hypothesis}
    The term artificial intelligence, generically, refers to the comparison between two actual constructs. If the current human - or us - are the ones evaluating certain constructs as intelligent, then it is equivalent to generalize human into a construct on its own, of sufficient analysis such that the comparison can be conducted.
\end{hypothesis}

This conjecture, in one way or another was called such is only because of its uncertainty by its own creator (myself) that it is true. Its purpose, is to justify and describe the thought processes thereof that leads to the discussion of \textit{intelligence} as a concept, and \textit{artificial intelligence} as a construct, whether the directed definition was raised in the correct orientation of the objected in focus or not. In such definition, however, it leaves much to desire - specifically, and especially of the relative tendency between two constructs - two of which for $A,B$ to be them, the figured measure $\mathcal{M}(A)$ was applied on $B$ such that $\mathcal{M}(A)> \mathcal{M}(B)$. 


By analogy, it is of human as $A$, and the machine, or any given of present, per term, low-intelligence lifeform $B$. However, the inconsistency starts when we consider the act of evaluating itself. By such concept and measure would we be able to model the state of self-conscious to the point of self-reflection and evaluation, that gives rise to this comparison in the first place? However, in most of the discussion, the subject matter of consciousness should be ignored, or at least assumed negligible, for it to be too complex that an entire corpus would not be enough to rather conjure on such idea alone. For now, we assume this ability is achieved of only the conventional end-goal.

We shall, then, define loosely what we meant by a \textbf{construct} for completeness of the above conjecture.

\begin{definition}[Construct]
    A \textbf{construct} is a conceptual encapsulation of two components: The \textbf{machine} in broad term which houses the \textit{operational facility}, and optionally, the \textit{existential facility} of the construct, and the \textbf{process}, or rather, its \textbf{state(s)} of being. These two makes up a functional construct of interest.
\end{definition}

What do we mean by \textit{existential facility}? This is the first step, or rather, the first application of the abstraction line of thoughts. Although the discovery of biological facts and rules are still insufficient to guarantee a desirable amount of admission that we truly understand the physical world or at least, the physical realization of living organism, virtually every living organism as of date needs certain standard basis on which its existences are guaranteed. For example, albeit the chance of taking this wrong is substantial, but there are, for example, the requirement for the existences of proteins for the functions of cells. So, in one way or another, There are layers of existential requirements for large constructs to be reasonably placed upon, and such treatment facilitates the relative \textit{existential facility} that we recalled. Note, as written, qualifying to be called existential or not is relative, meaning for certain level of 'abstracted reasoning', the main existential facility might be different, and certainly might be of dynamic configuration than what would be expected.

So, in general, the entire continuum at present of the artificial intelligence concepts, lies within this framework that we would be considering. 

\begin{theorem}[Artificial framework]
    An \textbf{artificial intelligence} framework considers the following aspect of exhibitive qualities:
    \begin{itemize}
        \item The set $T$ of traits from any given subject reference $A$ of which shall be (but conventionally be) called \textbf{intelligence} and their observables, their conditions, and their conceptual descriptions. This includes a \textbf{measure} takes either $A$, or similar concept of the same grade \footnote{Definition lacking. Later on need to define what does this mean.} denoted by $\mathcal{M}_{A}(A): A\to T$.
        \item The set $\mathcal{C}$ of all \textbf{constructs}, as the generalization of the notion \textbf{machine} (however can be identical in nature), expressed in shorthand terms as the pair $(\mathcal{F},P)$ of \textbf{facilities} and \textbf{processes}. 
    \end{itemize}
    Given $\mathcal{C}$ is the set of all constructs, its function might be assumed to hold the following rules, either for interpretability, or for better organization in the way it is structured: 
    \begin{enumerate}
        \item \textbf{Principle of abstraction}: For any given construct $C\in \mathcal{C}$, there exists a finite partition $\mathcal{P}_{n}(C)$ into $n$ arbitrary grades, or \textbf{layers} of such it follows the exhibition of knowledge - for any components $c\in C$ of a layer $n-1$, the layering partition effectively removes its correspondence to $n$ and its data availability to components and structure of layer $n$.
        \item \textbf{Principle of externality}: For every component or construct, of any layers, there are always the relative functional partition between \textbf{external processes} and \textbf{internal process}, every such concepts stripped down to the last implementation is expressed by an input-output procedure.  
        \item \textbf{Conjecture of recursive immergence}: With sufficient complexity, or given of given layer, one system can host another of the same type in itself, for such to enable self-consideration and recursive loops. \footnote{This conjecture follows from the observation that any sufficiently strong system, will have the ability to construct a logical system of similar complexity to itself in its own operating space. The foremost example is the action of simulating a computer, inside a computer. By the \textit{computational theory of mind}, if we take on the stance that computer to a given complexity is indeed intelligent being, then the conjecture follows.}
    \end{enumerate}
    For the set of construct $\mathcal{C}$, partitioned by layers similar to its internal structure, there exists a finite measurable set $\sigma\text{-}\mathcal{C}$ such that the set is expressed by $\sigma\text{-}\mathcal{C}[\mathcal{M}_{\sigma}]=\{C_{1},\dots,C_{n}\}$ for $n$ being such finite size. Then, with $A\in \mathcal{A}$ of the set of all subject reference, and $\mathcal{M}_{\sigma}\cong \mathcal{M}_{A}$ for $\mathcal{M}_{\sigma}(C_{k}): (\mathcal{F},P)\to T$, if $\mathcal{M}(\sigma\text{-}\mathcal{C})\cap \mathcal{M}_{A}(A)\neq \varnothing$, or, rather by $\mathcal{M}_{\sigma}(\sigma\text{-}\mathcal{C})\approx T_{A}$, then we call it \textbf{sufficiently intelligent}, and guarantees the existence of a given $C_{k}\in \mathcal{C}\supset \mathcal{\sigma\text{-}\mathcal{C}}$ that satisfies the above notion. \footnote{Note that however, this entire concept relies on the expanded order, perhaps analogically represented as a set funnel - then if $\mathcal{M}(\sigma\text{-}\mathcal{C})\cap \mathcal{M}_{A}(A)\neq \varnothing$ but also, $\mathcal{M}_{A}(A)\subset\mathcal{M}(\sigma\text{-}\mathcal{C})$, then we would have to call it \textit{above of the sufficient intelligent reference}. If this difference is over by a ratio $T_{\sigma\text{-}\mathcal{C}}/T_{A} = \lambda > 1$, we call it, if $A$ is the human references, then we would call it \textbf{artificial super-intelligence}.}
\end{theorem}
There are problems, of courses, and arguments against such construction made above, since it aligns and takes several aspects of old theories, beside the new content involved. Such problems must be addressed for the foundational basis to be considered broadly as much as possible, for the purpose of inclusion, and to define the practical limiting boundary onto current constructs. Additionally, from there, certain points and observations can be made, and new questions might be asked for problems that cannot be solved yet, for now. Arguing is the best source of thoughts, as the ancients predating us told of, rather true than not.

\section{Philosophical arguments against AI}
Philosophically, we have several considerations to make use of. Most of these comes of from historical remarks from ancient time (if to say 1700 is indeed ancient), and from 1950s onward, where capability of simulating models is possible. We would see what has been brought up about the topic of artificial intelligence, and especially, against the view of computationalism. 

\subsection{Descartes's argument (1700s)}
Descartes left a lot of his works upon the topic that now is conceived to be related to the philosophy of artificial intelligence, ignoring only the historical namesake itself. Most of his works are of great interest on the 'artificial construct' of the time - the idea and society of the 17th century's Age of Enlightenment's \textit{machina}. In the quest to understand his reasoning, for it holds values even now, and holds specific philosophical reasons in such discourse, we will examine \textbf{Discourse on Method}, the one specific piece of work directly target this line of thoughts. One of the first target, will be the distinction between man and machine. Though this has been directly argued above, in the example of Descartes and the machina animated to life, it is worthy of mention that this has not been solved, yet. In fact, one can even interpret the Turing test as one kind of empirical boundary for Descartes' criterion - by setting out a boundary on which human-like reaction can take. Moreover, it presumes the justification and the criterion that we should interdict whenever a similar structure is proposed - would it be reactive enough as a man, years long ago predicted?
\subsection{Categorization - Strong and weak AI (1991)}
Boden's concept of artificial general intelligence resembles John Searl's "strong AI".

“Weak” AI can be defined as the form of AI that aims at a system able to pass not just the Turing Test (again, abbreviated as TT), but the Total Turing Test (Harnad 1991). In TTT, a machine must muster more than linguistic indistinguishability: it must pass for a human in all behaviours - throwing a baseball, eating, teaching a class, etc. According to Searl, "weak AI" is a computer that can behave as if it were thinking wisely.

"Strong AI" is then differently defined as a computer that actually thinks like humans. For a quote:

\blockquote[Searl, 1991]{According to strong AI \dots the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.}

The theme of strong AI was frequently discussed in the late 20th century - however, it became clear that in order for a computer to be a strong AI, it must resolve various difficult problems. The most difficult philosophical problem was \textbf{the frame problem}.
\subsubsection{The frame problem}
The \textbf{frame problem} is the problem that an AI cannot autonomously distinguish important factors from unimportant ones when it tries to cope with something in a certain situation. The problem arises, for example, when we let AI robots operate in the real world. This problem was proposed by John McCarthy and Patrick J. Hayes in 1969. This is considered a philosophical problem that cannot be merely reduced to a technical problem. Historically, the problem is narrowly defined for the field of \textit{logic-based artificial intelligence}. But it was taken up in an embellished and modified form by philosophers of mind, and given a wider interpretation, and hence, is since then applicable to almost all formal system that wishes to call themselves artificial intelligence. We will not cover all of it here, at least for now. For trusted literature, it is recommended to refer to the Stanford's article on the frame problem \cite{sep-frame-problem}, and other literatures \cite{FrameGryzJarek,SeagerFrameAxiological,Briggs2014MachineE}. 

For now, the problem is unresolved, per Boden and specialists. Although there is no consensus about the definition of the frame problem, we could say that this is a problem centred around the question of how we can make an AI memorize the 'tacit knowledge' that almost all human adult can have in a given context.

Take an example of the normal cashier. Theoretically, and perhaps realistically speaking, a high schooler can be trained in a rather hasty fashion to do the job. While doing such work, there are many factors we take for granted. For example, during the process of using the computer to input the amount of cash required per transaction making, there might be a few terminal differences between different interface. The cashier knows that, and adapt to it. If she encounters an angry or hurry customer, the cashier can also act accordingly, without the consideration for the performance. "If the customer is in a hurry, then maybe I should use this or that or skip this". Or even "If I push this button, then the trading screen appears". In fact, there are too much knowledge to be involved in such normal and particularly easy job. However, we do not have to input the knowledge that the stock market will affects the customer's money that you are indeed doing transactions, or the fact that if the customer comes in with a bag of money, then in some cases, there will be missing bills, simply because it is not concerned of such.

Considering this, it becomes clear that there is an infinite amount of knowledge the robot must memorize. Who can make such a list of knowledge, and how is it possible to make the robot memorize them? The reason why this happens is that, when a robot encounters a new situation that it has never experienced, it cannot autonomously judge what kind of coping would be important to itself and what kind of coping would not, and therefore it cannot adequately solve the problem it faces. It is interesting that humans seem to be able to solve this kind of problem.

According to Dreyfus \cite{DreyfusImpasse1979}, for traditional AI to have the capacity to solve the frame problem, and become a true AI, it must become the Heideggerian AI, which incorporates Vorhandenheit (presence-at-hand) and Zuhandenheit (readiness-to-hand). Examining Rodney Brooks' robot architecture, he said that the robot respond only to fixed features of the environment, not to context or changing significance. But the robot 'continually referring to its sensors rather than to an internal world model'. Then, would the act of choosing, implementing a good enough sensor which have in itself the impending sensory power in need, and most importantly have the best data coverage be enough to mitigate this? Since even for human, the sensory power is limited, since our limbs, nervous system and cognitive realization systems are finite, but perhaps we have a major sensor of which brought us the power of solving the frame problem locally, as well as responding to 'context or changing significance'? There might be, but then the question is quite straightforward: What (part) of human exhibits such trait? A further concession if made would be even more in line: which construct would give rise to such desired adaptation in the absence of rigorous human benchmark, but for lower-intelligent animals as comparative instead? And even then, would it be reasonable to call computers no better than the smallest reptiles?
\subsection{The Chinese Room Argument (1980s)}
One of the most prominent critique for the philosophy of "strong AI" is the Chinese Room Argument. Accordingly,

\blockquote[Searl, 1980s]{CRA is based on a thought-experiment in which Searle himself stars. He is inside a room; outside the room are native Chinese speakers who don't know that Searle is inside it. Searle-in-the-box, like Searle-in-real-life, doesn't know any Chinese, but is fluent in English. The Chinese speakers send cards into the room through a slot; on these cards are written questions in Chinese. The box, courtesy of Searle's secret work therein, returns cards to the native Chinese speakers as output. Searle's output is produced by consulting a rulebook: this book is a lookup table that tells him what Chinese to produce based on what is sent in. To Searle, the Chinese is all just a bunch of --- to use Searle's language --- squiggle-squoggles.}

We denote $O$ the observers (Chinese speaker), $i$, $o$ being the input, output accordingly, and the rulebook $P$.

The argument of this experiment is simple - the Searle (in the box) is supposed to be everything a computer can be, and because he doesn't understand Chinese, no computer could have such understanding. Searle is mindlessly moving around, and according to the argument, that's all computers do, fundamentally.

Nowadays, CRA, among AI practitioners, is generally rejected. Among these practitioners, there is John Pollock, who writes:

\blockquote[Pollock, 1995]{Once (my intelligent system) OSCAR is fully functional, the argument from analogy will lead us inexorably to attribute thoughts and feelings to OSCAR with precisely the same credentials with which we attribute them to human beings. Philosophical arguments to the contrary will be passé.}

Still, despite such argument, the relevance of CRA is actually more apparent than ever. The brute fact is that deeply semantic NLP is rarely even pursued, hence the proponents of CRA are certainly not the ones feeling some discomfort in the light of the current state of AI. Searl would rightly point to any of the success stories of AI, and still able to proclaim that what we want - of an artificially intelligent agent has not been made, and understanding has not been reach, and philosophically, we can't refute it. 
\subsubsection{An analysis of CRA}
In one way or another, CRA infers with the notion of an \textit{input-output} procedure. In such case, CRA clearly tells us that a simple, mundane notion of an input-output machine which 'does its tasks' would not be sufficient of receiving the clarification and qualification for being intelligent. Which, by all means and purposes, are true. The construct if given in such circumstances, of the thought experiments, represents, if we took out the part where the argument said Searl is supposed to be everything a computer can be, then it's true that such constructs are false in its claim that it can 'understand'. In fact, relatively simple, a given input-output mechanism, from what was observed from the outside, do not exhibit anything, and do not have the ability to even \textit{think}, regardless that is valid of such. If we are to stand by Descartes arguments, then it is even more of the truth - the system in which the thought experiment was conducted provides it with no capabilities of any such.

Then what would be of the Chinese Room Argument that is worth it to dissect? Well, firstly, the claims of, at least in the acute interpretation - the man in the box is supposed to be everything a computer can be is \textit{false}. IF we are to stand by our construction of facilities, then we are innately arguing about such facilities, and not the processes, and the underlying operations itself. Rather, we are complaining that the machine is not capable enough, which is true. But we also, to a given reference point, pointing to the \textbf{existential facilities} instead of the arbitrary, yet reasonable operational facilities instead for the comparison. And in fact, if we think of it in the layer construction, it makes more sense - each layer is classified given its arbitrary for now, an interpretation thereof. Such interpretation is contained for such layer, and hence cannot be thoroughly or at least in a glance, interpreted by lower-layer components. This construct offers a one-way restriction on the property of interpretability. In such case, the \textbf{System Reply} is partially right - the man inside can not be, by all means and purposes, understand what does it mean by even 'English', or 'Chinese'. And even if such English 'understanding' is embedded in the reasonable interpretable space of Searl in the room, then Chinese would appear to be entirely unknown, \textit{unless} there exists a helper tool to resolve the situation of undefined operation. Indirectly, this prevents a Descartes situation from happens - an undefined situation with particular way to resolve.

But more than that, what constitute the notion of understanding? Taken only from the setting of the thought experiment, we cannot do but deceptively assume that understanding a language is similar to giving it certain ruleset to transfer from this word to others word only. By that, converting from base-2 to base-10 works the same - you know how to do it, yet there are things that constitute the philosophical, higher-level notion of 'understanding' in such that allows you to actually understand the conversion - otherwise, the conversion is a blind matching. In fact, given the setting, would a conversion rulebook exists for such language? Language is, by itself, a very high-level concept. Translating from text to texts requires it not only to provide the definition matching, which could be reasonably identified by such notion like the rulebook mention, but the interpretation of the string is dictated by the logic of the language - the context in which it appears, the logical conformation that it contributes, the grammatical structure that makes sense of what is said and what is transferred, and else. Language itself, is a medium of information exchange, by one of its definition. A conversion does not constitute an understanding. And if the argument going back and forth is that Searl can somehow figure it out the patterns mean something, then it violates what I called the \textit{principle of externality} - the 'lower components' up to a given point in which the \textit{law of recursive immergence} does (not) apply, cannot implement its higher constructions. Then, the Chinese Room Argument can be interpreted, in somewhat meagre form, the argument against telling the current conformation as able to understand, while it is not.

The only thing here that is wrong, however, is the fact that Searl claims that it is all the computer can do. However, artificial intelligence, as for now, using this term since chapter 3 which is not yet here, is not a computer in its form. We say, however, for an \textit{artificial intelligent subject with computers as its existential facilities}, CRA is partially right. But partially wrong since the comparison is limited to a form of internal structure in a well-formed system. That is to said - we need to create the (a) construct(s) that exceed(s) such argument. The problem is, how?
\subsection{The Gödelian argument (1961)}
In 1961, J. R. Lucas presents the Gödelian argument against the existence of a "strong" AI. His proof is based on Gödel theorem, which is stated as followed:

\blockquote[Lucas, 1961]{In any consistent system which is strong enough to produce simple arithmetic there are formulae which cannot be proved-in-the-system, but which we can see to be true. Essentially, we consider the formula which says, in effect, "This formula is unprovable-in-the-system". If this formula were provable-in-the-system, we should have a contradiction: for if it were provable-in-the-system, then it would not be unprovable-in-the-system, so that "This formula is unprovable-in-the-system" would be false: equally, if it were provable-in-the-system, then it would not be false, but would be true, since in any consistent system nothing false can be proved-in-the-system, but only truths.}
This theorem holds for all formal systems which are consistent, adequate for simple arithmetic, and shows that those formal systems are incomplete, with some fact being true, but unprovable.
\blockquote[Lucas, 1961]{It is of the essence of being a machine, that it should be a concrete instantiation of a formal system. It follows that given any machine which is consistent and capable of doing simple arithmetic, there is a formula which is incapable of producing as being true\dots}

Further argued, he then comes to such conclusion that no machine can be a complete or adequate model of the mind, since "the mind are essentially different from machines". Lucas's defenders, Roger Penrose, also state in his \textit{Shadow of the Mind} (1994). A human mathematician, if presented with a sound formal system $F$, could argue as followed:
\blockquote[Penrose, 1992, 3.2]{Though I don't know that I necessarily am $F$, I conclude that if I were, then the system $F$ would have to be sound and, more to the point, $F'$ would have to be sound, where $F'$ is $F$ supplemented by the further assertion "I am $F$"\footnote{The phrase "I am $F$" is merely a shorthand for "$F$ encapsulates all the humanly accessible methods of mathematical proof"}. I perceive that it follows from the assumption that I am $F$ that the Gödel argument $G(F')$ would have to be true and, furthermore, that it would not be a consequence of $F'$. But I have just perceived that "If I happen to be $F$, then $G(F')$ would have to be true", and perceptions of this nature would be precisely what $F'$ is supposed to achieve. Since I am therefore capable of perceiving something beyond the powers of $F'$, I deduce that I cannot be $F$ after all. Moreover, this applies to any other system, in place of $F$.}

By default, the argument supplemented from Penrose raised the contradiction of proof-ness. The Godelian argument implicitly creates layers, and levels, on which one puts those languages they are abided to seem fit of their expressions on the shelf, by the order of \textit{effectiveness}. Such notion then, would make the advancement of machine to human seems perpetually, unsophisticatedly, inoperable and impossible in essence.

\subsubsection{Subtle remark}
Before even taking a stance on such argument, what is the meaning and interpretations, as well as obstensively why it is even important to divulge into such point? The answers might be a bit difficult.

Human is variedly different from machine, for the current time with all the knowledge at present. Truth to take, the action of writing this itself is part of the endeavour to discover one's self, or rather, to understand $F$ with the assertion of 'I am $F$', for now, that we can, and is doing. By the language and construction of contemporary and propositional logic, a machine cannot do that.\footnote{In general, we cannot even say that it is true of the truth that human actually differs from what is proposed to be perceiving $F'$ being $F$. For human understanding of ourselves alone, we are trying to fit it into the interpretation and the rough 'understanding' of human itself. That is, there exists the space of reason and argument of a scientist, which interpretation follows. If, supposedly, this interpretation is strong enough, then we might be able to perceive and understand ourselves from ourselves - a looped interpretability. This mechanism, if ever, is not well understood if exists.}

However, if the converse situation happens, where we cannot totally perceive what we actually think, and how it is formed - per metric, being either consciousness, or one's self, surprisingly, it does not support the previous argument from an intuitive view (Bear in mind that this is a non-rigorous study). If stays rigid as it is, not counting being dynamic as we want, the model created from a human being can only imitate and represents what directly is entailed in the human mind of interpretation and logics. But logic and interpretation is a construct of the mind, for all intents and purposes, to directly infers to the physical world, the living world. However, if one is to use such inference on itself, for example, examining the brain itself, then to a certain point, what can be deduced from such observation can only fit in the interpretation space of what its creator, the human brain itself, can contrive. Thereby, we might conclude that figuratively, even human cannot understand human itself, from certain perspective. But the quality of succinctly interesting loop is to be taken seriously. The point now is, what type of construct, even logic, would be sufficient of taking the understanding, and will it make uses of the looped behaviors? By that, we then argue superficially that anything that relies on the machine cannot model the human existence and conscience itself. There are some assumptions thereof in the argument:
\begin{itemize}[noitemsep,topsep=1pt]
    \item Existences and the state of the world are in fact, modelled in mathematics, for one way or another. This is to facilitate the use of formal system in the argument. Everything is a set of rules, in which things operates.
    \item A machine per its definition, cybernetical machines are of all expressed by the single principle that it is born out of a formal system itself.
    \item Truth is the finite quantity that exists in such formal system, and is absolute. 
    \item The mind is an entity of which is inherently different from the logic of formal system.
\end{itemize}

Those fundamental, overlapping assumptions make up the bulk of the Gödelian argument, from the surface. However, is that true of all the merit? \footnote{It turns out, however, the Godelian argument has various proponents and opponents, and there are arguments of it being false. See (Bringsjord, 2001) for such argument, but it can be simplified as this. The Godelian argument makes use of two assumptions: $G(F')$ is true for a Godelian statement, and $F'\not\vdash G(F')$ for $F'\not \vdash G(F')$ for $F'$ being "I am $F$" with added semantic. Then, the statement on $G(F')$ is true is nothing but a \textit{satisfaction} claim, of meta-mathematical assertion which can be reduced to $\mathcal{I}\vDash G(F')$ is true for given interpretation $\mathcal{I}$. Thereby, there exists no contradiction thereof.}