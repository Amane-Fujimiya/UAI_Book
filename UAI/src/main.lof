\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces Well, look at that. Can you guess if we are about to face another one soon enough?}}{9}{figure.caption.6}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The diagram of a typical expert system. Here, human involvement is fairly representative in the figure, and is illustratively indicating the overwhelming reliance on human touches.}}{11}{figure.caption.7}%
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces An illustration of a vector in two-dimensional vector form in endpoints representation, and directional-magnitude representation.}}{55}{figure.caption.21}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces The \textbf {parallelogram law} for vector addition of two vectors $x$ and $y$ on adjacent side.}}{56}{figure.caption.22}%
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {6.1}{\ignorespaces Markov's inequality bounds the probability for the shaded region $\mathbb {P}[X\geq a]$}}{80}{figure.caption.35}%
\addvspace {10pt}
\contentsline {figure}{\numberline {7.1}{\ignorespaces The problem-solving scheme from the mathematical modelling perspective}}{93}{figure.caption.37}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces There exists an unbreakable wall in the black-box condition - throwing a dart in blind, except perhaps it can be right.}}{94}{figure.caption.38}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces A typical pendulum with degree 1, for parameter $\theta $ as angle, and a rod of length $\ell $ connecting the origin to the mass $m$.}}{95}{figure.caption.39}%
\contentsline {figure}{\numberline {7.4}{\ignorespaces With the question $Q$, you can ask everything, including the... not so pure one.}}{98}{figure.caption.40}%
\contentsline {figure}{\numberline {7.5}{\ignorespaces Plato's allegory of the cave by Jan Saenredam, according to Cornelis van Haarlem, 1604, Albertina, Vienna}}{101}{figure.caption.44}%
\addvspace {10pt}
\contentsline {figure}{\numberline {8.1}{\ignorespaces An illustration of statistical learning theory on the evaluation of the risks and errors, during learning process. $c'$ is presented in the `orbital' vicinity around $c$, with its distance of certain metric define how 'accurate' the reconstruction from distribution can be. Of the hypothesis set $\mathcal {H}$, there exists the Bayes hypothesis $h_{B}$ and an arbitrary `random' hypothesis $h$, and their respective measure.}}{111}{figure.caption.45}%
\contentsline {figure}{\numberline {8.2}{\ignorespaces An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal {D}$, and second is the $\mathsf {Update}$ process to re-align $c$ to the actual target.}}{114}{figure.caption.47}%
\contentsline {figure}{\numberline {8.3}{\ignorespaces VC-dimension of intervals on the real line. (a) Any two points can be shattered. (b) No sample of three points can be shattered as the $(+,-,+)$ labelling cannot be realized. Taken from \cite {10.5555/2371238}.}}{138}{figure.caption.51}%
\contentsline {figure}{\numberline {8.4}{\ignorespaces Unrealizable dichotomies for four points using hyperplanes in $\mathbb {R}^{2}$. (a) All four points lie on the convex hull. (b) Three points lie on the convex hull while the remaining point is interior. Taken from \cite {10.5555/2371238}.}}{139}{figure.caption.52}%
\addvspace {10pt}
\contentsline {figure}{\numberline {9.1}{\ignorespaces The simplistic, schematic illustration of the structure of the biological neuron.}}{142}{figure.caption.53}%
\contentsline {figure}{\numberline {9.2}{\ignorespaces An illustration of Santiago Ram√≥n y Cajal on the structure and design of a biological brain network. Many of these was made during his career.}}{144}{figure.caption.54}%
\contentsline {figure}{\numberline {9.3}{\ignorespaces Examples of the rich variety of nerve cell morphologies found in the human nervous system. Tracings are from actual nerve cells stained by impregnation with silver salts (the socalled Golgi technique the method used in the classical studies of Golgi and Cajal). Asterisks indicate that the axon runs on much farther than shown. Note that some cells, like the retinal bipolar cell, have a very short axon, and that others, like the retinal amacrine cell, have no axon at all. The drawings are not all at the same scale. Some more details about the jargon is the \textit {\color {orange!70!black}retinal bipolar cells}, which are neurons that connect the outer retina to the inner retina, for processing layer (or projection neurons, where all information are relayed from this connection.); the \textit {\color {orange!70!black}retinal ganglion cell, amacrine cells} are the same visual processing unit; Cerebellar Purkinje cells (a type of GABAergic neurons) uniquely determined for cerebella cortex (for processing large data, and coordinating functions like cognition and emotions.). Reused from \cite {purves_neuroscience_2004}.}}{145}{figure.caption.55}%
\contentsline {figure}{\numberline {9.4}{\ignorespaces An illustrative example of the abstraction and categorization by 'size' of different components and constructs in the neuron model. By the order of abstraction $k$, we assign a notion of size on different neural structure, by increasing complexity, and backward compatibility (described to be composed of previously defined objects). The first two stage for $k=1,2$ includes the standard basis components $\{n_{0,i}\}$ and the standard neuron class $N_{0}$, respectively.}}{147}{figure.caption.56}%
\contentsline {figure}{\numberline {9.5}{\ignorespaces The standard minimal configuration of any neuron $x\in \mathcal {N}_{i}$. We denote $p$, $q$ for particular neuron input and output sequences.}}{151}{figure.caption.57}%
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {11.1}{\ignorespaces Minimal neuron structure}}{160}{figure.caption.58}%
\contentsline {figure}{\numberline {11.2}{\ignorespaces The compound structure construction. The same component can be seen, for $n_i,n_o$ and $M$. Multiple consecutive components construct some components, and further outward. Also, we also reflect the complexity of $\mathcal {C}$ for a given architecture.}}{161}{figure.caption.59}%
\addvspace {10pt}
\contentsline {figure}{\numberline {12.1}{\ignorespaces (a) A typical example of bias-variance tradeoff in a statistical dataset. (b) When graphed into a continuous notion, we gain the complexity-error graph. Notice that it specifically goes for the \textit {\color {orange!70!black}test error}, which fits - the representative problem of prediction.}}{168}{figure.caption.60}%
\contentsline {figure}{\numberline {12.2}{\ignorespaces {\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}.}}{169}{figure.caption.61}%
\contentsline {figure}{\numberline {12.4}{\ignorespaces {\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs.}}{172}{figure.caption.66}%
\contentsline {figure}{\numberline {12.3}{\ignorespaces {\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Reproduced from \cite {nakkiran_deep_2019}. }}{172}{figure.caption.65}%
\contentsline {figure}{\numberline {12.5}{\ignorespaces The representative order of representation and description. As of the name implied, in transition to a mathematical formalism and language, there must then exist a representation to each and every element of certain subject. The process of doing is this called \textit {\color {orange!70!black}external encoding}, and is true also between portion of mathematical-encoded system to each other, if they are distinct. The reverse act is called again, \textit {\color {orange!70!black}decoding}, and between mathematical subjects to each other might as well be called \textit {\color {orange!70!black}internal encoding}, with respect to the mathematical language.}}{182}{figure.caption.68}%
\contentsline {figure}{\numberline {12.6}{\ignorespaces A conceptual illustration on the running flow of an $n$-layer GNN on particular structure of interest. Note that the data section itself has particular embedding structure on its own.}}{201}{figure.caption.72}%
\addvspace {10pt}
\contentsline {figure}{\numberline {13.1}{\ignorespaces (a) Figure of the original organization of the biological model of the brain functions. (b) Specifically, note that it is specifically for optical case, but can be extended to others type. Furthermore, the layer between last $A$-unit and the response units, there exists a pattern of feedback loop.}}{205}{figure.caption.73}%
\addvspace {10pt}
\contentsline {figure}{\numberline {1}{\ignorespaces The typical hard limit transfer function with fixed $a$, and fixed range for $x$ in $[0,1]$.}}{209}{figure.caption.78}%
\contentsline {figure}{\numberline {2}{\ignorespaces The typical hard limit transfer function with variable inhibition $a$, and fixed range for $x$ in $[0,1]$.}}{210}{figure.caption.79}%
\contentsline {figure}{\numberline {3}{\ignorespaces The typical symmetric hard limit transfer function with static inhibition $a$, and fixed range for $x$ in $[-1,+1]$. As specified, this is the normal-extended range.}}{211}{figure.caption.81}%
\contentsline {figure}{\numberline {4}{\ignorespaces The saturating linear with linear region of $[0,1]$. A smoother variation would be something like sigmoidal functions, that is.}}{211}{figure.caption.83}%
\contentsline {figure}{\numberline {5}{\ignorespaces The symmetric saturating linear with linear region of $[-1,1]$, a positive-negative variation of the saturating linear.}}{212}{figure.caption.84}%
\contentsline {figure}{\numberline {6}{\ignorespaces The sigmoidal function channel}}{212}{figure.caption.86}%
\contentsline {figure}{\numberline {7}{\ignorespaces The logarithmic sigmoidal function channel. Notice that the range of \texttt {logsigmoid} is $[-\infty , 0]$, making it somewhat weird of a choice for a transfer function.}}{212}{figure.caption.87}%
\contentsline {figure}{\numberline {8}{\ignorespaces The hyperbolic tangent transfer function channel.}}{213}{figure.caption.89}%
