
@inproceedings{oono_graph_2020,
	title = {Graph {Neural} {Networks} {Exponentially} {Lose} {Expressive} {Power} for {Node} {Classification}},
	url = {https://openreview.net/forum?id=S1ldO2EFPr},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Oono, Kenta and Suzuki, Taiji},
	year = {2020},
}

@book{kearns_introduction_1994,
	address = {Cambridge, MA, USA},
	title = {An introduction to computational learning theory},
	isbn = {0-262-11193-4},
	publisher = {MIT Press},
	author = {Kearns, Michael J. and Vazirani, Umesh V.},
	year = {1994},
}

@article{brown_biasvariance_2024,
	title = {Bias/{Variance} is not the same as {Approximation}/{Estimation}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=4TnFbv16hK},
	journal = {Transactions on Machine Learning Research},
	author = {Brown, Gavin and Ali, Riccardo},
	year = {2024},
}

@misc{pfau_generalized_2013,
	title = {A {Generalized} {Bias}-{Variance} {Decomposition} for {Bregman} {Divergences}},
	author = {Pfau, David},
	year = {2013},
	note = {Published: Technical report},
}

@book{goodfellow_deep_2016,
	title = {Deep learning},
	volume = {1},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	year = {2016},
}

@article{valiant_theory_1984,
	title = {A theory of the learnable},
	volume = {27},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1968.1972},
	doi = {10.1145/1968.1972},
	number = {11},
	journal = {Commun. ACM},
	author = {Valiant, L. G.},
	month = nov,
	year = {1984},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {inductive inference, probabilistic models of learning, propositional expressions},
	pages = {1134--1142},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65 6},
	url = {https://api.semanticscholar.org/CorpusID:12781225},
	journal = {Psychological review},
	author = {Rosenblatt, Frank},
	year = {1958},
	pages = {386--408},
}

@misc{shi_homophily_2024,
	title = {Homophily modulates double descent generalization in graph convolution networks},
	url = {https://arxiv.org/abs/2212.13069},
	author = {Shi, Cheng and Pan, Liming and Hu, Hong and Dokmanić, Ivan},
	year = {2024},
	note = {\_eprint: 2212.13069},
}

@book{hajek_statistical_2021,
	title = {Statistical {Learning} {Theory}},
	volume = {1},
	url = {https://maxim.ece.illinois.edu/teaching/SLT/},
	author = {Hajek, Bruce and Raginsky, Maxim},
	year = {2021},
}

@misc{bousquet_theory_2020,
	title = {A {Theory} of {Universal} {Learning}},
	url = {https://arxiv.org/abs/2011.04483},
	author = {Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Handel, Ramon van and Yehudayoff, Amir},
	year = {2020},
	note = {\_eprint: 2011.04483},
}

@book{shalev-shwartz_understanding_2014,
	address = {USA},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {1-107-05713-2},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@inproceedings{sharma_bias-variance_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Bias-variance tradeoffs in program analysis},
	isbn = {978-1-4503-2544-8},
	url = {https://doi.org/10.1145/2535838.2535853},
	doi = {10.1145/2535838.2535853},
	abstract = {It is often the case that increasing the precision of a program analysis leads to worse results. It is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs. We show that bias-variance tradeoffs, an idea from learning theory, can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for coping with such limitations. Learning theory captures precision using a combinatorial quantity called the VC dimension. We compute the VC dimension for different abstractions and report on its usefulness as a precision metric for program analyses. We evaluate cross validation, a technique for addressing bias-variance tradeoffs, on an industrial strength program verification tool called YOGI. The tool produced using cross validation has significantly better running time, finds new defects, and has fewer time-outs than the current production version. Finally, we make some recommendations for tackling bias-variance tradeoffs in program analysis.},
	urldate = {2024-07-15},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Sharma, Rahul and Aiken, Alex},
	year = {2014},
	pages = {127--137},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\LEF8MS34\\Sharma et al. - 2014 - Bias-variance tradeoffs in program analysis.pdf:application/pdf},
}

@misc{noauthor_bias-variance_nodate,
	title = {Bias-variance tradeoffs in program analysis {\textbackslash}textbar {Proceedings} of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	url = {https://dl.acm.org/doi/10.1145/2535838.2535853},
	urldate = {2024-07-16},
	file = {Bias-variance tradeoffs in program analysis | Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages:C\:\\Users\\Admin\\Zotero\\storage\\KNKR324X\\2535838.html:text/html},
}

@misc{mehrabi_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1908.09635},
	abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.1908.09635},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\BIXE6VPN\\Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\BQ4JYISY\\1908.html:text/html},
}

@misc{schaeffer_double_2023,
	title = {Double {Descent} {Demystified}: {Identifying}, {Interpreting} \& {Ablating} the {Sources} of a {Deep} {Learning} {Puzzle}},
	shorttitle = {Double {Descent} {Demystified}},
	url = {http://arxiv.org/abs/2303.14151},
	abstract = {Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila Rani and Koyejo, Oluwasanmi},
	month = mar,
	year = {2023},
	doi = {10.48550/arXiv.2303.14151},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\7DQ9LS62\\Schaeffer et al. - 2023 - Double Descent Demystified Identifying, Interpret.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\M4VQGEDJ\\2303.html:text/html},
}

@misc{hellstrom_bias_2020,
	title = {Bias in {Machine} {Learning} – {What} is it {Good} for?},
	url = {http://arxiv.org/abs/2004.00686},
	abstract = {In public media as well as in scientific publications, the term {\textbackslash}textbackslashemph\{bias\} is used in conjunction with machine learning in many different contexts, and with many different meanings. This paper proposes a taxonomy of these different meanings, terminology, and definitions by surveying the, primarily scientific, literature on machine learning. In some cases, we suggest extensions and modifications to promote a clear terminology and completeness. The survey is followed by an analysis and discussion on how different types of biases are connected and depend on each other. We conclude that there is a complex relation between bias occurring in the machine learning pipeline that leads to a model, and the eventual bias of the model (which is typically related to social discrimination). The former bias may or may not influence the latter, in a sometimes bad, and sometime good way.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Hellström, Thomas and Dignum, Virginia and Bensch, Suna},
	month = sep,
	year = {2020},
	doi = {10.48550/arXiv.2004.00686},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\SBHVL63F\\Hellström et al. - 2020 - Bias in Machine Learning -- What is it Good for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\N42HHB4W\\2004.html:text/html},
}

@misc{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	doi = {10.48550/arXiv.1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\E3VRDICZ\\Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\XUQBTTFY\\1912.html:text/html},
}

@article{molavi_model_2024,
	title = {Model {Complexity}, {Expectations}, and {Asset} {Prices}},
	volume = {91},
	issn = {0034-6527},
	url = {https://doi.org/10.1093/restud/rdad073},
	doi = {10.1093/restud/rdad073},
	abstract = {This paper analyses how limits to the complexity of statistical models used by market participants can shape asset prices. We consider an economy in which the stochastic process that governs the evolution of economic variables may not have a simple representation, and yet, agents are only capable of entertaining statistical models with a certain level of complexity. As a result, they may end up with a lower-dimensional approximation that does not fully capture the intertemporal complexity of the true data-generating process. We first characterize the implications of the resulting departure from rational expectations and relate the extent of return and forecast-error predictability at various horizons to the complexity of agents’ models and the statistical properties of the underlying process. We then apply our framework to study violations of uncovered interest rate parity in foreign exchange markets. We find that constraints on the complexity of agents’ models can generate return predictability patterns that are simultaneously consistent with the well-known forward discount and predictability reversal puzzles.},
	number = {4},
	urldate = {2024-07-16},
	journal = {The Review of Economic Studies},
	author = {Molavi, Pooya and Tahbaz-Salehi, Alireza and Vedolin, Andrea},
	month = jul,
	year = {2024},
	pages = {2462--2507},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\TIVQMXRY\\Molavi et al. - 2024 - Model Complexity, Expectations, and Asset Prices.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\6FUGYD63\\7222145.html:text/html},
}

@misc{al_model_nodate,
	title = {Model {Complexity}, {Expectations}, and {Asset} {Prices} {\textbackslash}textbar {The} {Review} of {Economic} {Studies} {\textbackslash}textbar {Oxford} {Academic}},
	url = {https://academic.oup.com/restud/article-abstract/91/4/2462/7222145?redirectedFrom=fulltext&login=false},
	urldate = {2024-07-16},
	author = {al, Molavi et},
	file = {Model Complexity, Expectations, and Asset Prices | The Review of Economic Studies | Oxford Academic:C\:\\Users\\Admin\\Zotero\\storage\\FS8VLBU7\\7222145.html:text/html},
}

@misc{buschjager_generalized_2020,
	title = {Generalized {Negative} {Correlation} {Learning} for {Deep} {Ensembling}},
	url = {http://arxiv.org/abs/2011.02952},
	abstract = {Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm's error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble's diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks. We make our code publicly available under https://github.com/sbuschjaeger/gncl},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Buschjäger, Sebastian and Pfahler, Lukas and Morik, Katharina},
	month = dec,
	year = {2020},
	doi = {10.48550/arXiv.2011.02952},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\MLP5A4SK\\Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\2NTBSA7B\\2011.html:text/html},
}

@misc{buschjager_generalized_2020-1,
	title = {Generalized {Negative} {Correlation} {Learning} for {Deep} {Ensembling}},
	url = {http://arxiv.org/abs/2011.02952},
	abstract = {Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm’s error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble’s diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks. We make our code publicly available under https: //github.com/sbuschjaeger/gncl.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Buschjäger, Sebastian and Pfahler, Lukas and Morik, Katharina},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:C\:\\Users\\Admin\\Zotero\\storage\\UXDWT2W3\\Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:application/pdf},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1812.11118},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
	number = {32},
	urldate = {2024-07-16},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {15849--15854},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\FUNQ8YWT\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\269CSDF5\\1812.html:text/html},
}

@article{belkin_reconciling_2019-1,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1812.11118},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the ﬁeld, the bias-variance trade-oﬀ, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-oﬀ implies that a model should balance under-ﬁtting and over-ﬁtting: rich enough to express underlying structure in data, simple enough to avoid ﬁtting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly ﬁt (i.e., interpolate) the data. Classically, such models would be considered over-ﬁt, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.},
	language = {en},
	number = {32},
	urldate = {2024-07-16},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {15849--15854},
	file = {Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:C\:\\Users\\Admin\\Zotero\\storage\\QK9L5RXQ\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:application/pdf},
}

@misc{yang_rethinking_2020,
	title = {Rethinking {Bias}-{Variance} {Trade}-off for {Generalization} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2002.11328},
	abstract = {The classical bias-variance trade-off predicts that bias decreases and variance increases with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and conﬁrm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random ﬁrst layer. Finally, evaluation on out-ofdistribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we ﬁnd that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:C\:\\Users\\Admin\\Zotero\\storage\\QTG7394T\\Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:application/pdf},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {A} {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications} {\textbackslash}textbar {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	urldate = {2024-07-16},
	file = {[PDF] A Unifeid Bias-Variance Decomposition and its Applications | Semantic Scholar:C\:\\Users\\Admin\\Zotero\\storage\\K9YRLG57\\e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3.html:text/html},
}

@misc{noauthor_pdf_nodate-1,
	title = {[{PDF}] {A} {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications} {\textbackslash}textbar {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	urldate = {2024-07-16},
	file = {[PDF] A Unifeid Bias-Variance Decomposition and its Applications | Semantic Scholar:C\:\\Users\\Admin\\Zotero\\storage\\FBR4NQ7L\\e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3.html:text/html},
}

@inproceedings{domingos_unifeid_2000,
	title = {A {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	abstract = {This paper presents a unified bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassification costs, and other loss functions. The unified decomposition sheds light on a number of significant issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.’s (1997) notion of margin, and the nature of the trade-off between bias and variance in classification. While the biasvariance behavior of zero-one loss and variable misclassification costs is quite different from that of squared loss, this difference derives directly from the different definitions of loss. We have applied the proposed decomposition to decision tree learning, instancebased learning and boosting on a large suite of benchmark data sets, and made several significant observations.},
	urldate = {2024-07-16},
	booktitle = {Semantic {Scholar}},
	author = {Domingos, Pedro M.},
	month = jun,
	year = {2000},
}

@misc{khan_bayesian_2024,
	title = {The {Bayesian} {Learning} {Rule}},
	url = {http://arxiv.org/abs/2107.04562},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the {\textbackslash}textbackslashemph\{Bayesian learning rule\}. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Khan, Mohammad Emtiyaz and Rue, Håvard},
	month = jun,
	year = {2024},
	doi = {10.48550/arXiv.2107.04562},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\PFDDECCU\\Khan and Rue - 2024 - The Bayesian Learning Rule.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\7CTS92RQ\\2107.html:text/html},
}

@misc{hu_model_2021,
	title = {Model {Complexity} of {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Model {Complexity} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/2103.05127},
	abstract = {Model complexity is a fundamental problem in deep learning. In this paper we conduct a systematic overview of the latest studies on model complexity in deep learning. Model complexity of deep learning can be categorized into expressive capacity and effective model complexity. We review the existing studies on those two categories along four important factors, including model framework, model size, optimization process and data complexity. We also discuss the applications of deep learning model complexity including understanding model generalization, model optimization, and model selection and design. We conclude by proposing several interesting future directions.},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Hu, Xia and Chu, Lingyang and Pei, Jian and Liu, Weiqing and Bian, Jiang},
	month = aug,
	year = {2021},
	doi = {10.48550/arXiv.2103.05127},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\JF2YWYGM\\Hu et al. - 2021 - Model Complexity of Deep Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\XZY6J9UM\\2103.html:text/html},
}

@misc{noauthor_neural_nodate,
	title = {Neural {Networks} and the {Bias}/{Variance} {Dilemma} {\textbackslash}textbar {MIT} {Press} {Journals} \& {Magazine} {\textbackslash}textbar {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/6797087},
	urldate = {2024-07-18},
	file = {Neural Networks and the Bias/Variance Dilemma | MIT Press Journals & Magazine | IEEE Xplore:C\:\\Users\\Admin\\Zotero\\storage\\Y3295S9M\\6797087.html:text/html},
}

@inproceedings{quetu_can_2023,
	title = {Can we avoid {Double} {Descent} in {Deep} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/2302.13259},
	doi = {10.1109/ICIP49359.2023.10222624},
	abstract = {Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the “double descent”, has caught the attention of the deep learning community. As the model’s size grows, the performance gets first worse and then goes back to improving. It raises serious questions about the optimal model’s size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple ℓ2 regularization is already positively contributing to such a perspective.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {2023 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Quétu, Victor and Tartaglione, Enzo},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {1625--1629},
	file = {Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:C\:\\Users\\Admin\\Zotero\\storage\\H2GEN5PC\\Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:application/pdf},
}

@misc{lafon_understanding_2024,
	title = {Understanding the {Double} {Descent} {Phenomenon} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2403.10459},
	abstract = {Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Lafon, Marc and Thomas, Alexandre},
	month = mar,
	year = {2024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lafon and Thomas - 2024 - Understanding the Double Descent Phenomenon in Dee.pdf:C\:\\Users\\Admin\\Zotero\\storage\\WWDXUANR\\Lafon and Thomas - 2024 - Understanding the Double Descent Phenomenon in Dee.pdf:application/pdf},
}

@misc{davies_unifying_2023,
	title = {Unifying {Grokking} and {Double} {Descent}},
	url = {http://arxiv.org/abs/2303.06173},
	abstract = {A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied grokking, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superﬁcially similar double descent. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the ﬁrst demonstration of model-wise grokking.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Davies, Xander and Langosco, Lauro and Krueger, David},
	month = mar,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf:C\:\\Users\\Admin\\Zotero\\storage\\K34TZ79W\\Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf:application/pdf},
}

@inproceedings{quetu_can_2023-1,
	title = {Can we avoid {Double} {Descent} in {Deep} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/2302.13259},
	doi = {10.1109/ICIP49359.2023.10222624},
	abstract = {Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the “double descent”, has caught the attention of the deep learning community. As the model’s size grows, the performance gets first worse and then goes back to improving. It raises serious questions about the optimal model’s size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple ℓ2 regularization is already positively contributing to such a perspective.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {2023 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Quétu, Victor and Tartaglione, Enzo},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {1625--1629},
	file = {Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:C\:\\Users\\Admin\\Zotero\\storage\\A4AAW8KD\\Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:application/pdf},
}

@inproceedings{d_ascoli_triple_2020,
	title = {Triple descent and the two kinds of overfitting: where \& why do they appear?},
	volume = {33},
	shorttitle = {Triple descent and the two kinds of overfitting},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1fd09c5f59a8ff35d499c0ee25a1d47e-Abstract.html},
	abstract = {A recent line of research has highlighted the existence of a “double descent” phenomenon in deep learning, whereby increasing the number of training examples N causes the generalization error of neural networks to peak when N is of the same order as the number of parameters P. In earlier works, a similar phenomenon was shown to exist in simpler models such as linear regression, where the peak instead occurs when N is equal to the input dimension D. Since both peaks coincide with the interpolation threshold, they are often conflated in the litterature. In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is then governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent. As shown previously, the nonlinear peak at N=P is a true divergence caused by the extreme sensitivity of the output function to both the noise corrupting the labels and the initialization of the random features (or the weights in neural networks). This peak survives in the absence of noise, but can be suppressed by regularization. In contrast, the linear peak at N=D is solely due to overfitting the noise in the labels, and forms earlier during training. We show that this peak is implicitly regularized by the nonlinearity, which is why it only becomes salient at high noise and is weakly affected by explicit regularization. Throughout the paper, we compare the analytical results obtained in the random feature model with the outcomes of numerical experiments involving realistic neural networks.},
	urldate = {2024-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {d' Ascoli, Stéphane and Sagun, Levent and Biroli, Giulio},
	year = {2020},
	pages = {3058--3069},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\WMCBKWNA\\d' Ascoli et al. - 2020 - Triple descent and the two kinds of overfitting w.pdf:application/pdf},
}

@misc{liu_understanding_2023,
	title = {Understanding the {Role} of {Optimization} in {Double} {Descent}},
	url = {https://arxiv.org/abs/2312.03951},
	author = {Liu, Chris Yuhao and Flanigan, Jeffrey},
	year = {2023},
	note = {\_eprint: 2312.03951},
}

@misc{olmin_towards_2024,
	title = {Towards {Understanding} {Epoch}-wise {Double} descent in {Two}-layer {Linear} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2407.09845},
	author = {Olmin, Amanda and Lindsten, Fredrik},
	year = {2024},
	note = {\_eprint: 2407.09845},
}

@inproceedings{domingos_unified_2000,
	title = {A {Unified} {Bias}-{Variance} {Decomposition} for {Zero}-{One} and {Squared} {Loss}},
	url = {https://api.semanticscholar.org/CorpusID:2063488},
	booktitle = {{AAAI}/{IAAI}},
	author = {Domingos, Pedro M.},
	year = {2000},
}

@article{hamilton_graph_nodate,
	title = {Graph {Representation} {Learning}},
	volume = {14},
	number = {3},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Hamilton, William L.},
	note = {Publisher: Morgan and Claypool},
	pages = {1--159},
}

@book{mohri_foundations_2012,
	title = {Foundations of {Machine} {Learning}},
	isbn = {0-262-01825-X},
	abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.},
	publisher = {The MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2012},
}

@book{sugiyama_introduction_2015,
	address = {San Francisco, CA, USA},
	title = {Introduction to {Statistical} {Machine} {Learning}},
	isbn = {978-0-12-802350-1},
	abstract = {Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. Introduction to Statistical Machine Learning provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks. Provides the necessary background material to understand machine learning such as statistics, probability, linear algebra, and calculus. Complete coverage of the generative approach to statistical pattern recognition and the discriminative approach to statistical machine learning. Includes MATLAB/Octave programs so that readers can test the algorithms numerically and acquire both mathematical and practical skills in a wide range of data analysis tasks Discusses a wide range of applications in machine learning and statistics and provides examples drawn from image processing, speech processing, natural language processing, robot control, as well as biology, medicine, astronomy, physics, and materials. Table of Contents Part I: Introduction to Statistics and Probability 1. Random variables and probability distributions 2. Examples of discrete probability distributions 3. Examples of continuous probability distributions 4. Multi-dimensional probability distributions 5. Examples of multi-dimensional probability distributions 6. Random sample generation from arbitrary probability distributions 7. Probability distributions of the sum of independent random variables 8. Probability inequalities 9. Statistical inference 10. Hypothesis testing Part II: Generative Approach to Statistical Pattern Recognition 11. Fundamentals of statistical pattern recognition 12. Criteria for developing classifiers 13. Maximum likelihood estimation 14. Theoretical properties of maximum likelihood estimation 15. Linear discriminant analysis 16. Model selection for maximum likelihood estimation 17. Maximum likelihood estimation for Gaussian mixture model 18. Bayesian inference 19. Numerical computation in Bayesian inference 20. Model selection in Bayesian inference 21. Kernel density estimation 22. Nearest neighbor density estimation Part III: Discriminative Approach to Statistical Machine Learning 23. Fundamentals of statistical machine learning 24. Learning Models 25. Least-squares regression 26. Constrained least-squares regression 27. Sparse regression 28. Robust regression 29. Least-squares classification 30. Support vector classification 31. Ensemble classification 32. Probabilistic classification 33. Structured classification Part IV: Further Topics 34. Outlier detection 35. Unsupervised dimensionality reduction 36. Clustering 37. Online learning 38. Semi-supervised learning 39. Supervised dimensionality reduction 40. Transfer learning 41. Multi-task learning},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Sugiyama, Masashi},
	year = {2015},
}

@book{vapnik_nature_1999,
	title = {The {Nature} of {Statistical} {Learning} {Theory}},
	publisher = {Springer: New York},
	author = {Vapnik, Vladimir},
	year = {1999},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	doi = {10.1109/TNN.2008.2005605},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	year = {2009},
	keywords = {Biological system modeling, Biology, Chemistry, Computer vision, Data engineering, Data mining, graph neural networks (GNNs), graph processing, Graphical domains, Neural networks, Parameter estimation, Pattern recognition, recursive neural networks, Supervised learning},
	pages = {61--80},
}

@article{bronstein_geometric_2017,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} data},
	volume = {34},
	issn = {1558-0792},
	url = {http://dx.doi.org/10.1109/MSP.2017.2693418},
	doi = {10.1109/msp.2017.2693418},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = jul,
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {18--42},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	url = {https://arxiv.org/abs/2104.13478},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	year = {2021},
	note = {\_eprint: 2104.13478},
}

@article{velickovic_everything_2023,
	title = {Everything is connected: {Graph} neural networks},
	volume = {79},
	issn = {0959-440X},
	url = {http://dx.doi.org/10.1016/j.sbi.2023.102538},
	doi = {10.1016/j.sbi.2023.102538},
	journal = {Current Opinion in Structural Biology},
	author = {Veličković, Petar},
	month = apr,
	year = {2023},
	note = {Publisher: Elsevier BV},
	pages = {102538},
}

@misc{tanis_introduction_2024,
	title = {Introduction to {Graph} {Neural} {Networks}: {A} {Starting} {Point} for {Machine} {Learning} {Engineers}},
	url = {https://arxiv.org/abs/2412.19419},
	author = {Tanis, James H. and Giannella, Chris and Mariano, Adrian V.},
	year = {2024},
	note = {\_eprint: 2412.19419},
}

@misc{lopushanskyy_graph_2024,
	title = {Graph {Neural} {Networks} on {Graph} {Databases}},
	url = {https://arxiv.org/abs/2411.11375},
	author = {Lopushanskyy, Dmytro and Shi, Borun},
	year = {2024},
	note = {\_eprint: 2411.11375},
}

@article{geman_neural_1992,
	title = {Neural {Networks} and the {Bias}/{Variance} {Dilemma}},
	volume = {4},
	doi = {10.1162/neco.1992.4.1.1},
	number = {1},
	journal = {Neural Computation},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
	year = {1992},
	pages = {1--58},
}

@misc{fortmann_understanding_2012,
	title = {Understanding the {Bias}-{Variance} {Tradeoff}},
	url = {https://scott.fortmann-roe.com/docs/BiasVariance.html},
	urldate = {2025-02-26},
	author = {Fortmann, Scott},
	year = {2012},
	file = {Understanding the Bias-Variance Tradeoff:C\:\\Users\\Admin\\Zotero\\storage\\6JHUF46Z\\BiasVariance.html:text/html},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
}

@misc{noauthor_mathematical_2024,
	title = {Mathematical {Modeling} and {Simulation}: {Introduction} for {Scientists} and {Engineers}, 2nd {Edition} {\textbackslash}textbar {Wiley}},
	shorttitle = {Mathematical {Modeling} and {Simulation}},
	url = {https://www.wiley.com/en-us/Mathematical+Modeling+and+Simulation%3A+Introduction+for+Scientists+and+Engineers%2C+2nd+Edition-p-9783527839407},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater\textbf{Learn to use modeling and simulation methods to attack real-world problems, from physics to engineering, from life sciences to process engineering}{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreater\textbf{Reviews of the \textit{first edition}} (2009):{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreater"Perfectly fits introductory modeling courses [...] and is an enjoyable reading in the first place. Highly recommended [...]"{\textbackslash}textlessbr /{\textbackslash}textgreater\textbf{\textit{Zentralblatt MATH,}} European Mathematical Society, 2009{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreater"This book differs from almost all other available modeling books in that [the authors address] both mechanistic and statistical models as well as 'hybrid' models. [...] The modeling range is enormous."{\textbackslash}textlessbr /{\textbackslash}textgreater\textbf{\textit{SIAM Society of Industrial and Applied Mathematics,}} USA, 2011{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreaterThis completely revised and substantially extended second edition answers the most important questions in the field of modeling: What is a mathematical model? What types of models do exist? Which model is appropriate for a particular problem? What are simulation, parameter estimation, and validation? What kind of mathematical problems appear and how can these be efficiently solved using professional free of charge open source software?{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreaterThe book addresses undergraduates and practitioners alike. Although only basic knowledge of calculus and linear algebra is required, the most important mathematical structures are discussed in sufficient detail, ranging from statistical models to partial differential equations and accompanied by examples from biology, ecology, economics, medicine, agricultural, chemical, electrical, mechanical, and process engineering.{\textbackslash}textless/p{\textbackslash}textgreater {\textbackslash}textlessp{\textbackslash}textgreaterAbout 200 pages of additional material include a unique chapter on virtualization, Crash Courses on the data analysis and programming languages R and Python and on the computer algebra language Maxima, many new methods and examples scattered throughout the book, an update of all software-related procedures, and a comprehensive book software providing templates for typical modeling tasks in thousands of code lines. The book software includes GmLinux, an operating system specifically designed for this book providing preconfigured and ready-to-use installations of OpenFOAM, Salome, FreeCAD/CfdOF workbench, ParaView, R, Maxima/wxMaxima, Python, Rstudio, Quarto/Markdown and other free of charge open source software used in the book.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {en},
	urldate = {2025-04-04},
	year = {2024},
	note = {Publication Title: Wiley.com},
	file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\SMIMUHX3\\Mathematical+Modeling+and+Simulation+Introduction+for+Scientists+and+Engineers,+2nd+Edition-p-9.html:text/html},
}

@misc{al_history_2006,
	title = {The {History} of {Artificial} {Intelligence}},
	author = {al, Chris Smith et},
	year = {2006},
	note = {Published: Technical review},
}

@book{russell_artificial_2009,
	address = {USA},
	edition = {3rd},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}},
	isbn = {0-13-604259-7},
	publisher = {Prentice Hall Press},
	author = {Russell, Stuart and Norvig, Peter},
	year = {2009},
}

@incollection{shanahan_frame_2016,
	edition = {Spring 2016},
	title = {The {Frame} {Problem}},
	url = {https://plato.stanford.edu/archives/spr2016/entries/frame-problem/},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Shanahan, Murray},
	editor = {Zalta, Edward N.},
	year = {2016},
}

@article{gryz_frame_2013,
	title = {The {Frame} {Problem} in {Artificial} {Intelligence} and {Philosophy}},
	volume = {21},
	journal = {Filozofia Nauki},
	author = {Gryz, Jarek},
	month = jun,
	year = {2013},
	pages = {15--30},
}

@article{seager_frame_nodate,
	title = {Frame {Problems}, {Emotions} and {Axiological} {Projectionism}},
	journal = {Philosophical report},
	author = {Seager, William},
}

@inproceedings{briggs_machine_2014,
	title = {Machine {Ethics} , the {Frame} {Problem} , and {Theory} of {Mind}},
	url = {https://api.semanticscholar.org/CorpusID:14954096},
	author = {Briggs, Gordon},
	year = {2014},
}

@article{dreyfus_micro-worlds_1979,
	title = {From {Micro}-{Worlds} to {Knowledge} {Representation} : {AI} at an {Impasse}},
	author = {Dreyfus, Hubert L.},
	year = {1979},
}

@book{descartes_discourse_1950,
	address = {Harmondsworth,},
	title = {Discourse on {Method}},
	publisher = {Harmondsworth, Penguin},
	author = {Descartes, Rene?},
	year = {1950},
}

@article{newell_logic_1956,
	title = {The logic theory machine–{A} complex information processing system},
	volume = {2},
	doi = {10.1109/TIT.1956.1056797},
	number = {3},
	journal = {IRE Transactions on Information Theory},
	author = {Newell, A. and Simon, H.},
	year = {1956},
	keywords = {Automatic programming, Formal languages, Heuristic algorithms, Humans, Information analysis, Information processing, Logic, Pattern recognition, Problem-solving},
	pages = {61--79},
}

@article{goel_looking_2022,
	title = {Looking {Back}, {Looking} {Ahead}: {Symbolic} versus {Connectionist} {AI}},
	volume = {42},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15111},
	doi = {10.1609/aaai.12026},
	number = {4},
	journal = {AI Magazine},
	author = {Goel, Ashok},
	month = jan,
	year = {2022},
	pages = {83--85},
}

@misc{abhishek_introduction_2019,
	title = {Introduction to {Concentration} {Inequalities}},
	url = {https://arxiv.org/abs/1910.02884},
	author = {Abhishek, Kumar and Maheshwari, Sneha and Gujar, Sujit},
	year = {2019},
	note = {\_eprint: 1910.02884},
}

@book{boucheron_concentration_2013,
	title = {Concentration {Inequalities}: {A} {Nonasymptotic} {Theory} of {Independence}},
	isbn = {978-0-19-953525-5},
	url = {https://doi.org/10.1093/acprof:oso/9780199535255.001.0001},
	abstract = {This monograph presents a mathematical theory of concentration inequalities for functions of independent random variables. The basic phenomenon under investigation is that if a function of many independent random variables does not depend too much on any of them then it is concentrated around its expected value. This book offers a host of inequalities to quantify this statement. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities, transportation arguments, to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are presented. The book offers a self-contained introduction to concentration inequalities, including a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed. Special attention is paid to applications to the supremum of empirical processes.},
	publisher = {Oxford University Press},
	author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
	month = feb,
	year = {2013},
	doi = {10.1093/acprof:oso/9780199535255.001.0001},
	doi = {10.1093/acprof:oso/9780199535255.001.0001},
	doi = {10.1093/acprof:oso/9780199535255.001.0001},
}

@misc{neal_bias-variance_2019,
	title = {On the {Bias}-{Variance} {Tradeoff}: {Textbooks} {Need} an {Update}},
	url = {https://arxiv.org/abs/1912.08286},
	author = {Neal, Brady},
	year = {2019},
	note = {\_eprint: 1912.08286},
}

@article{grenander_empirical_1952,
	title = {On empirical spectral analysis of stochastic processes},
	volume = {1},
	url = {https://api.semanticscholar.org/CorpusID:122878699},
	journal = {Arkiv för Matematik},
	author = {Grenander, Ulf},
	year = {1952},
	pages = {503--531},
}

@article{sterkenburg_statistical_2024,
	title = {Statistical {Learning} {Theory} and {Occam}’s {Razor}: {The} {Core} {Argument}},
	volume = {35},
	issn = {1572-8641},
	url = {http://dx.doi.org/10.1007/s11023-024-09703-y},
	doi = {10.1007/s11023-024-09703-y},
	number = {1},
	journal = {Minds and Machines},
	author = {Sterkenburg, Tom F.},
	month = nov,
	year = {2024},
	note = {Publisher: Springer Science and Business Media LLC},
}

@book{james_introduction_2013,
	title = {An introduction to statistical learning : with applications in {R}},
	url = {https://search.library.wisc.edu/catalog/9910207152902121},
	abstract = {xiv, 426 pages : illustrations (some color) ; 24 cm},
	publisher = {New York : Springer, [2013] ©2013},
	author = {James, Gareth and Hastie, Trevor and Tibshirani, Robert and Witten, Daniela},
	year = {2013},
}

@article{metaxiotis_expert_2000,
	title = {Expert {Systems} in {Medicine}: {Academic} {Illusion} or {Real} {Power}?},
	volume = {8},
	doi = {10.1108/09685220010694017},
	journal = {Information Management \& Computer Security},
	author = {Metaxiotis, Kostas and Samouilidis, J-E},
	month = may,
	year = {2000},
	pages = {75--79},
}

@book{demuth_neural_2014,
	address = {Stillwater, OK, USA},
	edition = {2nd},
	title = {Neural {Network} {Design}},
	isbn = {0-9717321-1-6},
	abstract = {This book, by the authors of the Neural Network Toolbox for MATLAB, provides a clear and detailed coverage of fundamental neural network architectures and learning rules. In it, the authors emphasize a coherent presentation of the principal neural networks, methods for training them and their applications to practical problems. Features Extensive coverage of training methods for both feedforward networks (including multilayer and radial basis networks) and recurrent networks. In addition to conjugate gradient and Levenberg-Marquardt variations of the backpropagation algorithm, the text also covers Bayesian regularization and early stopping, which ensure the generalization ability of trained networks. Associative and competitive networks, including feature maps and learning vector quantization, are explained with simple building blocks. A chapter of practical training tips for function approximation, pattern recognition, clustering and prediction, along with five chapters presenting detailed real-world case studies. Detailed examples and numerous solved problems. Slides and comprehensive demonstration software can be downloaded from hagan.okstate.edu/nnd.html.},
	publisher = {Martin Hagan},
	author = {Demuth, Howard B. and Beale, Mark H. and De Jess, Orlando and Hagan, Martin T.},
	year = {2014},
}

@misc{zhang_dive_2023,
	title = {Dive into {Deep} {Learning}},
	url = {https://arxiv.org/abs/2106.11342},
	author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	year = {2023},
	note = {\_eprint: 2106.11342},
}

@article{werker_cross-language_1984,
	title = {Cross-language speech perception: {Evidence} for perceptual reorganization during the first year of life},
	volume = {7},
	issn = {0163-6383},
	url = {https://www.sciencedirect.com/science/article/pii/S0163638384800223},
	doi = {https://doi.org/10.1016/S0163-6383(84)80022-3},
	abstract = {Previous work in which we compared English infants, English adults, and Hindi adults on their ability to discriminate two pairs of Hindi (non-English) speech contrasts has indicated that infants discriminate speech sounds according to phonetic category without prior specific language experience (Werker, Gilbert, Humphrey, \& Tees, 1981), whereas adults and children as young as age 4 (Werker \& Tees, in press), may lose this ability as a function of age and or linguistic experience. The present work was designed to (a) determine the generalizability of such a decline by comparing adult English, adult Salish, and English infant subjects on their perception of a new non-English (Salish) speech contrast, and (b) delineate the time course of the developmental decline in this ability. The results of these experiments replicate our original findings by showing that infants can discriminate nonnative speech contrasts without relevant experience, and that there is a decline in this ability during ontogeny. Furthermore, data from both cross-sectional and longitudinal studies shows that this decline occurs within the first year of life, and that it is a function of specific language experience.},
	number = {1},
	journal = {Infant Behavior and Development},
	author = {Werker, Janet F. and Tees, Richard C.},
	year = {1984},
	keywords = {cross-language, decline, infants, speech perception},
	pages = {49--63},
}

@incollection{kandel_notitle_2021,
	address = {New York, NY},
	url = {accessbiomedicalscience.mhmedical.com/content.aspx?aid=1180370208},
	booktitle = {Principles of {Neural} {Science}, 6e},
	publisher = {McGraw Hill},
	author = {Kandel, Eric R. and Koester, John D. and Mack, Sarah H. and Siegelbaum, Steven A.},
	year = {2021},
}

@book{purves_neuroscience_2004,
	address = {Sunderland, MA, US},
	series = {Neuroscience, 3rd ed},
	title = {Neuroscience, 3rd ed},
	isbn = {978-0-87893-725-7},
	abstract = {Whether judged in molecular, cellular, systemic, behavioral, or cognitive terms, the human nervous system is a stupendous piece of biological machinery. Given its accomplishments–all the artifacts of human culture, for instance–there is good reason for wanting to understand how the brain and the rest of the nervous system works. The debilitating and costly effects of neurological and psychiatric disease add a further sense of urgency to this quest. The aim of this book is to highlight the intellectual challenges and excitement–as well as the uncertainties–of what many see as the last great frontier of biological science. The information presented should serve as a starting point for undergraduates, medical students, graduate students in the neurosciences, and others who want to understand how the human nervous system operates. Like any other great challenge, neuroscience should be, and is, full of debate, dissension, and considerable fun. All these ingredients have gone into the construction of the third edition of this book; we hope they will be conveyed in equal measure to readers at all levels. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Sinauer Associates},
	editor = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Hall, William C. and LaMantia, Anthony-Samuel and McNamara, James O. and Williams, S. Mark},
	year = {2004},
	keywords = {Nervous System, Neurosciences},
	file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\N586BLRU\\2004-16062-000.html:text/html},
}

@misc{mishqat_neuron_nodate,
	title = {The {Neuron} {Doctrine} (1860-1895) {\textbackslash}textbar {Embryo} {Project} {Encyclopedia}},
	url = {https://embryo.asu.edu/pages/neuron-doctrine-1860-1895},
	urldate = {2025-04-30},
	author = {Mishqat, Isra},
	file = {The Neuron Doctrine (1860-1895) | Embryo Project Encyclopedia:C\:\\Users\\Admin\\Zotero\\storage\\X9XEUDXE\\neuron-doctrine-1860-1895.html:text/html},
}

@article{rozo_cajal_2024,
	title = {Cajal, the neuronal theory and the idea of brain plasticity},
	volume = {18},
	issn = {1662-5129},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10910026/},
	doi = {10.3389/fnana.2024.1331666},
	abstract = {This paper reviews the importance of Cajal’s neuronal theory (the Neuron Doctrine) and the origin and importance of the idea of brain plasticity that emerges from this theory. We first comment on the main Cajal’s discoveries that gave rise and confirmed his Neuron Doctrine: the improvement of staining techniques, his approach to morphological laws, the concepts of dynamic polarisation, neurogenesis and neurotrophic theory, his first discoveries of the nerve cell as an independent cell, his research on degeneration and regeneration and his fight against reticularism. Second, we review Cajal’s ideas on brain plasticity and the years in which they were published, to finally focus on the debate on the origin of the term plasticity and its conceptual meaning, and the originality of Cajal’s proposal compared to those of other authors of the time.},
	urldate = {2025-04-30},
	journal = {Front Neuroanat},
	author = {Rozo, Jairo A. and Martínez-Gallego, Irene and Rodríguez-Moreno, Antonio},
	month = feb,
	year = {2024},
	pmid = {38440067},
	pmcid = {PMC10910026},
	pages = {1331666},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\3D37YPUE\\Rozo et al. - 2024 - Cajal, the neuronal theory and the idea of brain p.pdf:application/pdf},
}

@article{gerlach_ueber_1872,
	title = {Ueber die {Structur} der grauen {Substanz} des menschlichen {Grosshirns}. {Vorläufige} {Mitheilung}},
	volume = {10},
	url = {https://www.booklooker.de/B%C3%BCcher/Joseph-Gerlach%2BUeber-die-Structur-der-grauen-Substanz-des-menschlichen-Grosshirns-Vorl%C3%A4ufige/id/A02ohXHY01ZZu},
	language = {German},
	journal = {Centralblatt für die medizinischen Wissenschaften},
	author = {Gerlach, Joseph von},
	year = {1872},
	pages = {273--288},
}

@misc{liu_kan_2025,
	title = {{KAN}: {Kolmogorov}-{Arnold} {Networks}},
	url = {https://arxiv.org/abs/2404.19756},
	author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
	year = {2025},
	note = {\_eprint: 2404.19756},
}

@inproceedings{cristianini_introduction_2000,
	title = {An {Introduction} to {Support} {Vector} {Machines} and {Other} {Kernel}-based {Learning} {Methods}},
	url = {https://api.semanticscholar.org/CorpusID:60486887},
	author = {Cristianini, Nello and Shawe-Taylor, John},
	year = {2000},
}

@book{e_l_lehmann_theory_1998,
	address = {New York},
	series = {Springer {Texts} in {Statistics}},
	title = {Theory of {Point} {Estimation}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-98502-2},
	url = {http://link.springer.com/10.1007/b98854},
	language = {en},
	urldate = {2025-05-25},
	publisher = {Springer-Verlag},
	author = {E. L. Lehmann, George Casella},
	year = {1998},
	doi = {10.1007/b98854},
	keywords = {average, Likelihood, minimum, Point Estimation, statistical inference, statistics, Variance},
}

@misc{paninski_statistics_2005,
	title = {Statistics 4107: {Intro} to {Math} {Stat} ({Fall} 2005)},
	url = {https://sites.stat.columbia.edu/liam/teaching/4107-fall05/},
	urldate = {2025-05-25},
	author = {Paninski, Liam},
	year = {2005},
	file = {Statistics 4107\: Intro to Math Stat:C\:\\Users\\Admin\\Zotero\\storage\\PS76UCZI\\4107-fall05.html:text/html},
}

@phdthesis{piera_sample_2005,
	type = {Doctoral thesis},
	title = {Sample {Covariance} {Based} {Parameter} {Estimation} {For} {Digital} {Communications}},
	copyright = {ADVERTIMENT. L'accés als continguts d'aquesta tesi doctoral i la seva utilització ha de respectar els drets de la persona autora. Pot ser utilitzada per a consulta o estudi personal, així com en activitats o materials d'investigació i docència en els termes establerts a l'art. 32 del Text Refós de la Llei de Propietat Intel·lectual (RDL 1/1996). Per altres utilitzacions es requereix l'autorització prèvia i expressa de la persona autora. En qualsevol cas, en la utilització dels seus continguts caldrà indicar de forma clara el nom i cognoms de la persona autora i el títol de la tesi doctoral. No s'autoritza la seva reproducció o altres formes d'explotació efectuades amb finalitats de lucre ni la seva comunicació pública des d'un lloc aliè al servei TDX. Tampoc s'autoritza la presentació del seu contingut en una finestra o marc aliè a TDX (framing). Aquesta reserva de drets afecta tant als continguts de la tesi com als seus resums i índexs.},
	url = {https://upcommons.upc.edu/handle/2117/94206},
	abstract = {DOI: 10.5821/dissertation-2117-94206},
	language = {eng},
	urldate = {2025-05-25},
	school = {Universitat Politècnica de Catalunya},
	author = {Piera, Villares and Javier, Nemesio},
	month = oct,
	year = {2005},
	doi = {10.5821/dissertation-2117-94206},
	keywords = {Àrees temàtiques de la UPC::Enginyeria de la telecomunicació, Comunicacions digitals, estimació, estimadors quadràtics, filtre de kalman, màxima versemblança, nuisance parameters, processament d'arrays, sincronització},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\9U7HJ5S6\\Piera and Javier - 2005 - Sample Covariance Based Parameter Estimation For D.pdf:application/pdf},
}

@misc{kay_fundamentals_1993,
	title = {Fundamentals of statistical signal processing: estimation theory {\textbackslash}textbar {Guide} books {\textbackslash}textbar {ACM} {Digital} {Library}},
	url = {https://dl.acm.org/doi/10.5555/151045},
	urldate = {2025-05-25},
	author = {Kay, Steven M.},
	year = {1993},
	file = {Fundamentals of statistical signal processing\: estimation theory | Guide books | ACM Digital Library:C\:\\Users\\Admin\\Zotero\\storage\\FMSFP4RC\\151045.html:text/html},
}

@misc{hu_model_2021-1,
	title = {Model {Complexity} of {Deep} {Learning}: {A} {Survey}},
	url = {https://arxiv.org/abs/2103.05127},
	author = {Hu, Xia and Chu, Lingyang and Pei, Jian and Liu, Weiqing and Bian, Jiang},
	year = {2021},
	note = {\_eprint: 2103.05127},
}

@misc{luo_investigating_2024,
	title = {Investigating the {Impact} of {Model} {Complexity} in {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2410.00699},
	author = {Luo, Jing and Wang, Huiyuan and Huang, Weiran},
	year = {2024},
	note = {\_eprint: 2410.00699},
}

@misc{barcelo_model_2020,
	title = {Model {Interpretability} through the {Lens} of {Computational} {Complexity}},
	url = {https://arxiv.org/abs/2010.12265},
	author = {Barceló, Pablo and Monet, Mikaël and Pérez, Jorge and Subercaseaux, Bernardo},
	year = {2020},
	note = {\_eprint: 2010.12265},
}

@incollection{molnar_quantifying_2020,
	title = {Quantifying {Model} {Complexity} via {Functional} {Decomposition} for {Better} {Post}-hoc {Interpretability}},
	isbn = {978-3-030-43823-4},
	url = {http://dx.doi.org/10.1007/978-3-030-43823-4_17},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	year = {2020},
	doi = {10.1007/978-3-030-43823-4_17},
	note = {ISSN: 1865-0937},
	pages = {193--204},
}

@misc{janik_complexity_2021,
	title = {Complexity for deep neural networks and other characteristics of deep feature representations},
	url = {https://arxiv.org/abs/2006.04791},
	author = {Janik, Romuald A. and Witaszczyk, Przemek},
	year = {2021},
	note = {\_eprint: 2006.04791},
}

@misc{rossi_temporal_2020,
	title = {Temporal {Graph} {Networks} for {Deep} {Learning} on {Dynamic} {Graphs}},
	url = {https://arxiv.org/abs/2006.10637},
	author = {Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
	year = {2020},
	note = {\_eprint: 2006.10637},
}

@inproceedings{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {https://arxiv.org/abs/1903.02428},
	booktitle = {{ICLR} {Workshop} on {Representation} {Learning} on {Graphs} and {Manifolds}},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	year = {2019},
}

@misc{pytorch_geometric_team_creating_2025,
	title = {Creating {Message} {Passing} {Networks}},
	url = {https://pytorch-geometric.readthedocs.io/en/2.6.1/notes/create_gnn.html},
	author = {{PyTorch Geometric Team}},
	year = {2025},
}

@techreport{sontag_vapnik-chervonenkis_1994,
	title = {Vapnik-{Chervonenkis} {Dimension} of {Recurrent} {Neural} {Networks}},
	url = {http://www.sontaglab.org/FTPDIR/vc-expo.pdf},
	institution = {Sontag Lab},
	author = {Sontag, Eduardo D.},
	year = {1994},
}

@book{zhang_mathematical_2023,
	title = {Mathematical {Analysis} of {Machine} {Learning} {Algorithms}},
	url = {https://www.cambridge.org/core/books/mathematical-analysis-of-machine-learning-algorithms/EB9BABB05A5C312F19C38E5A01A5ECFC},
	publisher = {Cambridge University Press},
	author = {Zhang, Tong},
	year = {2023},
	doi = {10.1017/9781009093057},
}

@article{achlioptas_stochastic_nodate,
	title = {Stochastic {Gradient} {Descent} in {Theory} and {Practice}},
	language = {en},
	journal = {Lecture note, Stanford's AI},
	author = {Achlioptas, Panos},
	file = {Achlioptas - Stochastic Gradient Descent in Theory and Practice.pdf:C\:\\Users\\Admin\\Zotero\\storage\\7B7AKI5E\\Achlioptas - Stochastic Gradient Descent in Theory and Practice.pdf:application/pdf},
}

@misc{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	doi = {10.48550/arXiv.1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2025-06-28},
	publisher = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1609.04747 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Admin\\Zotero\\storage\\QIVZWHXY\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\KYVXYR4N\\1609.html:text/html},
}

@misc{zhang_gradient_2019,
	title = {Gradient {Descent} based {Optimization} {Algorithms} for {Deep} {Learning} {Models} {Training}},
	url = {http://arxiv.org/abs/1903.03614},
	doi = {10.48550/arXiv.1903.03614},
	abstract = {In this paper, we aim at providing an introduction to the gradient descent based optimization algorithms for learning deep neural network models. Deep learning models involving multiple nonlinear projection layers are very challenging to train. Nowadays, most of the deep learning model training still relies on the back propagation algorithm actually. In back propagation, the model variables will be updated iteratively until convergence with gradient descent based optimization algorithms. Besides the conventional vanilla gradient descent algorithm, many gradient descent variants have also been proposed in recent years to improve the learning performance, including Momentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this paper respectively.},
	urldate = {2025-06-28},
	publisher = {arXiv},
	author = {Zhang, Jiawei},
	month = mar,
	year = {2019},
	note = {arXiv:1903.03614 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Admin\\Zotero\\storage\\RIRNXDED\\Zhang - 2019 - Gradient Descent based Optimization Algorithms for.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4CLP7N7A\\1903.html:text/html},
}
